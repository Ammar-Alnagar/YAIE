<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Mini-YAIE: Educational LLM Inference Engine</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon-de23e50b.svg">
        <link rel="shortcut icon" href="favicon-8114d1fc.png">
        <link rel="stylesheet" href="css/variables-8adf115d.css">
        <link rel="stylesheet" href="css/general-2459343d.css">
        <link rel="stylesheet" href="css/chrome-ae938929.css">
        <link rel="stylesheet" href="css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "searchindex-cbd8f59e.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc-676105c4.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                        <button id="mdbook-search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="mdbook-searchbar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352c79.5 0 144-64.5 144-144s-64.5-144-144-144S64 128.5 64 208s64.5 144 144 144z"/></svg></span>
                        </button>
                    </div>

                    <h1 class="menu-title">Mini-YAIE: Educational LLM Inference Engine</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>
                        <a href="https://github.com/ammar-alnagar/Mini-YAIE" title="Git repository" aria-label="Git repository">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span>
                        </a>

                    </div>
                </div>

                <div id="mdbook-search-wrapper" class="hidden">
                    <form id="mdbook-searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="mdbook-searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="mdbook-searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <span class=fa-svg id="fa-spin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M304 48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zm0 416c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM48 304c26.5 0 48-21.5 48-48s-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48zm464-48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM142.9 437c18.7-18.7 18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zm0-294.2c18.7-18.7 18.7-49.1 0-67.9S93.7 56.2 75 75s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zM369.1 437c18.7 18.7 49.1 18.7 67.9 0s18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9z"/></svg></span>
                            </div>
                        </div>
                    </form>
                    <div id="mdbook-searchresults-outer" class="searchresults-outer hidden">
                        <div id="mdbook-searchresults-header" class="searchresults-header"></div>
                        <ul id="mdbook-searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="welcome-to-mini-yaie"><a class="header" href="#welcome-to-mini-yaie">Welcome to Mini-YAIE</a></h1>
<p><strong>Mini-YAIE</strong> (Yet Another Inference Engine) is an educational project designed to demystify modern Large Language Model (LLM) inference engines.</p>
<p>Driven by the need for efficiency, modern engines like <strong>SGLang</strong>, <strong>vLLM</strong>, and <strong>TensorRT-LLM</strong> use sophisticated techniques to maximize GPU throughput and minimize latency. Mini-YAIE provides a simplified, clean implementation of these concepts, focusing on:</p>
<ul>
<li><strong>Continuous Batching</strong></li>
<li><strong>Paged KV Caching</strong></li>
<li><strong>Radix Attention (Prefix Sharing)</strong></li>
</ul>
<h2 id="how-to-use-this-guide"><a class="header" href="#how-to-use-this-guide">How to use this guide</a></h2>
<p>This documentation is structured to take you from high-level concepts to low-level implementation.</p>
<ol>
<li><strong>Core Concepts</strong>: Start here to understand the <em>why</em> and <em>what</em> of inference optimization.</li>
<li><strong>Architecture</strong>: Understand how the system components fit together.</li>
<li><strong>Implementation Guides</strong>: Step-by-step guides to implementing the missing “kernels” in Python and CUDA.</li>
</ol>
<h2 id="your-mission"><a class="header" href="#your-mission">Your Mission</a></h2>
<p>The codebase contains <strong>placeholders</strong> (<code>NotImplementedError</code>) for critical components. Your goal is to implement these components following this guide, turning Mini-YAIE from a skeleton into a fully functional inference engine.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h1>
<p>To successfully implement the kernels in Mini-YAIE, you should be familiar with:</p>
<h2 id="programming-languages"><a class="header" href="#programming-languages">Programming Languages</a></h2>
<ul>
<li><strong>Python (Intermediate)</strong>: Understanding of classes, inheritance, type hinting, and PyTorch tensors.</li>
<li><strong>C++ (Basic)</strong>: For reading and writing the CUDA kernels (though much of the boilerplate is provided).</li>
<li><strong>CUDA (Basic)</strong>: Understanding of the GPU execution model (blocks, threads, shared memory).</li>
</ul>
<h2 id="machine-learning-concepts"><a class="header" href="#machine-learning-concepts">Machine Learning Concepts</a></h2>
<ul>
<li><strong>Transformer Architecture</strong>: Queries, Keys, Values, Attention mechanism.</li>
<li><strong>Tensors</strong>: Shapes, dimensions, matrix multiplication.</li>
</ul>
<h2 id="tools"><a class="header" href="#tools">Tools</a></h2>
<ul>
<li><strong>Git</strong>: For version control.</li>
<li><strong>Linux/Unix Shell</strong>: For running commands.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="environment-setup"><a class="header" href="#environment-setup">Environment Setup</a></h1>
<h2 id="1-clone-the-repository"><a class="header" href="#1-clone-the-repository">1. Clone the Repository</a></h2>
<pre><code class="language-bash">git clone https://github.com/yourusername/YAIE.git
cd YAIE
</code></pre>
<h2 id="2-python-environment"><a class="header" href="#2-python-environment">2. Python Environment</a></h2>
<p>It is highly recommended to use a virtual environment.</p>
<pre><code class="language-bash">python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
pip install -e .
</code></pre>
<h2 id="3-cuda-requirements-optional"><a class="header" href="#3-cuda-requirements-optional">3. CUDA Requirements (Optional)</a></h2>
<p>To build and run the CUDA kernels, you need:</p>
<ul>
<li>NVIDIA GPU (Compute Capability 7.0+)</li>
<li>CUDA Toolkit 11.8+</li>
<li>PyTorch with CUDA support</li>
</ul>
<p>If you do not have a GPU, you can still implement the Python logic and the CPU fallback kernels.</p>
<h2 id="4-documentation-setup"><a class="header" href="#4-documentation-setup">4. Documentation Setup</a></h2>
<p>To serve this documentation locally:</p>
<ol>
<li>
<p><strong>Install mdbook</strong>:</p>
<pre><code class="language-bash"># If you have Rust/Cargo installed:
cargo install mdbook

# Or download the binary from their GitHub releases.
</code></pre>
</li>
<li>
<p><strong>Serve the docs</strong>:</p>
<pre><code class="language-bash">mdbook serve docs
</code></pre>
<p>Navigate to <code>http://localhost:3000</code> in your browser.</p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="llm-inference-the-basics"><a class="header" href="#llm-inference-the-basics">LLM Inference: The Basics</a></h1>
<p>Large Language Model (LLM) inference is the process of generating text from a trained model. It consists of two distinct phases.</p>
<h2 id="1-prefill-phase-the-prompt"><a class="header" href="#1-prefill-phase-the-prompt">1. Prefill Phase (The “Prompt”)</a></h2>
<ul>
<li><strong>Input</strong>: The user’s prompt (e.g., “Write a poem about cats”).</li>
<li><strong>Operation</strong>: The model processes all input tokens in parallel.</li>
<li><strong>Output</strong>: The KV (Key-Value) cache for the prompt and the first generated token.</li>
<li><strong>Characteristic</strong>: Compute-bound. We maximize parallelism here.</li>
</ul>
<h2 id="2-decode-phase-the-generation"><a class="header" href="#2-decode-phase-the-generation">2. Decode Phase (The “Generation”)</a></h2>
<ul>
<li><strong>Input</strong>: The previously generated token.</li>
<li><strong>Operation</strong>: The model generates one token at a time, autoregressively.</li>
<li><strong>Output</strong>: The next token and an updated KV cache.</li>
<li><strong>Characteristic</strong>: Memory-bound. We are limited by how fast we can move weights and KV cache from memory to the compute units.</li>
</ul>
<h2 id="the-kv-cache"><a class="header" href="#the-kv-cache">The KV Cache</a></h2>
<p>State management is crucial. Instead of re-computing the attention for all previous tokens at every step, we cache the <strong>Key</strong> and <strong>Value</strong> vectors for every token in the sequence. This is the <strong>KV Cache</strong>. Managing this cache efficiently is the main challenge of high-performance inference engines.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="continuous-batching"><a class="header" href="#continuous-batching">Continuous Batching</a></h1>
<h2 id="the-problem-static-batching"><a class="header" href="#the-problem-static-batching">The Problem: Static Batching</a></h2>
<p>In traditional deep learning (like training), we use static batches: all sequences in a batch must have the same length (padded to the max length).</p>
<ul>
<li><strong>Waste</strong>: Padding wastes computation and memory.</li>
<li><strong>Latency</strong>: We must wait for the longest sequence to finish generating before finishing the batch.</li>
</ul>
<h2 id="the-solution-continuous-batching-orca"><a class="header" href="#the-solution-continuous-batching-orca">The Solution: Continuous Batching (Orca)</a></h2>
<p>Introduced by the Orca paper, Continuous Batching (or Iteration-level Batching) decouples the implementation of a batch from the user’s view.</p>
<ol>
<li><strong>Iteration Level</strong>: The engine runs one iteration (one forward pass) at a time.</li>
<li><strong>Dynamic Insertion</strong>: As soon as a request finishes, it enters the “Completed” state. A new request from the queue can immediately take its place in the next iteration.</li>
<li><strong>No Padding</strong>: We process only the valid tokens for each request.</li>
</ol>
<p>This significantly improves <strong>throughput</strong> (requests per second) without hurting <strong>latency</strong> (time per token) for individual requests.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="radix-attention-sglang"><a class="header" href="#radix-attention-sglang">Radix Attention (SGLang)</a></h1>
<p><strong>Radix Attention</strong> is the core innovation of SGLang. It optimizes the <strong>Prefill Phase</strong> by reusing computation from previous requests.</p>
<h2 id="the-intuition"><a class="header" href="#the-intuition">The Intuition</a></h2>
<p>If two users ask:</p>
<ol>
<li>“Write a Python script to <strong>scrape a website</strong>.”</li>
<li>“Write a Python script to <strong>sort a list</strong>.”</li>
</ol>
<p>They share the prefix “Write a Python script to “. In a standard engine, we would compute the KV cache for this prefix twice.</p>
<h2 id="the-radix-tree"><a class="header" href="#the-radix-tree">The Radix Tree</a></h2>
<p>SGLang maintains a <strong>Radix Tree</strong> (Trie) of all token sequences currently in the KV cache.</p>
<ul>
<li><strong>Nodes</strong>: Sequences of tokens.</li>
<li><strong>Edges</strong>: Transitions to new tokens.</li>
</ul>
<p>When a new request arrives, we map its prompt to the longest matching path in the Radix Tree.</p>
<ul>
<li><strong>Hit</strong>: We reuse the KV Cache for the matched part. The prefill only needs to compute the <em>new</em> suffix.</li>
<li><strong>Miss</strong>: We compute from scratch.</li>
</ul>
<h2 id="benefits"><a class="header" href="#benefits">Benefits</a></h2>
<ul>
<li><strong>Reduced Latency</strong>: “Time To First Token” (TTFT) is nearly zero for cached prefixes.</li>
<li><strong>Higher Throughput</strong>: Less computation required per request.</li>
<li><strong>Complex Workflows</strong>: Enables efficient multi-turn chat, few-shot learning, and tree-of-thought prompting.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="paged-attention-vllm"><a class="header" href="#paged-attention-vllm">Paged Attention (vLLM)</a></h1>
<p><strong>Paged Attention</strong> is the core innovation of vLLM. It optimizes the <strong>Decode Phase</strong> by managing memory like an Operating System.</p>
<h2 id="the-problem-memory-fragmentation"><a class="header" href="#the-problem-memory-fragmentation">The Problem: Memory Fragmentation</a></h2>
<p>Before vLLM, engines allocated contiguous memory for the maximum possible length of a request.</p>
<ul>
<li><strong>Internal Fragmentation</strong>: If a request was shorter than max length, memory was wasted.</li>
<li><strong>External Fragmentation</strong>: We couldn’t fit a new request even if total free memory was sufficient, because no single contiguous block was large enough.</li>
</ul>
<h2 id="the-solution-pagiing"><a class="header" href="#the-solution-pagiing">The Solution: Pagiing</a></h2>
<p>Inspired by virtual memory in OS:</p>
<ol>
<li><strong>Blocks</strong>: Divide KV Cache into fixed-size blocks (e.g., 16 tokens per block).</li>
<li><strong>Non-Contiguous</strong>: Blocks can be stored anywhere in physical GPU memory.</li>
<li><strong>Mapping</strong>: A “Block Table” maps logical token positions to physical block addresses.</li>
</ol>
<h2 id="the-kernel"><a class="header" href="#the-kernel">The Kernel</a></h2>
<p>The Paged Attention kernel allows the Attention mechanism to read keys and values from these non-contiguous blocks on the fly, enabling near-zero memory waste.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="system-overview"><a class="header" href="#system-overview">System Overview</a></h1>
<p>Mini-YAIE follows a modular architecture similar to vLLM.</p>
<h2 id="high-level-components"><a class="header" href="#high-level-components">High-Level Components</a></h2>
<pre><code class="language-mermaid">graph TD
    User[User / API Client] --&gt; API[FastAPI Server (server/api.py)]
    API --&gt; Engine[Inference Engine (engine.py)]

    subgraph Core Logic
        Engine --&gt; Scheduler[SGLang Scheduler (core/sglang_scheduler.py)]
        Engine --&gt; MM[Memory Manager (kernels/kv_cache.py)]
        Engine --&gt; Model[LLM Model (HuggingFace)]
    end

    subgraph Kernels
        Scheduler --&gt; RadixTree[Radix Tree (kernels/radix_tree.py)]
        Model --&gt; RadixAttn[Radix Attention (kernels/radix_attention.py)]
        Model --&gt; PagedAttn[Paged Attention (kernels/cuda/paged_attention.cu)]
    end
</code></pre>
<h2 id="data-flow"><a class="header" href="#data-flow">Data Flow</a></h2>
<ol>
<li><strong>Request</strong>: User sends a prompt to the API.</li>
<li><strong>Scheduling</strong>: Scheduler analyzes the prompt, checks the Radix Tree for cached prefixes, and assigns a Request ID.</li>
<li><strong>Batching</strong>: <code>schedule_step</code> groups requests into Prefill (new) and Decode (running) batches.</li>
<li><strong>Execution</strong>: The Engine runs the model.
<ul>
<li><strong>Prefill</strong>: Computes initial KV cache for new prompts.</li>
<li><strong>Decode</strong>: Generates one token for running requests.</li>
</ul>
</li>
<li><strong>Memory</strong>: The Memory Manager allocates/frees GPU blocks as needed.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="the-engine-enginepy"><a class="header" href="#the-engine-enginepy">The Engine (<code>engine.py</code>)</a></h1>
<p>The <code>InferenceEngine</code> class is the conductor of the orchestra. It ties everything together.</p>
<h2 id="responsibilities"><a class="header" href="#responsibilities">Responsibilities</a></h2>
<ul>
<li><strong>Initialization</strong>: Loads the Model, Tokenizer, Scheduler, and Memory Manager.</li>
<li><strong>Request Entry</strong>: Accepts <code>generate()</code> calls and pushes them to the Scheduler.</li>
<li><strong>The Loop</strong>: Runs the continuous batching loop <code>_run_generation_loop</code>.</li>
</ul>
<h2 id="key-method-_run_generation_loop"><a class="header" href="#key-method-_run_generation_loop">Key Method: <code>_run_generation_loop</code></a></h2>
<p>This is the heart of continuous batching.</p>
<pre><code class="language-python">while has_active_requests:
    # 1. Ask Scheduler for work
    prefill_batch, decode_batch = scheduler.schedule_step()

    # 2. Run Prefill (Compute-bound)
    if prefill_batch:
        run_prefill(prefill_batch)

    # 3. Run Decode (Memory-bound)
    if decode_batch:
        run_decode(decode_batch)
</code></pre>
<p>In Mini-YAIE, the loop logic is simplified for education, but the structure mirrors production engines.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="the-scheduler-coresglang_schedulerpy"><a class="header" href="#the-scheduler-coresglang_schedulerpy">The Scheduler (<code>core/sglang_scheduler.py</code>)</a></h1>
<p>The <code>SGLangScheduler</code> is the “Brain”. It decides <em>what</em> runs <em>when</em>.</p>
<h2 id="sglang-features"><a class="header" href="#sglang-features">SGLang Features</a></h2>
<p>Unlike a simple FIFO queue, this scheduler is aware of <strong>Radix Attention</strong>.</p>
<h3 id="prefix-hashing"><a class="header" href="#prefix-hashing">Prefix Hashing</a></h3>
<p>When a request arrives, <code>_calculate_prefix_hash</code> identifies its common prefix.</p>
<pre><code class="language-python"># Simplified Logic
prefix_hash = sha256(prompt)
</code></pre>
<p>In a full implementation, this uses the Radix Tree to find the precise token match.</p>
<h3 id="scheduling-policy"><a class="header" href="#scheduling-policy">Scheduling Policy</a></h3>
<ol>
<li><strong>Prioritize Decode</strong>: To minimize latency, we always try to run pending decode steps first.</li>
<li><strong>Fill with Prefill</strong>: If there is leftover GPU memory (or batch size capacity), we pull new requests from the queue for prefill.</li>
</ol>
<h2 id="state-machine"><a class="header" href="#state-machine">State Machine</a></h2>
<p>Requests transition through states:
<code>PENDING</code> -&gt; <code>RUNNING_PREFILL</code> -&gt; <code>SCHEDULED_DECODE</code> -&gt; <code>RUNNING_DECODE</code> -&gt; <code>COMPLETED</code></p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="memory-manager-kernelskv_cachepy"><a class="header" href="#memory-manager-kernelskv_cachepy">Memory Manager (<code>kernels/kv_cache.py</code>)</a></h1>
<p>The <code>KVCacheManager</code> is the “Allocator”. It manages the Paged KV Cache.</p>
<h2 id="the-block-table"><a class="header" href="#the-block-table">The Block Table</a></h2>
<p>Just like an OS manages RAM pages, this manager tracks GPU memory blocks.</p>
<ul>
<li><strong>Physical Blocks</strong>: Fixed-size tensors in GPU memory (e.g., <code>[num_blocks, block_size, head_dim]</code>).</li>
<li><strong>Logical Slots</strong>: The token positions in a sequence.</li>
</ul>
<h2 id="key-methods"><a class="header" href="#key-methods">Key Methods</a></h2>
<ul>
<li><code>allocate_blocks(request)</code>: Finds free physical blocks and assigns them to a request.</li>
<li><code>free_blocks(request)</code>: Returns blocks to the free pool.</li>
<li><code>get_block_table(request)</code>: Returns the mapping <code>[logical_idx -&gt; physical_idx]</code> for the attention kernel.</li>
</ul>
<p><strong>Student Task</strong>: You will implement the allocation logic in the Python Kernels phase.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="python-kernels-guide"><a class="header" href="#python-kernels-guide">Python Kernels Guide</a></h1>
<p>This section guides you through implementing the core Python logic “kernels”. These are not CUDA kernels, but critical algorithmic components.</p>
<h2 id="your-tasks"><a class="header" href="#your-tasks">Your Tasks</a></h2>
<ol>
<li><strong>Radix Tree</strong>: Implement the Trie data structure for prefix matching.</li>
<li><strong>KV Cache Manager</strong>: Implement the block allocation strategy.</li>
<li><strong>Sampling</strong>: Implement the token sampling logic.</li>
</ol>
<p><strong>Why Python?</strong>
While computation happens in CUDA, the <em>logic</em> of memory management and prefix matching is complex and best handled in Python (or C++ CPU code) before dispatching to the GPU.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="radix-tree-kernelsradix_treepy"><a class="header" href="#radix-tree-kernelsradix_treepy">Radix Tree (<code>kernels/radix_tree.py</code>)</a></h1>
<h2 id="concept"><a class="header" href="#concept">Concept</a></h2>
<p>A <strong>Radix Tree</strong> (or Compressed Trie) is a space-optimized prefix tree.</p>
<h2 id="implementation-goal"><a class="header" href="#implementation-goal">Implementation Goal</a></h2>
<p>You need to implement the <code>RadixTree</code> class with:</p>
<h3 id="1-inserttoken_ids-request_id"><a class="header" href="#1-inserttoken_ids-request_id">1. <code>insert(token_ids, request_id)</code></a></h3>
<ul>
<li>Traverse the tree with <code>token_ids</code>.</li>
<li>If a path exists, follow it.</li>
<li>If tokens diverge, <strong>split</strong> the edge and create a new node.</li>
<li>Store the <code>request_id</code> at the leaf.</li>
</ul>
<h3 id="2-match_prefixtoken_ids"><a class="header" href="#2-match_prefixtoken_ids">2. <code>match_prefix(token_ids)</code></a></h3>
<ul>
<li>Traverse the tree to find the longest common prefix.</li>
<li>Return the <code>node_id</code> and the length of the match.</li>
<li>This tells the scheduler how many tokens we can skip computing!</li>
</ul>
<h3 id="3-removerequest_id"><a class="header" href="#3-removerequest_id">3. <code>remove(request_id)</code></a></h3>
<ul>
<li>When a request finishes, decrement reference counts.</li>
<li>If a node has no references, free its associated KV cache blocks.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="kv-cache-manager-kernelskv_cachepy"><a class="header" href="#kv-cache-manager-kernelskv_cachepy">KV Cache Manager (<code>kernels/kv_cache.py</code>)</a></h1>
<h2 id="concept-1"><a class="header" href="#concept-1">Concept</a></h2>
<p>Manage a pool of fixed-size integer IDs representing GPU memory blocks.</p>
<h2 id="implementation-goal-1"><a class="header" href="#implementation-goal-1">Implementation Goal</a></h2>
<p>Implement <code>KVCacheManager</code>:</p>
<h3 id="1-__init__"><a class="header" href="#1-__init__">1. <code>__init__</code></a></h3>
<ul>
<li>Create a list of all available block indices: <code>free_blocks = [0, 1, ..., N-1]</code>.</li>
<li>Initialize an empty mapping <code>request_to_blocks = {}</code>.</li>
</ul>
<h3 id="2-allocaterequest-num_tokens"><a class="header" href="#2-allocaterequest-num_tokens">2. <code>allocate(request, num_tokens)</code></a></h3>
<ul>
<li>Calculate blocks needed: <code>ceil(num_tokens / block_size)</code>.</li>
<li>Pop that many indices from <code>free_blocks</code>.</li>
<li>Store them in <code>request_to_blocks</code>.</li>
<li><strong>Edge Case</strong>: If not enough blocks, raise OutOfMemory (or trigger eviction).</li>
</ul>
<h3 id="3-freerequest"><a class="header" href="#3-freerequest">3. <code>free(request)</code></a></h3>
<ul>
<li>Retrieve the blocks owned by the request.</li>
<li>Append them back to <code>free_blocks</code>.</li>
<li>Delete the request mapping.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="radix-attention-module"><a class="header" href="#radix-attention-module">Radix Attention Module</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="sampling-kernelssamplingpy"><a class="header" href="#sampling-kernelssamplingpy">Sampling (<code>kernels/sampling.py</code>)</a></h1>
<h2 id="concept-2"><a class="header" href="#concept-2">Concept</a></h2>
<p>Convert the model’s raw output logits (probabilities) into a single token ID.</p>
<h2 id="implementation-goal-2"><a class="header" href="#implementation-goal-2">Implementation Goal</a></h2>
<p>Implement <code>SamplingKernel.sample</code>:</p>
<h3 id="1-temperature"><a class="header" href="#1-temperature">1. Temperature</a></h3>
<ul>
<li><code>logits = logits / temperature</code></li>
<li>Higher temp = flatter distribution (more random).</li>
<li>Lower temp = sharper distribution (more deterministic).</li>
</ul>
<h3 id="2-top-p-nucleus"><a class="header" href="#2-top-p-nucleus">2. Top-P (Nucleus)</a></h3>
<ul>
<li>Sort probabilities descending.</li>
<li>Compute cumulative sum.</li>
<li>Cut off where sum &gt; <code>top_p</code>.</li>
<li>Renormalize remaining probabilities.</li>
</ul>
<h3 id="3-selection"><a class="header" href="#3-selection">3. Selection</a></h3>
<ul>
<li><code>torch.multinomial(probs, 1)</code></li>
<li>Return the selected token ID.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="cuda-setup"><a class="header" href="#cuda-setup">CUDA Setup</a></h1>
<p>Implementing custom CUDA kernels requires compiling C++/CUDA code and binding it to Python.</p>
<h2 id="setuppy"><a class="header" href="#setuppy"><code>setup.py</code></a></h2>
<p>We use <code>torch.utils.cpp_extension</code> to handle compilation.
The <code>setup.py</code> file in the root directory is already configured to look for kernels in <code>src/kernels/cuda/</code>.</p>
<h3 id="triggering-compilation"><a class="header" href="#triggering-compilation">Triggering Compilation</a></h3>
<p>To compile your kernels, simply run:</p>
<pre><code class="language-bash">pip install -e .
</code></pre>
<p>This command invokes <code>nvcc</code> (NVIDIA CUDA Compiler) on your <code>.cu</code> files.</p>
<h3 id="using-the-kernels"><a class="header" href="#using-the-kernels">Using the Kernels</a></h3>
<p>Once compiled, you can import them in Python:</p>
<pre><code class="language-python">import mini_yaie_kernels

# Call your C++ function
mini_yaie_kernels.flash_attention.forward(...)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="memory-operations-kernelscudamemory_opscu"><a class="header" href="#memory-operations-kernelscudamemory_opscu">Memory Operations (<code>kernels/cuda/memory_ops.cu</code>)</a></h1>
<h2 id="concept-3"><a class="header" href="#concept-3">Concept</a></h2>
<p>Moving data between different GPU memory locations is a frequent operation in Paged Attention.</p>
<h2 id="implementation-goal-3"><a class="header" href="#implementation-goal-3">Implementation Goal</a></h2>
<p>Implement <code>copy_blocks_kernel</code>:</p>
<h3 id="signature"><a class="header" href="#signature">Signature</a></h3>
<pre><code class="language-cpp">void copy_blocks_kernel(
    torch::Tensor key_cache,      // [num_blocks, block_size, head_dim]
    torch::Tensor value_cache,    // [num_blocks, block_size, head_dim]
    torch::Tensor block_mapping,  // [num_mappings, 2] (src, dst)
    int num_mappings
);
</code></pre>
<h3 id="logic"><a class="header" href="#logic">Logic</a></h3>
<ol>
<li><strong>Parallelism</strong>: Launch one thread per token to copy.</li>
<li><strong>Indexing</strong>:
<ul>
<li><code>mapping_idx = blockIdx.x</code></li>
<li><code>src_block = block_mapping[mapping_idx][0]</code></li>
<li><code>dst_block = block_mapping[mapping_idx][1]</code></li>
</ul>
</li>
<li><strong>Copy</strong>:
<ul>
<li>Read <code>key/value</code> from <code>src_block</code> at <code>threadIdx</code> offset.</li>
<li>Write to <code>dst_block</code>.</li>
</ul>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="flash-attention-kernelscudaflash_attentioncu"><a class="header" href="#flash-attention-kernelscudaflash_attentioncu">Flash Attention (<code>kernels/cuda/flash_attention.cu</code>)</a></h1>
<h2 id="concept-4"><a class="header" href="#concept-4">Concept</a></h2>
<p>A standard attention implementation is $O(N^2)$ in memory usage. Flash Attention uses tiling to compute attention in constant memory.</p>
<h2 id="implementation-goal-4"><a class="header" href="#implementation-goal-4">Implementation Goal</a></h2>
<p>Implement <code>flash_attention_forward</code>:</p>
<h3 id="algorithm-simplified"><a class="header" href="#algorithm-simplified">Algorithm (simplified)</a></h3>
<ol>
<li><strong>Tiling</strong>: Load a block of Queries (Q) into shared memory (SRAM).</li>
<li><strong>Loop</strong>: Iterate over blocks of Keys (K) and Values (V) from HBM (Global Memory).
<ul>
<li>Load K, V block into SRAM.</li>
<li>Compute QK^T (Attention Scores).</li>
<li>Apply Softmax (using online softmax scaling).</li>
<li>Compute Score * V.</li>
<li>Accumulate result to Output.</li>
</ul>
</li>
<li><strong>Write Output</strong>: Store final result to HBM.</li>
</ol>
<blockquote>
<p><strong>Note</strong>: For this educational project, a naive CUDA implementation is acceptable if tiling is too complex.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="paged-attention-kernelscudapaged_attentioncu"><a class="header" href="#paged-attention-kernelscudapaged_attentioncu">Paged Attention (<code>kernels/cuda/paged_attention.cu</code>)</a></h1>
<h2 id="concept-5"><a class="header" href="#concept-5">Concept</a></h2>
<p>Compute attention where K and V are stored in non-contiguous blocks.</p>
<h2 id="implementation-goal-5"><a class="header" href="#implementation-goal-5">Implementation Goal</a></h2>
<p>Implement <code>paged_attention_kernel</code>:</p>
<h3 id="inputs"><a class="header" href="#inputs">Inputs</a></h3>
<ul>
<li><code>block_tables</code>: A tensor mapping <code>[request_id, logical_block_idx] -&gt; physical_block_idx</code>.</li>
</ul>
<h3 id="logic-1"><a class="header" href="#logic-1">Logic</a></h3>
<ol>
<li><strong>Thread Mapping</strong>: Each thread block handles one sequence (request).</li>
<li><strong>Gathering</strong>:
<ul>
<li>Instead of <code>K[i]</code>, we must compute the physical address.</li>
<li><code>block_number = block_tables[request_id][token_index / block_size]</code></li>
<li><code>block_offset = token_index % block_size</code></li>
<li><code>physical_addr = base_ptr + block_number * stride + block_offset</code></li>
</ul>
</li>
<li><strong>Attention</strong>:
<ul>
<li>Load K, V using the calculated physical addresses.</li>
<li>Compute Attention as usual.</li>
</ul>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="radix-operations-kernelscudaradix_opscu"><a class="header" href="#radix-operations-kernelscudaradix_opscu">Radix Operations (<code>kernels/cuda/radix_ops.cu</code>)</a></h1>
<h2 id="concept-6"><a class="header" href="#concept-6">Concept</a></h2>
<p>If we have a Radix Tree, we can optimize attention even further by knowing exactly which tokens are shared.</p>
<h2 id="implementation-goal-6"><a class="header" href="#implementation-goal-6">Implementation Goal</a></h2>
<p>This is an advanced extension.</p>
<h3 id="logic-2"><a class="header" href="#logic-2">Logic</a></h3>
<ol>
<li><strong>Tree Traversal on GPU</strong>: Mapping the Radix Tree structure to a GPU-friendly format (e.g., flattened arrays).</li>
<li><strong>Prefix Matching</strong>: A kernel that takes a batch of prompts and quickly identifies the longest common prefix node ID for each.</li>
</ol>
<p><em>Note: In the simplified version, this logic is often kept in CPU (Python) and only the KV indices are passed to the GPU.</em></p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="api-endpoints-serverapipy"><a class="header" href="#api-endpoints-serverapipy">API Endpoints (<code>server/api.py</code>)</a></h1>
<h2 id="openai-compatibility"><a class="header" href="#openai-compatibility">OpenAI Compatibility</a></h2>
<p>Mini-YAIE strives to be drop-in compatible with OpenAI’s API format.</p>
<h3 id="post-v1chatcompletions"><a class="header" href="#post-v1chatcompletions">POST <code>/v1/chat/completions</code></a></h3>
<p><strong>Request Body</strong>:</p>
<pre><code class="language-json">{
  "model": "gpt2",
  "messages": [{ "role": "user", "content": "Hello!" }],
  "temperature": 0.7,
  "max_tokens": 100
}
</code></pre>
<p><strong>Response</strong>:</p>
<pre><code class="language-json">{
  "id": "chat-123",
  "object": "chat.completion",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Hi there!"
      },
      "finish_reason": "stop"
    }
  ]
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="cli-usage"><a class="header" href="#cli-usage">CLI Usage</a></h1>
<h2 id="starting-the-server"><a class="header" href="#starting-the-server">Starting the Server</a></h2>
<pre><code class="language-bash">yaie serve --model gpt2 --port 8000
</code></pre>
<h2 id="interactive-chat"><a class="header" href="#interactive-chat">Interactive Chat</a></h2>
<pre><code class="language-bash">yaie chat --model gpt2
</code></pre>
<h2 id="arguments"><a class="header" href="#arguments">Arguments</a></h2>
<ul>
<li><code>--model</code>: Name of the HuggingFace model or path to local model.</li>
<li><code>--max-batch-size</code>: Limit the number of concurrent requests.</li>
<li><code>--gpu-memory-utilization</code>: Fraction of GPU memory to use for KV cache (default 0.9).</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="production-considerations"><a class="header" href="#production-considerations">Production Considerations</a></h1>
<p>While Mini-YAIE is educational, here is what you would need for production:</p>
<h2 id="1-batching--latency"><a class="header" href="#1-batching--latency">1. Batching &amp; Latency</a></h2>
<ul>
<li><strong>Timeouts</strong>: Requests waiting too long in the queue should be handled.</li>
<li><strong>Preemption</strong>: If high-priority requests come in, lower priority ones should be paused (swapped to CPU).</li>
</ul>
<h2 id="2-distributed-inference"><a class="header" href="#2-distributed-inference">2. Distributed Inference</a></h2>
<ul>
<li><strong>Tensor Parallelism</strong>: Splitting the model weights across multiple GPUs (Megatron-LM style).</li>
<li><strong>Pipeline Parallelism</strong>: Splitting layers across GPUs.</li>
</ul>
<h2 id="3-quantization"><a class="header" href="#3-quantization">3. Quantization</a></h2>
<ul>
<li><strong>FP8 / INT8</strong>: Running with lower precision to save memory and increase compute speed (using library like <code>bitsandbytes</code>).</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="references"><a class="header" href="#references">References</a></h1>
<ol>
<li><strong>SGLang</strong>: Efficient Execution of Structured Language Model Programs. <a href="https://github.com/sgl-project/sglang">Link</a></li>
<li><strong>vLLM</strong>: Easy, Fast, and Cheap LLM Serving with PagedAttention. <a href="https://github.com/vllm-project/vllm">Link</a></li>
<li><strong>FlashAttention</strong>: Fast and Memory-Efficient Exact Attention with IO-Awareness. <a href="https://github.com/Dao-AILab/flash-attention">Link</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="troubleshooting"><a class="header" href="#troubleshooting">Troubleshooting</a></h1>
<h2 id="cuda-kernel-not-found"><a class="header" href="#cuda-kernel-not-found">“CUDA kernel not found”</a></h2>
<ul>
<li>Ensure you ran <code>pip install -e .</code>.</li>
<li>Check if <code>nvcc</code> is in your path: <code>nvcc --version</code>.</li>
</ul>
<h2 id="outofmemoryerror"><a class="header" href="#outofmemoryerror">“OutOfMemoryError”</a></h2>
<ul>
<li>Decrease <code>max_batch_size</code>.</li>
<li>Decrease <code>kv_cache_manager</code> block count.</li>
</ul>
<h2 id="importerror-attempted-relative-import"><a class="header" href="#importerror-attempted-relative-import">“ImportError: attempted relative import…”</a></h2>
<ul>
<li>Ensure you are running the <code>yaie</code> command, or running python as a module <code>python -m src.cli.main</code>.</li>
<li>Do not run scripts directly like <code>python src/engine.py</code>.</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr-ef4e11c1.min.js"></script>
        <script src="mark-09e88c2c.min.js"></script>
        <script src="searcher-c2a407aa.js"></script>

        <script src="clipboard-1626706a.min.js"></script>
        <script src="highlight-abc7f01d.js"></script>
        <script src="book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>


    </div>
    </body>
</html>
