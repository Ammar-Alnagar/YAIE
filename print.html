<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Mini-YAIE: Educational LLM Inference Engine</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        <!-- Custom Head with Mermaid Support -->
        <script type="module">
            import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10.9.1/dist/mermaid.esm.min.mjs';
            mermaid.initialize({ startOnLoad: false });
        
            document.addEventListener('DOMContentLoaded', async () => {
                const elements = document.querySelectorAll('code.language-mermaid');
                for (const element of elements) {
                    const pre = element.parentElement;
                    const div = document.createElement('div');
                    div.classList.add('mermaid');
                    div.textContent = element.textContent;
                    pre.replaceWith(div);
                }
                await mermaid.run({
                    querySelector: '.mermaid'
                });
            });
        </script>

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon-de23e50b.svg">
        <link rel="shortcut icon" href="favicon-8114d1fc.png">
        <link rel="stylesheet" href="css/variables-8adf115d.css">
        <link rel="stylesheet" href="css/general-2459343d.css">
        <link rel="stylesheet" href="css/chrome-ae938929.css">
        <link rel="stylesheet" href="css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "searchindex-1139ad4c.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc-7b94e012.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                        <button id="mdbook-search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="mdbook-searchbar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352c79.5 0 144-64.5 144-144s-64.5-144-144-144S64 128.5 64 208s64.5 144 144 144z"/></svg></span>
                        </button>
                    </div>

                    <h1 class="menu-title">Mini-YAIE: Educational LLM Inference Engine</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>
                        <a href="https://github.com/ammar-alnagar/Mini-YAIE" title="Git repository" aria-label="Git repository">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span>
                        </a>

                    </div>
                </div>

                <div id="mdbook-search-wrapper" class="hidden">
                    <form id="mdbook-searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="mdbook-searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="mdbook-searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <span class=fa-svg id="fa-spin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M304 48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zm0 416c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM48 304c26.5 0 48-21.5 48-48s-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48zm464-48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM142.9 437c18.7-18.7 18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zm0-294.2c18.7-18.7 18.7-49.1 0-67.9S93.7 56.2 75 75s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zM369.1 437c18.7 18.7 49.1 18.7 67.9 0s18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9z"/></svg></span>
                            </div>
                        </div>
                    </form>
                    <div id="mdbook-searchresults-outer" class="searchresults-outer hidden">
                        <div id="mdbook-searchresults-header" class="searchresults-header"></div>
                        <ul id="mdbook-searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="welcome-to-mini-yaie"><a class="header" href="#welcome-to-mini-yaie">Welcome to Mini-YAIE</a></h1>
<p><strong>Mini-YAIE</strong> (Yet Another Inference Engine) is an educational project designed to demystify modern Large Language Model (LLM) inference engines.</p>
<p>Driven by the need for efficiency, modern engines like <strong>SGLang</strong>, <strong>vLLM</strong>, and <strong>TensorRT-LLM</strong> use sophisticated techniques to maximize GPU throughput and minimize latency. Mini-YAIE provides a simplified, clean implementation of these concepts, focusing on:</p>
<ul>
<li><strong>Continuous Batching</strong></li>
<li><strong>Paged KV Caching</strong></li>
<li><strong>Radix Attention (Prefix Sharing)</strong></li>
</ul>
<h2 id="how-to-use-this-guide"><a class="header" href="#how-to-use-this-guide">How to use this guide</a></h2>
<p>This documentation is structured to take you from high-level concepts to low-level implementation.</p>
<ol>
<li><strong>Core Concepts</strong>: Start here to understand the <em>why</em> and <em>what</em> of inference optimization.</li>
<li><strong>Architecture</strong>: Understand how the system components fit together.</li>
<li><strong>Implementation Guides</strong>: Step-by-step guides to implementing the missing “kernels” in Python and CUDA.</li>
</ol>
<h2 id="your-mission"><a class="header" href="#your-mission">Your Mission</a></h2>
<p>The codebase contains <strong>placeholders</strong> (<code>NotImplementedError</code>) for critical components. Your goal is to implement these components following this guide, turning Mini-YAIE from a skeleton into a fully functional inference engine.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h1>
<p>To successfully implement the kernels in Mini-YAIE, you should be familiar with:</p>
<h2 id="programming-languages"><a class="header" href="#programming-languages">Programming Languages</a></h2>
<ul>
<li><strong>Python (Intermediate)</strong>: Understanding of classes, inheritance, type hinting, and PyTorch tensors.</li>
<li><strong>C++ (Basic)</strong>: For reading and writing the CUDA kernels (though much of the boilerplate is provided).</li>
<li><strong>CUDA (Basic)</strong>: Understanding of the GPU execution model (blocks, threads, shared memory).</li>
</ul>
<h2 id="machine-learning-concepts"><a class="header" href="#machine-learning-concepts">Machine Learning Concepts</a></h2>
<ul>
<li><strong>Transformer Architecture</strong>: Queries, Keys, Values, Attention mechanism.</li>
<li><strong>Tensors</strong>: Shapes, dimensions, matrix multiplication.</li>
</ul>
<h2 id="tools"><a class="header" href="#tools">Tools</a></h2>
<ul>
<li><strong>Git</strong>: For version control.</li>
<li><strong>Linux/Unix Shell</strong>: For running commands.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="environment-setup"><a class="header" href="#environment-setup">Environment Setup</a></h1>
<h2 id="1-clone-the-repository"><a class="header" href="#1-clone-the-repository">1. Clone the Repository</a></h2>
<pre><code class="language-bash">git clone https://github.com/yourusername/Mini-YAIE.git
cd Mini-YAIE
</code></pre>
<h2 id="2-python-environment"><a class="header" href="#2-python-environment">2. Python Environment</a></h2>
<p>It is highly recommended to use a virtual environment.</p>
<pre><code class="language-bash">python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
pip install -e .
</code></pre>
<h2 id="3-cuda-requirements-optional"><a class="header" href="#3-cuda-requirements-optional">3. CUDA Requirements (Optional)</a></h2>
<p>To build and run the CUDA kernels, you need:</p>
<ul>
<li>NVIDIA GPU (Compute Capability 7.0+)</li>
<li>CUDA Toolkit 11.8+</li>
<li>PyTorch with CUDA support</li>
</ul>
<p>If you do not have a GPU, you can still implement the Python logic and the CPU fallback kernels.</p>
<h2 id="4-documentation-setup"><a class="header" href="#4-documentation-setup">4. Documentation Setup</a></h2>
<p>To serve this documentation locally:</p>
<ol>
<li>
<p><strong>Install mdbook</strong>:</p>
<pre><code class="language-bash"># If you have Rust/Cargo installed:
cargo install mdbook

# Or download the binary from their GitHub releases.
</code></pre>
</li>
<li>
<p><strong>Serve the docs</strong>:</p>
<pre><code class="language-bash">mdbook serve docs
</code></pre>
<p>Navigate to <code>http://localhost:3000</code> in your browser.</p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="model-loading"><a class="header" href="#model-loading">Model Loading</a></h1>
<p>YAIE supports loading models from HuggingFace Hub with automatic caching and local model support.</p>
<h2 id="modelloader-class"><a class="header" href="#modelloader-class">ModelLoader Class</a></h2>
<p>The <code>ModelLoader</code> class in <code>src/models/loader.py</code> handles all model and tokenizer loading operations.</p>
<h3 id="initialization"><a class="header" href="#initialization">Initialization</a></h3>
<pre><code class="language-python">from src.models.loader import ModelLoader

# Load from HuggingFace Hub
loader = ModelLoader("microsoft/DialoGPT-medium")

# Load from local path
loader = ModelLoader("/path/to/local/model")
</code></pre>
<h3 id="loading-models"><a class="header" href="#loading-models">Loading Models</a></h3>
<pre><code class="language-python"># Load the model
model = loader.load_model()

# Load the tokenizer
tokenizer = loader.load_tokenizer()
</code></pre>
<h2 id="supported-model-sources"><a class="header" href="#supported-model-sources">Supported Model Sources</a></h2>
<h3 id="huggingface-hub-models"><a class="header" href="#huggingface-hub-models">HuggingFace Hub Models</a></h3>
<p>YAIE can load any compatible model from HuggingFace Hub:</p>
<pre><code class="language-python"># Popular conversational models
loader = ModelLoader("microsoft/DialoGPT-medium")
loader = ModelLoader("microsoft/DialoGPT-large")

# Code generation models
loader = ModelLoader("Salesforce/codegen-350M-mono")

# General purpose models
loader = ModelLoader("gpt2")
loader = ModelLoader("gpt2-medium")
</code></pre>
<h3 id="local-models"><a class="header" href="#local-models">Local Models</a></h3>
<p>You can also load models from local directories:</p>
<pre><code class="language-python"># Load from local path
loader = ModelLoader("./models/my-custom-model")
</code></pre>
<h2 id="caching-behavior"><a class="header" href="#caching-behavior">Caching Behavior</a></h2>
<h3 id="automatic-caching"><a class="header" href="#automatic-caching">Automatic Caching</a></h3>
<p>Models are automatically cached in the standard HuggingFace cache directory:</p>
<ul>
<li><strong>Linux/macOS</strong>: <code>~/.cache/huggingface/</code></li>
<li><strong>Windows</strong>: <code>C:\Users\&lt;username&gt;\.cache\huggingface\</code></li>
</ul>
<h3 id="cache-structure"><a class="header" href="#cache-structure">Cache Structure</a></h3>
<pre><code>~/.cache/huggingface/
├── hub/
│   └── models--microsoft--DialoGPT-medium/
│       ├── blobs/
│       ├── refs/
│       └── snapshots/
│           └── abc123.../
│               ├── config.json
│               ├── pytorch_model.bin
│               └── tokenizer.json
</code></pre>
<h3 id="cache-management"><a class="header" href="#cache-management">Cache Management</a></h3>
<p>The loader automatically:</p>
<ol>
<li>Checks for existing models in cache</li>
<li>Downloads missing models from HuggingFace Hub</li>
<li>Uses cached models for subsequent loads</li>
</ol>
<h2 id="model-configuration"><a class="header" href="#model-configuration">Model Configuration</a></h2>
<h3 id="data-types"><a class="header" href="#data-types">Data Types</a></h3>
<p>Models are loaded with optimized data types:</p>
<pre><code class="language-python"># Models are loaded with float16 by default for efficiency
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float16,  # Half precision
    device_map="auto"          # Automatic device placement
)
</code></pre>
<h3 id="device-placement"><a class="header" href="#device-placement">Device Placement</a></h3>
<ul>
<li><strong>Single GPU</strong>: Model is loaded directly to GPU</li>
<li><strong>Multi-GPU</strong>: Automatically distributed across available GPUs</li>
<li><strong>CPU</strong>: Falls back to CPU if no GPU available</li>
</ul>
<h2 id="tokenizer-configuration"><a class="header" href="#tokenizer-configuration">Tokenizer Configuration</a></h2>
<h3 id="automatic-pad-token"><a class="header" href="#automatic-pad-token">Automatic Pad Token</a></h3>
<p>The loader ensures tokenizers have a pad token:</p>
<pre><code class="language-python">if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
</code></pre>
<p>This is important for batch processing where sequences need to be padded to the same length.</p>
<h2 id="memory-optimization"><a class="header" href="#memory-optimization">Memory Optimization</a></h2>
<h3 id="lazy-loading"><a class="header" href="#lazy-loading">Lazy Loading</a></h3>
<p>Models are loaded on-demand, not at import time:</p>
<pre><code class="language-python"># Model is not loaded here
loader = ModelLoader("gpt2")

# Model is loaded here when requested
model = loader.load_model()
</code></pre>
<h3 id="memory-mapping"><a class="header" href="#memory-mapping">Memory Mapping</a></h3>
<p>Large models use memory mapping to reduce RAM usage during loading.</p>
<h2 id="error-handling"><a class="header" href="#error-handling">Error Handling</a></h2>
<h3 id="network-issues"><a class="header" href="#network-issues">Network Issues</a></h3>
<p>If downloading fails, the loader will retry and provide clear error messages.</p>
<h3 id="incompatible-models"><a class="header" href="#incompatible-models">Incompatible Models</a></h3>
<p>Models must be compatible with <code>AutoModelForCausalLM</code>. Incompatible models will raise clear errors.</p>
<h3 id="disk-space"><a class="header" href="#disk-space">Disk Space</a></h3>
<p>Large models require significant disk space. The loader shows download progress and estimated sizes.</p>
<h2 id="performance-tips"><a class="header" href="#performance-tips">Performance Tips</a></h2>
<h3 id="pre-download-models"><a class="header" href="#pre-download-models">Pre-download Models</a></h3>
<p>For production deployments, pre-download models:</p>
<pre><code class="language-bash"># This will cache the model
python -c "from src.models.loader import ModelLoader; loader = ModelLoader('microsoft/DialoGPT-medium'); loader.load_model()"
</code></pre>
<h3 id="cache-location"><a class="header" href="#cache-location">Cache Location</a></h3>
<p>You can customize the cache location by setting the <code>HF_HOME</code> environment variable:</p>
<pre><code class="language-bash">export HF_HOME=/path/to/custom/cache
</code></pre>
<h3 id="model-selection"><a class="header" href="#model-selection">Model Selection</a></h3>
<p>Choose appropriate model sizes for your hardware:</p>
<ul>
<li><strong>Small models</strong> (&lt; 1GB): <code>gpt2</code>, <code>DialoGPT-small</code></li>
<li><strong>Medium models</strong> (1-5GB): <code>gpt2-medium</code>, <code>DialoGPT-medium</code></li>
<li><strong>Large models</strong> (&gt; 5GB): <code>gpt2-large</code>, <code>DialoGPT-large</code></li>
</ul>
<h2 id="troubleshooting"><a class="header" href="#troubleshooting">Troubleshooting</a></h2>
<h3 id="common-issues"><a class="header" href="#common-issues">Common Issues</a></h3>
<p><strong>“Model not found” errors:</strong></p>
<ul>
<li>Check model name spelling</li>
<li>Verify model exists on HuggingFace Hub</li>
<li>Ensure internet connection for downloads</li>
</ul>
<p><strong>Out of memory errors:</strong></p>
<ul>
<li>Try smaller models</li>
<li>Reduce batch sizes in configuration</li>
<li>Use CPU-only mode if GPU memory is insufficient</li>
</ul>
<p><strong>Tokenizer issues:</strong></p>
<ul>
<li>Some models may require special token handling</li>
<li>Check the model’s documentation on HuggingFace Hub
<parameter name="filePath">docs/src/intro/model_loading.md</parameter></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="llm-inference-the-basics"><a class="header" href="#llm-inference-the-basics">LLM Inference: The Basics</a></h1>
<p>Large Language Model (LLM) inference is the process of generating text from a trained model. It consists of two distinct phases.</p>
<h2 id="1-prefill-phase-the-prompt"><a class="header" href="#1-prefill-phase-the-prompt">1. Prefill Phase (The “Prompt”)</a></h2>
<ul>
<li><strong>Input</strong>: The user’s prompt (e.g., “Write a poem about cats”).</li>
<li><strong>Operation</strong>: The model processes all input tokens in parallel.</li>
<li><strong>Output</strong>: The KV (Key-Value) cache for the prompt and the first generated token.</li>
<li><strong>Characteristic</strong>: Compute-bound. We maximize parallelism here.</li>
</ul>
<h2 id="the-process-visualized"><a class="header" href="#the-process-visualized">The Process Visualized</a></h2>
<pre><code class="language-mermaid">sequenceDiagram
    participant U as User
    participant E as Engine
    participant M as Model

    rect rgb(200, 220, 255)
    note right of U: Prefill Phase (Parallel)
    U-&gt;&gt;E: Prompt: "A B C"
    E-&gt;&gt;M: Forward(["A", "B", "C"])
    M--&gt;&gt;E: KV Cache + Logits(C)
    end

    rect rgb(220, 255, 200)
    note right of U: Decode Phase (Serial)
    loop Until EOS
        E-&gt;&gt;M: Forward([Last Token])
        M--&gt;&gt;E: Update KV + Logits
        E-&gt;&gt;E: Sample Next Token
    end
    end
    E-&gt;&gt;U: Response
</code></pre>
<h2 id="2-decode-phase-the-generation"><a class="header" href="#2-decode-phase-the-generation">2. Decode Phase (The “Generation”)</a></h2>
<ul>
<li><strong>Input</strong>: The previously generated token.</li>
<li><strong>Operation</strong>: The model generates one token at a time, autoregressively.</li>
<li><strong>Output</strong>: The next token and an updated KV cache.</li>
<li><strong>Characteristic</strong>: Memory-bound. We are limited by how fast we can move weights and KV cache from memory to the compute units.</li>
</ul>
<h2 id="the-kv-cache"><a class="header" href="#the-kv-cache">The KV Cache</a></h2>
<p>State management is crucial. Instead of re-computing the attention for all previous tokens at every step, we cache the <strong>Key</strong> and <strong>Value</strong> vectors for every token in the sequence. This is the <strong>KV Cache</strong>. Managing this cache efficiently is the main challenge of high-performance inference engines.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="continuous-batching"><a class="header" href="#continuous-batching">Continuous Batching</a></h1>
<h2 id="the-problem-static-batching"><a class="header" href="#the-problem-static-batching">The Problem: Static Batching</a></h2>
<p>In traditional deep learning (like training), we use static batches: all sequences in a batch must have the same length (padded to the max length).</p>
<ul>
<li><strong>Waste</strong>: Padding wastes computation and memory.</li>
<li><strong>Latency</strong>: We must wait for the longest sequence to finish generating before finishing the batch.</li>
</ul>
<h2 id="visualizing-the-difference"><a class="header" href="#visualizing-the-difference">Visualizing the Difference</a></h2>
<pre><code class="language-mermaid">gantt
    title Static Batching (Inefficient)
    dateFormat YYYY-MM-DD
    axisFormat %H:%M

    section Batch 1
    Req A (Short) :done, a1, 2024-01-01, 2d
    Padding       :crit, 2024-01-03, 2d
    Req B (Long)  :active, b1, 2024-01-01, 4d

    section Batch 2
    Req C :c1, 2024-01-05, 2d
</code></pre>
<pre><code class="language-mermaid">gantt
    title Continuous Batching (Efficient)
    dateFormat YYYY-MM-DD
    axisFormat %H:%M

    section GPU Stream
    Req A (Short) :done, a1, 2024-01-01, 2d
    Req C (New!)  :active, c1, 2024-01-03, 2d

    section GPU Stream 2
    Req B (Long)  :active, b1, 2024-01-01, 4d
</code></pre>
<h2 id="the-solution-continuous-batching-orca"><a class="header" href="#the-solution-continuous-batching-orca">The Solution: Continuous Batching (Orca)</a></h2>
<p>Introduced by the Orca paper, Continuous Batching (or Iteration-level Batching) decouples the implementation of a batch from the user’s view.</p>
<ol>
<li><strong>Iteration Level</strong>: The engine runs one iteration (one forward pass) at a time.</li>
<li><strong>Dynamic Insertion</strong>: As soon as a request finishes, it enters the “Completed” state. A new request from the queue can immediately take its place in the next iteration.</li>
<li><strong>No Padding</strong>: We process only the valid tokens for each request.</li>
</ol>
<p>This significantly improves <strong>throughput</strong> (requests per second) without hurting <strong>latency</strong> (time per token) for individual requests.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="radix-attention-sglang"><a class="header" href="#radix-attention-sglang">Radix Attention (SGLang)</a></h1>
<p><strong>Radix Attention</strong> is the core innovation of SGLang. It optimizes the <strong>Prefill Phase</strong> by reusing computation from previous requests.</p>
<h2 id="the-intuition"><a class="header" href="#the-intuition">The Intuition</a></h2>
<p>If two users ask:</p>
<ol>
<li>“Write a Python script to <strong>scrape a website</strong>.”</li>
<li>“Write a Python script to <strong>sort a list</strong>.”</li>
</ol>
<p>They share the prefix “Write a Python script to “. In a standard engine, we would compute the KV cache for this prefix twice.</p>
<pre><code class="language-mermaid">graph TD
    classDef shared fill:#aaffaa,stroke:#333,stroke-width:2px;
    classDef unique fill:#ffaaaa,stroke:#333,stroke-width:2px;

    Root((Root)) --&gt; Node1["Write a Python script to"]:::shared
    Node1 --&gt; Node2["scrape a website"]:::unique
    Node1 --&gt; Node3["sort a list"]:::unique

    style Node1 fill:#aaffaa
</code></pre>
<h2 id="the-radix-tree"><a class="header" href="#the-radix-tree">The Radix Tree</a></h2>
<p>SGLang maintains a <strong>Radix Tree</strong> (Trie) of all token sequences currently in the KV cache.</p>
<ul>
<li><strong>Nodes</strong>: Sequences of tokens.</li>
<li><strong>Edges</strong>: Transitions to new tokens.</li>
</ul>
<p>When a new request arrives, we map its prompt to the longest matching path in the Radix Tree.</p>
<ul>
<li><strong>Hit</strong>: We reuse the KV Cache for the matched part. The prefill only needs to compute the <em>new</em> suffix.</li>
<li><strong>Miss</strong>: We compute from scratch.</li>
</ul>
<h2 id="benefits"><a class="header" href="#benefits">Benefits</a></h2>
<ul>
<li><strong>Reduced Latency</strong>: “Time To First Token” (TTFT) is nearly zero for cached prefixes.</li>
<li><strong>Higher Throughput</strong>: Less computation required per request.</li>
<li><strong>Complex Workflows</strong>: Enables efficient multi-turn chat, few-shot learning, and tree-of-thought prompting.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="paged-attention-vllm"><a class="header" href="#paged-attention-vllm">Paged Attention (vLLM)</a></h1>
<p><strong>Paged Attention</strong> is the core innovation of vLLM. It optimizes the <strong>Decode Phase</strong> by managing memory like an Operating System.</p>
<h2 id="the-problem-memory-fragmentation"><a class="header" href="#the-problem-memory-fragmentation">The Problem: Memory Fragmentation</a></h2>
<p>Before vLLM, engines allocated contiguous memory for the maximum possible length of a request.</p>
<ul>
<li><strong>Internal Fragmentation</strong>: If a request was shorter than max length, memory was wasted.</li>
<li><strong>External Fragmentation</strong>: We couldn’t fit a new request even if total free memory was sufficient, because no single contiguous block was large enough.</li>
</ul>
<h2 id="the-solution-paging"><a class="header" href="#the-solution-paging">The Solution: Paging</a></h2>
<p>Inspired by virtual memory in OS:</p>
<ol>
<li><strong>Blocks</strong>: Divide KV Cache into fixed-size blocks (e.g., 16 tokens per block).</li>
<li><strong>Non-Contiguous</strong>: Blocks can be stored anywhere in physical GPU memory.</li>
<li><strong>Mapping</strong>: A “Block Table” maps logical token positions to physical block addresses.</li>
</ol>
<pre><code class="language-mermaid">graph LR
    subgraph Logical[Logical Sequence Request]
        L0[Block 0: "Hello"]
        L1[Block 1: "World"]
        L2[Block 2: "!"]
    end

    subgraph Table[Page Table]
        T0[0 -&gt; 7]
        T1[1 -&gt; 2]
        T2[2 -&gt; 9]
    end

    subgraph Physical[GPU Memory Physical Blocks]
        B0[Block 0]
        B1[Block 1]
        B2[Block 2: "World"]:::used
        B3...
        B7[Block 7: "Hello"]:::used
        B8...
        B9[Block 9: "!"]:::used
    end

    L0 --&gt; T0 --&gt; B7
    L1 --&gt; T1 --&gt; B2
    L2 --&gt; T2 --&gt; B9

    classDef used fill:#aaffaa;
</code></pre>
<h2 id="the-kernel"><a class="header" href="#the-kernel">The Kernel</a></h2>
<p>The Paged Attention kernel allows the Attention mechanism to read keys and values from these non-contiguous blocks on the fly, enabling near-zero memory waste.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="system-architecture-overview"><a class="header" href="#system-architecture-overview">System Architecture Overview</a></h1>
<h2 id="introduction"><a class="header" href="#introduction">Introduction</a></h2>
<p>Mini-YAIE (Yet Another Inference Engine) is an educational implementation of modern LLM inference techniques, specifically designed to demonstrate concepts from state-of-the-art systems like SGLang, vLLM, and TensorRT-LLM. The architecture focuses on three core optimizations:</p>
<ol>
<li><strong>Continuous Batching</strong>: Dynamically batching incoming requests to maximize GPU utilization</li>
<li><strong>Radix Attention</strong>: Efficient attention mechanism with prefix sharing for similar requests</li>
<li><strong>Paged KV-Cache</strong>: Memory-efficient key-value cache management</li>
</ol>
<h2 id="high-level-architecture"><a class="header" href="#high-level-architecture">High-Level Architecture</a></h2>
<pre><code>┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   API Layer     │    │  Engine Core    │    │  Model/Kernels  │
│  (FastAPI)      │◄──►│  (Scheduler,   │◄──►│  (PyTorch/     │
│                 │    │  Attention)    │    │  CUDA)         │
└─────────────────┘    └─────────────────┘    └─────────────────┘
         ▲                       ▲                       ▲
         │                       │                       │
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   CLI Layer     │    │  Model Loading  │    │  Memory Mgmt    │
│  (yaie serve/   │    │  (HuggingFace  │    │  (Paged Cache)  │
│   yaie chat)    │    │  Integration)   │    │                 │
└─────────────────┘    └─────────────────┘    └─────────────────┘
</code></pre>
<h2 id="core-components"><a class="header" href="#core-components">Core Components</a></h2>
<h3 id="1-main-inference-engine-enginepy"><a class="header" href="#1-main-inference-engine-enginepy">1. Main Inference Engine (<code>engine.py</code>)</a></h3>
<p>The main inference engine orchestrates all components and provides the high-level API for inference. It implements SGLang-style continuous batching with radix attention and prefix sharing.</p>
<p><strong>Key Responsibilities:</strong></p>
<ul>
<li>Request orchestration and management</li>
<li>Integration between scheduler, attention mechanisms, and memory management</li>
<li>API layer communication</li>
<li>Model loading and tokenizer management</li>
</ul>
<h3 id="2-sglang-scheduler-coresglang_schedulerpy"><a class="header" href="#2-sglang-scheduler-coresglang_schedulerpy">2. SGLang Scheduler (<code>core/sglang_scheduler.py</code>)</a></h3>
<p>The SGLang-style scheduler implements advanced request scheduling with:</p>
<ul>
<li><strong>Prefix-based request grouping</strong>: Groups requests with common prefixes for computation sharing</li>
<li><strong>Separate prefill and decode scheduling</strong>: Optimizes for the different computational patterns</li>
<li><strong>Memory-aware batch sizing</strong>: Considers available KV-cache memory when scheduling</li>
<li><strong>Continuous batching optimization</strong>: Maintains high GPU utilization</li>
</ul>
<h3 id="3-radix-attention-system-kernelsradix_attentionpy"><a class="header" href="#3-radix-attention-system-kernelsradix_attentionpy">3. Radix Attention System (<code>kernels/radix_attention.py</code>)</a></h3>
<p>Implements the radial attention mechanism with:</p>
<ul>
<li><strong>Prefix sharing</strong>: Reduces redundant computation for requests with common prefixes</li>
<li><strong>Paged KV-cache integration</strong>: Efficient memory management for variable-length requests</li>
<li><strong>RoPE (Rotary Position Embeddings)</strong>: Supports position-aware attention</li>
</ul>
<h3 id="4-paged-kv-cache-management-kernelskv_cachepy"><a class="header" href="#4-paged-kv-cache-management-kernelskv_cachepy">4. Paged KV-Cache Management (<code>kernels/kv_cache.py</code>)</a></h3>
<p>Efficient memory management using page-based allocation:</p>
<ul>
<li><strong>Fixed-size blocks</strong>: Reduces memory fragmentation</li>
<li><strong>Request-to-block mapping</strong>: Tracks which blocks belong to which requests</li>
<li><strong>Dynamic allocation/deallocation</strong>: Manages memory based on request lifecycle</li>
</ul>
<h3 id="5-radix-tree-system-kernelsradix_treepy"><a class="header" href="#5-radix-tree-system-kernelsradix_treepy">5. Radix Tree System (<code>kernels/radix_tree.py</code>)</a></h3>
<p>Enables efficient prefix matching and computation sharing:</p>
<ul>
<li><strong>Trie-based structure</strong>: Organizes token sequences hierarchically</li>
<li><strong>Request grouping</strong>: Identifies requests with shared prefixes</li>
<li><strong>Computation optimization</strong>: Provides information for scheduler optimization</li>
</ul>
<h3 id="6-sampling-kernel-kernelssamplingpy"><a class="header" href="#6-sampling-kernel-kernelssamplingpy">6. Sampling Kernel (<code>kernels/sampling.py</code>)</a></h3>
<p>Implements core sampling algorithms:</p>
<ul>
<li><strong>Temperature scaling</strong>: Controls randomness in generation</li>
<li><strong>Top-K sampling</strong>: Limits selection to top K most probable tokens</li>
<li><strong>Top-P (Nucleus) sampling</strong>: Limits selection to tokens that sum to probability P</li>
</ul>
<h3 id="7-api-server-serverapipy"><a class="header" href="#7-api-server-serverapipy">7. API Server (<code>server/api.py</code>)</a></h3>
<p>Provides OpenAI-compatible API endpoints:</p>
<ul>
<li><strong>RESTful design</strong>: Follows OpenAI’s API specification</li>
<li><strong>Streaming support</strong>: Real-time token streaming</li>
<li><strong>Health monitoring</strong>: Server status endpoints</li>
</ul>
<h2 id="data-flow"><a class="header" href="#data-flow">Data Flow</a></h2>
<p>The system processes requests in the following sequence:</p>
<ol>
<li><strong>Request Arrival</strong>: Client sends a request through the API layer</li>
<li><strong>Request Scheduling</strong>: SGLang scheduler groups requests with common prefixes</li>
<li><strong>Prefill Phase</strong>: Process full prompt sequences using radial attention</li>
<li><strong>Decode Phase</strong>: Generate tokens one-by-one with shared computation</li>
<li><strong>KV-Cache Management</strong>: Efficient memory allocation and sharing</li>
<li><strong>Response Generation</strong>: Return results via API layer</li>
</ol>
<h2 id="key-design-principles"><a class="header" href="#key-design-principles">Key Design Principles</a></h2>
<h3 id="modularity"><a class="header" href="#modularity">Modularity</a></h3>
<p>Each component is designed to be independent, allowing for focused learning and experimentation.</p>
<h3 id="educational-focus"><a class="header" href="#educational-focus">Educational Focus</a></h3>
<p>Clean, well-documented code with comprehensive explanations of key concepts.</p>
<h3 id="sglang-style-optimization"><a class="header" href="#sglang-style-optimization">SGLang-Style Optimization</a></h3>
<p>Focus on prefix sharing and radix trees to maximize computational efficiency.</p>
<h3 id="memory-efficiency"><a class="header" href="#memory-efficiency">Memory Efficiency</a></h3>
<p>Paged cache management to reduce memory fragmentation and maximize utilization.</p>
<h2 id="architecture-benefits"><a class="header" href="#architecture-benefits">Architecture Benefits</a></h2>
<ol>
<li><strong>High Throughput</strong>: Continuous batching and prefix sharing maximize GPU utilization</li>
<li><strong>Memory Efficiency</strong>: Paged KV-cache reduces fragmentation and enables larger batch sizes</li>
<li><strong>Scalability</strong>: Modular design allows for optimization of individual components</li>
<li><strong>Educational Value</strong>: Clean implementation of state-of-the-art techniques</li>
</ol>
<h2 id="integration-points"><a class="header" href="#integration-points">Integration Points</a></h2>
<p>The system integrates components through well-defined interfaces:</p>
<ul>
<li>Engine connects to scheduler for request management</li>
<li>Scheduler connects to memory manager for KV-cache coordination</li>
<li>Attention mechanisms access KV-cache through the memory manager</li>
<li>Sampler provides token selection for generation</li>
<li>API layer communicates with the engine for request processing</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="configuration-management"><a class="header" href="#configuration-management">Configuration Management</a></h1>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>Mini-YAIE uses a flexible configuration system that allows users to customize various aspects of the inference engine without modifying the code. The configuration system provides settings for memory management, scheduling, model loading, and performance optimization.</p>
<h2 id="configuration-structure"><a class="header" href="#configuration-structure">Configuration Structure</a></h2>
<p>The configuration system is built around the <code>SGLangConfig</code> dataclass in <code>src/config.py</code>. The system supports:</p>
<ul>
<li>Dataclass-based configuration with type hints</li>
<li>Default values for all parameters</li>
<li>Dictionary-based overrides</li>
<li>Component-specific configuration sections</li>
</ul>
<h2 id="key-configuration-parameters"><a class="header" href="#key-configuration-parameters">Key Configuration Parameters</a></h2>
<h3 id="scheduler-configuration"><a class="header" href="#scheduler-configuration">Scheduler Configuration</a></h3>
<pre><code class="language-python"># Maximum batch size for processing requests
max_batch_size: int = 8

# Maximum batch size for prefill operations
max_prefill_batch_size: int = 16

# Maximum batch size for decode operations
max_decode_batch_size: int = 256

# Maximum sequence length allowed
max_seq_len: int = 2048
</code></pre>
<h3 id="kv-cache-configuration"><a class="header" href="#kv-cache-configuration">KV Cache Configuration</a></h3>
<pre><code class="language-python"># Number of GPU memory blocks for KV-cache
num_gpu_blocks: int = 2000

# Number of CPU memory blocks for swapping
num_cpu_blocks: int = 1000

# Size of each memory block (in tokens)
block_size: int = 16
</code></pre>
<h3 id="model-configuration-1"><a class="header" href="#model-configuration-1">Model Configuration</a></h3>
<pre><code class="language-python"># Data type for model weights and KV-cache
dtype: str = "float16"  # Options: "float16", "float32", "bfloat16"

# Tensor parallelism size
tensor_parallel_size: int = 1

# GPU memory utilization fraction
gpu_memory_utilization: float = 0.9

# CPU swap space in GB
swap_space: int = 4
</code></pre>
<h3 id="generation-configuration"><a class="header" href="#generation-configuration">Generation Configuration</a></h3>
<pre><code class="language-python"># Default maximum tokens to generate per request
default_max_tokens: int = 1024

# Default sampling temperature
default_temperature: float = 1.0

# Default top-p value
default_top_p: float = 1.0
</code></pre>
<h3 id="sglang-specific-features"><a class="header" href="#sglang-specific-features">SGLang-Specific Features</a></h3>
<pre><code class="language-python"># Enable radix attention cache for prefix sharing
enable_radix_cache: bool = True

# Enable chunked prefill for long prompts
enable_chunked_prefill: bool = True

# Scheduling policy: "fcfs" (first-come-first-served)
schedule_policy: str = "fcfs"

# Enable prefix caching
enable_prefix_caching: bool = True

# Maximum scheduling steps before preemption
max_num_schedule_steps: int = 1000
</code></pre>
<h2 id="configuration-loading"><a class="header" href="#configuration-loading">Configuration Loading</a></h2>
<h3 id="default-configuration"><a class="header" href="#default-configuration">Default Configuration</a></h3>
<p>When no explicit configuration is provided, the system uses sensible defaults that work well for most educational purposes:</p>
<ul>
<li>Conservative memory usage to work on most GPUs</li>
<li>Balanced performance settings</li>
<li>Safe batch sizes that avoid out-of-memory errors</li>
</ul>
<h3 id="custom-configuration"><a class="header" href="#custom-configuration">Custom Configuration</a></h3>
<p>Users can customize configurations by:</p>
<ol>
<li><strong>Direct parameter passing</strong> to constructors</li>
<li><strong>Environment variables</strong> for deployment scenarios</li>
<li><strong>Configuration files</strong> (when implemented)</li>
</ol>
<h2 id="configuration-best-practices"><a class="header" href="#configuration-best-practices">Configuration Best Practices</a></h2>
<h3 id="performance-tuning"><a class="header" href="#performance-tuning">Performance Tuning</a></h3>
<p>For production use, consider these configuration adjustments:</p>
<ul>
<li><strong>Increase batch sizes</strong> based on available GPU memory</li>
<li><strong>Adjust block size</strong> for optimal cache utilization</li>
<li><strong>Tune memory pool size</strong> based on request patterns</li>
</ul>
<h3 id="memory-management"><a class="header" href="#memory-management">Memory Management</a></h3>
<p>Configure memory settings based on your hardware:</p>
<pre><code class="language-python"># For high-end GPUs (24GB+ VRAM)
num_blocks = 4000
max_batch_size = 32

# For mid-range GPUs (8-16GB VRAM)
num_blocks = 1000
max_batch_size = 8

# For entry-level GPUs (4-8GB VRAM)
num_blocks = 500
max_batch_size = 4
</code></pre>
<h2 id="integration-with-components"><a class="header" href="#integration-with-components">Integration with Components</a></h2>
<h3 id="engine-integration"><a class="header" href="#engine-integration">Engine Integration</a></h3>
<p>The main engine uses the SGLangConfig for initialization:</p>
<pre><code class="language-python">from src.config import SGLangConfig, get_sglang_config

# Use default config
config = get_sglang_config()

# Or override specific parameters
config = get_sglang_config(
    max_batch_size=16,
    num_gpu_blocks=4000
)

# Initialize components with config values
scheduler = SGLangScheduler(
    max_batch_size=config.max_batch_size,
    max_prefill_batch_size=config.max_prefill_batch_size,
    max_decode_batch_size=config.max_decode_batch_size
)
</code></pre>
<h3 id="scheduler-configuration-1"><a class="header" href="#scheduler-configuration-1">Scheduler Configuration</a></h3>
<p>The SGLang scheduler uses configuration for scheduling policies:</p>
<ul>
<li>Batch size limits</li>
<li>Prefill/decode phase sizing</li>
<li>Memory-aware scheduling decisions</li>
</ul>
<h3 id="memory-manager-configuration"><a class="header" href="#memory-manager-configuration">Memory Manager Configuration</a></h3>
<p>The KV-cache manager uses configuration for:</p>
<ul>
<li>Total memory pool size</li>
<li>Block allocation strategies</li>
<li>Memory optimization policies</li>
</ul>
<h2 id="environment-specific-configuration"><a class="header" href="#environment-specific-configuration">Environment-Specific Configuration</a></h2>
<h3 id="development-configuration"><a class="header" href="#development-configuration">Development Configuration</a></h3>
<p>For development and learning:</p>
<ul>
<li>Conservative memory limits</li>
<li>Detailed logging</li>
<li>Debug information enabled</li>
</ul>
<h3 id="production-configuration"><a class="header" href="#production-configuration">Production Configuration</a></h3>
<p>For production deployment:</p>
<ul>
<li>Optimized batch sizes</li>
<li>Performance-focused settings</li>
<li>Minimal logging overhead</li>
</ul>
<h2 id="configuration-validation"><a class="header" href="#configuration-validation">Configuration Validation</a></h2>
<p>The system validates configuration parameters to prevent:</p>
<ul>
<li>Memory allocation failures</li>
<li>Invalid parameter combinations</li>
<li>Performance-degrading settings</li>
</ul>
<h2 id="future-extensions"><a class="header" href="#future-extensions">Future Extensions</a></h2>
<p>The configuration system is designed to accommodate:</p>
<ul>
<li>Model-specific optimizations</li>
<li>Hardware-aware tuning</li>
<li>Runtime configuration updates</li>
<li>Performance auto-tuning</li>
</ul>
<h2 id="configuration-examples"><a class="header" href="#configuration-examples">Configuration Examples</a></h2>
<h3 id="basic-configuration"><a class="header" href="#basic-configuration">Basic Configuration</a></h3>
<pre><code class="language-python"># Minimal configuration for learning
config = {
    "max_batch_size": 4,
    "num_blocks": 1000,
    "block_size": 16
}
</code></pre>
<h3 id="performance-configuration"><a class="header" href="#performance-configuration">Performance Configuration</a></h3>
<pre><code class="language-python"># Optimized for throughput
config = {
    "max_batch_size": 32,
    "max_decode_batch_size": 512,
    "num_blocks": 4000,
    "block_size": 32
}
</code></pre>
<h2 id="troubleshooting-configuration-issues"><a class="header" href="#troubleshooting-configuration-issues">Troubleshooting Configuration Issues</a></h2>
<h3 id="memory-issues"><a class="header" href="#memory-issues">Memory Issues</a></h3>
<p>If experiencing out-of-memory errors:</p>
<ol>
<li>Reduce <code>num_blocks</code> in KV-cache</li>
<li>Lower batch sizes</li>
<li>Check available GPU memory</li>
</ol>
<h3 id="performance-issues"><a class="header" href="#performance-issues">Performance Issues</a></h3>
<p>If experiencing low throughput:</p>
<ol>
<li>Increase batch sizes</li>
<li>Optimize block size for your model</li>
<li>Verify CUDA availability and compatibility</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="engine-orchestration"><a class="header" href="#engine-orchestration">Engine Orchestration</a></h1>
<h2 id="overview-1"><a class="header" href="#overview-1">Overview</a></h2>
<p>The Inference Engine serves as the main orchestrator of the Mini-YAIE system, coordinating between various components to provide a unified interface for LLM inference. The engine implements SGLang-style continuous batching with radial attention and prefix sharing to maximize efficiency and throughput.</p>
<h2 id="engine-architecture"><a class="header" href="#engine-architecture">Engine Architecture</a></h2>
<p>The main engine is implemented in <code>src/engine.py</code> and follows a modular design pattern where each component is responsible for specific aspects of request processing:</p>
<pre><code>┌─────────────────┐
│   API Layer     │  ← Requests enter here
├─────────────────┤
│ Engine Orchestration │  ← Coordination happens here
├─────────────────┤
│   Scheduler     │  ← Request scheduling
├─────────────────┤
│  Memory Manager │  ← KV-cache management
├─────────────────┤
│  Attention Core │  ← Radial attention computation
├─────────────────┤
│  Model/Kernel   │  ← Forward pass execution
└─────────────────┘
</code></pre>
<h2 id="core-components-1"><a class="header" href="#core-components-1">Core Components</a></h2>
<h3 id="1-model-loading-integration"><a class="header" href="#1-model-loading-integration">1. Model Loading Integration</a></h3>
<p>The engine handles model and tokenizer loading through the ModelLoader component:</p>
<pre><code class="language-python">def __init__(self, model_name: str):
    self.tokenizer: PreTrainedTokenizer = self._load_tokenizer()
    self.model = self._load_model()
</code></pre>
<p>This ensures that models are properly loaded from HuggingFace or local cache with appropriate configuration.</p>
<h3 id="2-sglang-style-scheduler"><a class="header" href="#2-sglang-style-scheduler">2. SGLang-Style Scheduler</a></h3>
<p>The engine integrates with the SGLangScheduler for advanced request scheduling:</p>
<pre><code class="language-python">self.scheduler = SGLangScheduler(
    max_batch_size=8, 
    max_prefill_batch_size=16, 
    max_decode_batch_size=256
)
</code></pre>
<p>The scheduler implements prefix grouping and multi-step processing for computation sharing.</p>
<h3 id="3-radial-attention-system"><a class="header" href="#3-radial-attention-system">3. Radial Attention System</a></h3>
<p>The engine connects to the radial attention mechanism:</p>
<pre><code class="language-python">self.radix_attention = RadixAttentionWithPagedKVCache(
    num_layers=self.model.config.num_hidden_layers,
    num_heads=self.model.config.num_attention_heads,
    head_dim=self.model.config.hidden_size // self.model.config.num_attention_heads,
)
</code></pre>
<h3 id="4-kv-cache-management"><a class="header" href="#4-kv-cache-management">4. KV-Cache Management</a></h3>
<p>The engine manages memory through the KVCacheManager:</p>
<pre><code class="language-python">self.kv_cache_manager = KVCacheManager(
    num_blocks=2000,
    block_size=16,
    num_heads=self.model.config.num_attention_heads,
    head_dim=self.model.config.hidden_size // self.model.config.num_attention_heads,
    dtype=torch.float16,
)
</code></pre>
<h2 id="request-processing-flow"><a class="header" href="#request-processing-flow">Request Processing Flow</a></h2>
<h3 id="1-request-addition"><a class="header" href="#1-request-addition">1. Request Addition</a></h3>
<pre><code class="language-python">def generate(self, prompts: List[str], **kwargs) -&gt; List[str]:
    # Add requests to scheduler
    request_ids = []
    for prompt in prompts:
        req_id = self.scheduler.add_request(prompt, **kwargs)
        request_ids.append(req_id)
</code></pre>
<h3 id="2-generation-loop"><a class="header" href="#2-generation-loop">2. Generation Loop</a></h3>
<p>The engine runs a main generation loop that processes requests:</p>
<pre><code class="language-python">def _run_generation_loop(self, request_ids: List[str]) -&gt; List[str]:
    # Process requests in batches
    # Handle prefill and decode phases
    # Manage KV-cache efficiently
</code></pre>
<h3 id="3-response-generation"><a class="header" href="#3-response-generation">3. Response Generation</a></h3>
<p>The engine generates responses with proper tokenization and formatting:</p>
<pre><code class="language-python"># Generate response using the existing generate method
responses = self.generate([formatted_prompt], **kwargs)
generated_text = responses[0] if responses else ""
</code></pre>
<h2 id="sglang-style-optimization-features"><a class="header" href="#sglang-style-optimization-features">SGLang-Style Optimization Features</a></h2>
<h3 id="1-continuous-batching"><a class="header" href="#1-continuous-batching">1. Continuous Batching</a></h3>
<p>The engine supports continuous batching where requests at different stages can be processed together:</p>
<ul>
<li>Prefill requests (processing full prompts)</li>
<li>Decode requests (generating single tokens)</li>
<li>Mixed batches combining both types</li>
</ul>
<h3 id="2-prefix-sharing"><a class="header" href="#2-prefix-sharing">2. Prefix Sharing</a></h3>
<p>The engine enables computation sharing for requests with common prefixes:</p>
<ul>
<li>Radix tree identifies shared prefixes</li>
<li>Common computations are performed once</li>
<li>Results are shared among multiple requests</li>
</ul>
<h3 id="3-memory-efficiency"><a class="header" href="#3-memory-efficiency">3. Memory Efficiency</a></h3>
<p>The engine optimizes memory usage through:</p>
<ul>
<li>Paged KV-cache management</li>
<li>Block allocation strategies</li>
<li>Memory reclamation for completed requests</li>
</ul>
<h2 id="api-integration"><a class="header" href="#api-integration">API Integration</a></h2>
<h3 id="1-chat-completion-api"><a class="header" href="#1-chat-completion-api">1. Chat Completion API</a></h3>
<p>The engine provides OpenAI-compatible chat completion:</p>
<pre><code class="language-python">def chat_completion(self, messages: List[Dict[str, str]], **kwargs) -&gt; Dict[str, Any]:
    # Format messages using chat template
    # Process through generation pipeline
    # Return in OpenAI format
</code></pre>
<h3 id="2-streaming-support"><a class="header" href="#2-streaming-support">2. Streaming Support</a></h3>
<p>The engine supports streaming responses for real-time applications:</p>
<pre><code class="language-python">def chat_completion_stream(self, messages: List[Dict[str, str]], **kwargs):
    # Generate tokens one by one
    # Yield chunks immediately
    # Maintain OpenAI stream format
</code></pre>
<h2 id="performance-optimization"><a class="header" href="#performance-optimization">Performance Optimization</a></h2>
<h3 id="1-batch-size-management"><a class="header" href="#1-batch-size-management">1. Batch Size Management</a></h3>
<p>The engine dynamically adjusts batch sizes based on available memory and request characteristics:</p>
<ul>
<li>Prefill batches: Optimized for prompt processing</li>
<li>Decode batches: Optimized for token generation</li>
<li>Mixed batches: Balanced between both phases</li>
</ul>
<h3 id="2-memory-management"><a class="header" href="#2-memory-management">2. Memory Management</a></h3>
<p>The engine coordinates memory usage across components:</p>
<pre><code class="language-python"># Connect scheduler to memory manager for optimization
self.scheduler.connect_memory_manager(self.kv_cache_manager)
</code></pre>
<h3 id="3-computation-sharing"><a class="header" href="#3-computation-sharing">3. Computation Sharing</a></h3>
<p>The engine maximizes computation sharing through radix attention:</p>
<ul>
<li>Shared prefix processing</li>
<li>Common token computations</li>
<li>Reduced redundant calculations</li>
</ul>
<h2 id="error-handling-and-resilience"><a class="header" href="#error-handling-and-resilience">Error Handling and Resilience</a></h2>
<h3 id="1-request-validation"><a class="header" href="#1-request-validation">1. Request Validation</a></h3>
<p>The engine validates requests before processing:</p>
<ul>
<li>Input format validation</li>
<li>Parameter range checking</li>
<li>Resource availability verification</li>
</ul>
<h3 id="2-graceful-degradation"><a class="header" href="#2-graceful-degradation">2. Graceful Degradation</a></h3>
<p>When resources are constrained, the engine gracefully degrades:</p>
<ul>
<li>Reduced batch sizes</li>
<li>Fallback mechanisms</li>
<li>Proper error reporting</li>
</ul>
<h3 id="3-resource-management"><a class="header" href="#3-resource-management">3. Resource Management</a></h3>
<p>The engine manages system resources effectively:</p>
<ul>
<li>GPU memory monitoring</li>
<li>Request queue management</li>
<li>Memory cleanup for completed requests</li>
</ul>
<h2 id="integration-points-1"><a class="header" href="#integration-points-1">Integration Points</a></h2>
<h3 id="1-model-interface"><a class="header" href="#1-model-interface">1. Model Interface</a></h3>
<p>The engine interfaces with any HuggingFace-compatible model:</p>
<pre><code class="language-python">outputs = self.model(current_ids)  # Standard HuggingFace model interface
</code></pre>
<h3 id="2-sampling-integration"><a class="header" href="#2-sampling-integration">2. Sampling Integration</a></h3>
<p>The engine uses the sampling kernel for token generation:</p>
<pre><code class="language-python">sampling_kernel = SamplingKernel()
next_token_id = sampling_kernel.sample(
    next_token_logits,
    temperature=request.temperature,
    top_p=request.top_p
)
</code></pre>
<h3 id="3-scheduler-integration"><a class="header" href="#3-scheduler-integration">3. Scheduler Integration</a></h3>
<p>The engine coordinates closely with the scheduler:</p>
<pre><code class="language-python"># Add requests to scheduler
req_id = self.scheduler.add_request(prompt, **kwargs)
# Process in generation loop
responses = self._run_generation_loop(request_ids)
</code></pre>
<h2 id="engine-configuration"><a class="header" href="#engine-configuration">Engine Configuration</a></h2>
<p>The engine supports various configuration options:</p>
<ul>
<li>Model selection and loading</li>
<li>Batch size limits</li>
<li>Memory allocation settings</li>
<li>Performance optimization parameters</li>
</ul>
<h2 id="future-extensions-1"><a class="header" href="#future-extensions-1">Future Extensions</a></h2>
<p>The engine design supports:</p>
<ul>
<li>Additional optimization techniques</li>
<li>New attention mechanisms</li>
<li>Enhanced scheduling algorithms</li>
<li>Advanced memory management strategies</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="scheduler-logic-sglang-style-request-management"><a class="header" href="#scheduler-logic-sglang-style-request-management">Scheduler Logic: SGLang-Style Request Management</a></h1>
<h2 id="overview-2"><a class="header" href="#overview-2">Overview</a></h2>
<p>The SGLang-style scheduler (<code>core/sglang_scheduler.py</code>) implements advanced request scheduling with prefix grouping and computation sharing capabilities. Unlike traditional schedulers, this implementation focuses on maximizing computational efficiency by identifying and leveraging shared prefixes among different requests.</p>
<h2 id="key-concepts"><a class="header" href="#key-concepts">Key Concepts</a></h2>
<h3 id="request-states"><a class="header" href="#request-states">Request States</a></h3>
<p>The scheduler manages requests through several states:</p>
<ul>
<li><strong>PENDING</strong>: New requests awaiting initial processing</li>
<li><strong>SCHEDULED_PREFILL</strong>: Requests ready for prefill phase</li>
<li><strong>RUNNING_PREFILL</strong>: Currently processing full prompts</li>
<li><strong>SCHEDULED_DECODE</strong>: Requests ready for token generation</li>
<li><strong>RUNNING_DECODE</strong>: Currently generating tokens</li>
<li><strong>COMPLETED</strong>: Finished requests</li>
<li><strong>CANCELLED</strong>: Cancelled requests</li>
</ul>
<h3 id="prefix-based-grouping"><a class="header" href="#prefix-based-grouping">Prefix-Based Grouping</a></h3>
<p>The scheduler uses prefix hashing to group requests with common prefixes:</p>
<pre><code class="language-python">def _calculate_prefix_hash(self, prompt: str) -&gt; Optional[str]:
    # Calculate hash to identify common prefixes
    return hashlib.sha256(prompt.encode("utf-8")).hexdigest()
</code></pre>
<h2 id="architecture"><a class="header" href="#architecture">Architecture</a></h2>
<h3 id="core-data-structures"><a class="header" href="#core-data-structures">Core Data Structures</a></h3>
<h4 id="request-management"><a class="header" href="#request-management">Request Management</a></h4>
<pre><code class="language-python"># Separate queues for different processing phases
self.pending_requests: List[Request] = []
self.prefill_requests: List[Request] = []
self.running_prefill: List[Request] = []
self.decode_requests: List[Request] = []
self.running_decode: List[Request] = []
self.completed_requests: List[Request] = []
</code></pre>
<h4 id="prefix-grouping"><a class="header" href="#prefix-grouping">Prefix Grouping</a></h4>
<pre><code class="language-python"># Group requests by common prefixes for shared computation
self.prefix_groups: Dict[str, List[Request]] = defaultdict(list)
self.request_lookup: Dict[str, Request] = {}
</code></pre>
<h3 id="scheduling-strategy"><a class="header" href="#scheduling-strategy">Scheduling Strategy</a></h3>
<p>The scheduler implements a SGLang-inspired strategy:</p>
<ol>
<li><strong>Prioritize Decode Requests</strong>: Minimize token-to-token latency</li>
<li><strong>Maximize Prefill Efficiency</strong>: Process new requests efficiently</li>
<li><strong>Leverage Prefix Sharing</strong>: Share computation for similar requests</li>
<li><strong>Memory-Aware Scheduling</strong>: Respect KV-cache limitations</li>
</ol>
<h2 id="detailed-implementation"><a class="header" href="#detailed-implementation">Detailed Implementation</a></h2>
<h3 id="request-lifecycle"><a class="header" href="#request-lifecycle">Request Lifecycle</a></h3>
<h4 id="1-request-addition-1"><a class="header" href="#1-request-addition-1">1. Request Addition</a></h4>
<pre><code class="language-python">def add_request(self, prompt: str, max_tokens: int = 128, ...) -&gt; str:
    # Calculate prefix hash for grouping
    prefix_hash = self._calculate_prefix_hash(prompt)
    # Add to prefix group if applicable
    if prefix_hash:
        self.prefix_groups[prefix_hash].append(request)
        request.request_group = prefix_hash
</code></pre>
<h4 id="2-scheduling-step"><a class="header" href="#2-scheduling-step">2. Scheduling Step</a></h4>
<pre><code class="language-python">def schedule_step(self) -&gt; Tuple[List[Request], List[Request]]:
    # First, prioritize decode requests
    decode_batch = []
    prefill_batch = []
    
    # Calculate remaining capacity after decode allocation
    remaining_capacity = self.max_batch_size - len(decode_batch)
    
    # Fill remaining capacity with prefill requests
    if remaining_capacity &gt; 0:
        num_prefills = min(len(prefill_candidates), remaining_capacity, self.max_prefill_batch_size)
        prefill_batch = prefill_candidates[:num_prefills]
</code></pre>
<h3 id="batch-selection-policy"><a class="header" href="#batch-selection-policy">Batch Selection Policy</a></h3>
<p>The scheduler implements a multi-level priority system:</p>
<ol>
<li><strong>Decode Priority</strong>: Continue existing generation to minimize latency</li>
<li><strong>Prefill Efficiency</strong>: Process new requests in efficient batches</li>
<li><strong>Memory Management</strong>: Ensure sufficient KV-cache for all requests</li>
<li><strong>Prefix Sharing</strong>: Group similar requests for computation sharing</li>
</ol>
<h3 id="prefill-processing"><a class="header" href="#prefill-processing">Prefill Processing</a></h3>
<p>Prefill requests undergo full prompt processing:</p>
<pre><code class="language-python">def process_prefill_batch(self, requests: List[Request]) -&gt; List[Request]:
    for req in requests:
        # Process full prompt in one forward pass
        req.status = RequestStatus.SCHEDULED_DECODE
        # Initialize output sequence
        if req.output_ids is None:
            req.output_ids = []
</code></pre>
<h3 id="decode-processing"><a class="header" href="#decode-processing">Decode Processing</a></h3>
<p>Decode requests generate tokens one-by-one:</p>
<pre><code class="language-python">def process_decode_batch(self, requests: List[Request]) -&gt; List[Request]:
    for req in requests:
        # Get logits from model (simplified)
        dummy_logits = torch.randn(1, 1000)
        
        # Sample next token using kernel
        next_token_tensor = self.sampling_kernel.sample(
            dummy_logits,
            temperature=req.temperature,
            top_p=req.top_p
        )
        
        # Update position and check termination
        req.current_position += 1
        if req.current_position &gt;= req.max_tokens:
            req.status = RequestStatus.COMPLETED
</code></pre>
<h2 id="sglang-style-optimizations"><a class="header" href="#sglang-style-optimizations">SGLang-Style Optimizations</a></h2>
<h3 id="1-computation-sharing"><a class="header" href="#1-computation-sharing">1. Computation Sharing</a></h3>
<p>The scheduler identifies requests with shared prefixes:</p>
<pre><code class="language-python">def find_shared_prefixes(self, token_ids: List[int]) -&gt; Tuple[List[str], List[int]]:
    # Traverse radix tree to find matching prefixes
    # Return requests that can share computation
</code></pre>
<h3 id="2-memory-aware-scheduling"><a class="header" href="#2-memory-aware-scheduling">2. Memory-Aware Scheduling</a></h3>
<p>The scheduler connects to the memory manager for KV-cache coordination:</p>
<pre><code class="language-python">def connect_memory_manager(self, memory_manager):
    self.memory_manager = memory_manager
</code></pre>
<h3 id="3-continuous-batching"><a class="header" href="#3-continuous-batching">3. Continuous Batching</a></h3>
<p>The scheduler maintains continuous processing by balancing prefill and decode requests:</p>
<ul>
<li>Decode requests have higher priority (latency-sensitive)</li>
<li>Prefill requests fill remaining batch capacity</li>
<li>Memory requirements are considered during scheduling</li>
</ul>
<h2 id="performance-considerations"><a class="header" href="#performance-considerations">Performance Considerations</a></h2>
<h3 id="batch-size-optimization"><a class="header" href="#batch-size-optimization">Batch Size Optimization</a></h3>
<p>The scheduler uses different batch size limits:</p>
<ul>
<li><code>max_prefill_batch_size</code>: Limits prefill batch size for memory efficiency</li>
<li><code>max_decode_batch_size</code>: Larger limit for decode due to smaller memory footprint</li>
<li><code>max_batch_size</code>: Overall system limit</li>
</ul>
<h3 id="memory-management-1"><a class="header" href="#memory-management-1">Memory Management</a></h3>
<p>The scheduler coordinates with the KV-cache manager to:</p>
<ul>
<li>Allocate blocks for new requests</li>
<li>Track memory usage during processing</li>
<li>Ensure sufficient memory for scheduled requests</li>
</ul>
<h2 id="integration-with-other-components"><a class="header" href="#integration-with-other-components">Integration with Other Components</a></h2>
<h3 id="memory-manager-integration"><a class="header" href="#memory-manager-integration">Memory Manager Integration</a></h3>
<pre><code class="language-python">def process_prefill_batch(self, requests: List[Request]) -&gt; List[Request]:
    if self.memory_manager:
        # Allocate KV cache blocks for requests
        pass
</code></pre>
<h3 id="sampling-kernel-integration"><a class="header" href="#sampling-kernel-integration">Sampling Kernel Integration</a></h3>
<pre><code class="language-python">def process_decode_batch(self, requests: List[Request]) -&gt; List[Request]:
    # Use sampling kernel for token selection
    next_token_tensor = self.sampling_kernel.sample(...)
</code></pre>
<h2 id="request-status-monitoring"><a class="header" href="#request-status-monitoring">Request Status Monitoring</a></h2>
<h3 id="queue-status"><a class="header" href="#queue-status">Queue Status</a></h3>
<p>The scheduler provides detailed status information:</p>
<pre><code class="language-python">def get_queue_status(self) -&gt; Dict[str, int]:
    return {
        "pending": len(self.pending_requests),
        "prefill_queue": len(self.prefill_requests),
        "running_prefill": len(self.running_prefill),
        "decode_queue": len(self.decode_requests),
        "running_decode": len(self.running_decode),
        "completed": len(self.completed_requests),
        "total_active": self.get_active_request_count(),
    }
</code></pre>
<h3 id="request-result-access"><a class="header" href="#request-result-access">Request Result Access</a></h3>
<pre><code class="language-python">def get_request_result(self, req_id: str) -&gt; Optional[Dict[str, Any]]:
    # Check completed requests for results
</code></pre>
<h2 id="implementation-challenges"><a class="header" href="#implementation-challenges">Implementation Challenges</a></h2>
<h3 id="1-prefix-hashing"><a class="header" href="#1-prefix-hashing">1. Prefix Hashing</a></h3>
<p>For educational purposes, the implementation uses simple string hashing. In production:</p>
<ul>
<li>Use token ID sequences for more accurate prefix matching</li>
<li>Implement more sophisticated similarity measures</li>
<li>Consider semantic similarity for better grouping</li>
</ul>
<h3 id="2-memory-allocation"><a class="header" href="#2-memory-allocation">2. Memory Allocation</a></h3>
<p>The current implementation shows integration points for memory management. A full implementation would:</p>
<ul>
<li>Calculate precise memory requirements</li>
<li>Implement cache eviction policies</li>
<li>Handle memory fragmentation</li>
</ul>
<h3 id="3-computation-sharing-1"><a class="header" href="#3-computation-sharing-1">3. Computation Sharing</a></h3>
<p>The radix tree integration points exist but require full implementation of:</p>
<ul>
<li>Efficient tree traversal</li>
<li>Shared computation tracking</li>
<li>Result distribution to multiple requests</li>
</ul>
<h2 id="scheduling-algorithm-details"><a class="header" href="#scheduling-algorithm-details">Scheduling Algorithm Details</a></h2>
<h3 id="step-by-step-process"><a class="header" href="#step-by-step-process">Step-by-Step Process</a></h3>
<ol>
<li><strong>Decode Prioritization</strong>: Schedule as many decode requests as possible</li>
<li><strong>Capacity Calculation</strong>: Determine remaining batch capacity</li>
<li><strong>Prefill Scheduling</strong>: Fill remaining capacity with prefill requests</li>
<li><strong>Memory Verification</strong>: Confirm sufficient KV-cache availability</li>
<li><strong>Batch Execution</strong>: Process scheduled requests</li>
</ol>
<h3 id="optimization-strategies"><a class="header" href="#optimization-strategies">Optimization Strategies</a></h3>
<p>The scheduler implements several optimization strategies:</p>
<ol>
<li><strong>Temporal Multiplexing</strong>: Interleave prefill and decode for efficiency</li>
<li><strong>Spatial Multiplexing</strong>: Group similar requests for shared computation</li>
<li><strong>Memory Multiplexing</strong>: Optimize KV-cache usage across requests</li>
</ol>
<h2 id="future-extensions-2"><a class="header" href="#future-extensions-2">Future Extensions</a></h2>
<p>The scheduler design supports:</p>
<ul>
<li>Advanced prefix matching algorithms</li>
<li>Dynamic batch size adjustment</li>
<li>Request preemption and rescheduling</li>
<li>Multi-GPU coordination</li>
<li>Custom scheduling policies</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="memory-management-paged-kv-cache-system"><a class="header" href="#memory-management-paged-kv-cache-system">Memory Management: Paged KV-Cache System</a></h1>
<h2 id="overview-3"><a class="header" href="#overview-3">Overview</a></h2>
<p>The memory management system in Mini-YAIE implements a paged KV-cache mechanism inspired by systems like vLLM and SGLang. This approach addresses the memory fragmentation challenges in LLM inference by using fixed-size memory blocks (pages) that can be allocated and deallocated independently for each request.</p>
<h2 id="core-concepts"><a class="header" href="#core-concepts">Core Concepts</a></h2>
<h3 id="paged-memory-architecture"><a class="header" href="#paged-memory-architecture">Paged Memory Architecture</a></h3>
<p>Traditional KV-cache management allocates contiguous memory blocks for each request, leading to fragmentation when requests have varying lengths. Paged KV-cache solves this by:</p>
<ul>
<li>Dividing the KV-cache into fixed-size blocks (pages)</li>
<li>Allowing requests to use non-contiguous memory blocks</li>
<li>Enabling efficient memory reuse and sharing</li>
</ul>
<h3 id="key-benefits"><a class="header" href="#key-benefits">Key Benefits</a></h3>
<ol>
<li><strong>Reduced Fragmentation</strong>: Fixed-size blocks prevent memory fragmentation</li>
<li><strong>Efficient Memory Utilization</strong>: Unused blocks can be allocated to other requests</li>
<li><strong>Scalability</strong>: Supports variable-length requests without memory waste</li>
<li><strong>Computation Sharing</strong>: Enables shared prefixes to use the same memory blocks</li>
</ol>
<h2 id="architecture-1"><a class="header" href="#architecture-1">Architecture</a></h2>
<h3 id="kvcacheblock-class"><a class="header" href="#kvcacheblock-class">KVCacheBlock Class</a></h3>
<p>Each <code>KVCacheBlock</code> represents a fixed-size memory block:</p>
<pre><code class="language-python">class KVCacheBlock:
    def __init__(self, block_id: int, size: int, num_heads: int, head_dim: int, ...):
        self.block_id = block_id  # Unique identifier for the block
        self.size = size          # Number of tokens this block can hold
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.keys = None          # [size, num_heads, head_dim] tensor
        self.values = None        # [size, num_heads, head_dim] tensor
</code></pre>
<h3 id="kvcachemanager-class"><a class="header" href="#kvcachemanager-class">KVCacheManager Class</a></h3>
<p>The main manager orchestrates all memory operations:</p>
<pre><code class="language-python">class KVCacheManager:
    def __init__(self, num_blocks: int, block_size: int, num_heads: int, head_dim: int, ...):
        self.num_blocks = num_blocks      # Total number of blocks in the pool
        self.block_size = block_size      # Size of each block in tokens
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.blocks: List[KVCacheBlock] = []  # Pool of all blocks
        self.free_block_list: List[int] = []  # Available blocks for allocation
        self.block_tables: dict = {}      # Maps request_id to list of block_ids
</code></pre>
<h2 id="memory-management-operations"><a class="header" href="#memory-management-operations">Memory Management Operations</a></h2>
<h3 id="1-block-allocation"><a class="header" href="#1-block-allocation">1. Block Allocation</a></h3>
<p>When a request needs KV-cache memory, the manager allocates the required number of blocks:</p>
<pre><code class="language-python">def allocate_blocks(self, request_id: str, num_tokens: int) -&gt; List[int]:
    # Calculate required blocks
    num_blocks_needed = (num_tokens + self.block_size - 1) // self.block_size
    
    # Check availability
    if len(self.free_block_list) &lt; num_blocks_needed:
        raise RuntimeError("Not enough free blocks")
    
    # Allocate from free list
    allocated_block_ids = []
    for _ in range(num_blocks_needed):
        block_id = self.free_block_list.pop(0)  # Remove from free list
        allocated_block_ids.append(block_id)
        self.blocks[block_id].allocate()  # Allocate GPU memory
    
    # Track allocation
    self.block_tables[request_id] = allocated_block_ids
    return allocated_block_ids
</code></pre>
<p><strong>Key Aspects:</strong></p>
<ul>
<li>Calculates minimum blocks needed based on token count and block size</li>
<li>Ensures sufficient free blocks before allocation</li>
<li>Updates free block list and request tracking</li>
<li>Actually allocates GPU memory for the blocks</li>
</ul>
<h3 id="2-block-deallocation"><a class="header" href="#2-block-deallocation">2. Block Deallocation</a></h3>
<p>When requests complete, their blocks are returned to the free pool:</p>
<pre><code class="language-python">def free_blocks(self, request_id: str):
    if request_id in self.block_tables:
        block_ids = self.block_tables[request_id]
        self.free_block_list.extend(block_ids)  # Return to free pool
        self.free_block_list.sort()  # Maintain sorted order
        del self.block_tables[request_id]  # Remove tracking entry
        
        # Optionally clear tensors to free GPU memory
        for block_id in block_ids:
            self.blocks[block_id].keys = None
            self.blocks[block_id].values = None
</code></pre>
<p><strong>Key Aspects:</strong></p>
<ul>
<li>Returns blocks to the free list for reuse</li>
<li>Maintains sorted order for allocation efficiency</li>
<li>Removes request tracking information</li>
<li>Optionally clears GPU tensors to free memory</li>
</ul>
<h3 id="3-block-copying"><a class="header" href="#3-block-copying">3. Block Copying</a></h3>
<p>For advanced operations like request preemption or memory defragmentation:</p>
<pre><code class="language-python">def copy_blocks(self, src_block_ids: List[int], dst_block_ids: List[int]):
    if len(src_block_ids) != len(dst_block_ids):
        raise ValueError("Source and destination lists must have same length")
    
    for src_id, dst_id in zip(src_block_ids, dst_block_ids):
        src_block = self.blocks[src_id]
        dst_block = self.blocks[dst_id]
        
        # Allocate destination if needed
        if dst_block.keys is None or dst_block.values is None:
            dst_block.allocate()
        
        # Copy data
        with torch.no_grad():
            dst_block.keys.copy_(src_block.keys)
            dst_block.values.copy_(src_block.values)
</code></pre>
<h2 id="memory-layout-and-access"><a class="header" href="#memory-layout-and-access">Memory Layout and Access</a></h2>
<h3 id="block-organization"><a class="header" href="#block-organization">Block Organization</a></h3>
<p>The memory is organized as a collection of fixed-size blocks:</p>
<pre><code>Global Memory Pool:
┌─────────┬─────────┬─────────┬─────────┬─────────┬─────────┐
│ Block 0 │ Block 1 │ Block 2 │ Block 3 │ Block 4 │ Block 5 │
│ [16xHxD]│ [16xHxD]│ [16xHxD]│ [16xHxD]│ [16xHxD]│ [16xHxD]│
└─────────┴─────────┴─────────┴─────────┴─────────┴─────────┘

Request A: Uses [Block 0, Block 2] for non-contiguous sequence storage
Request B: Uses [Block 1, Block 4, Block 5] for its sequence
</code></pre>
<p>Where H = num_heads and D = head_dim</p>
<h3 id="block-tables"><a class="header" href="#block-tables">Block Tables</a></h3>
<p>Each request has an associated block table that maps logical token positions to physical blocks:</p>
<pre><code>Request A Block Table:
Logical: [0-15][16-31][32-47][48-63][64-79]
Physical: Block0 Block2  -     -     Block4
</code></pre>
<h2 id="integration-with-sglang-features"><a class="header" href="#integration-with-sglang-features">Integration with SGLang Features</a></h2>
<h3 id="computation-sharing"><a class="header" href="#computation-sharing">Computation Sharing</a></h3>
<p>The paged system enables computation sharing by allowing requests with shared prefixes to reference the same memory blocks:</p>
<ul>
<li>Requests with common prefixes can share the same KV-cache blocks</li>
<li>Multiple requests can reference the same physical memory location</li>
<li>Reduces redundant computation and memory usage</li>
</ul>
<h3 id="memory-efficiency-1"><a class="header" href="#memory-efficiency-1">Memory Efficiency</a></h3>
<p>By using fixed-size blocks:</p>
<ul>
<li>Memory fragmentation is eliminated</li>
<li>Block reuse is maximized</li>
<li>Memory utilization approaches optimal levels</li>
</ul>
<h2 id="performance-considerations-1"><a class="header" href="#performance-considerations-1">Performance Considerations</a></h2>
<h3 id="block-size-selection"><a class="header" href="#block-size-selection">Block Size Selection</a></h3>
<p>The block size parameter is critical for performance:</p>
<ul>
<li><strong>Smaller blocks</strong>: Less internal fragmentation, more overhead for block management</li>
<li><strong>Larger blocks</strong>: More internal fragmentation, less management overhead</li>
<li><strong>Typical values</strong>: 8-32 tokens per block work well in practice</li>
</ul>
<h3 id="memory-allocation-strategy"><a class="header" href="#memory-allocation-strategy">Memory Allocation Strategy</a></h3>
<p>The system uses a simple first-fit strategy:</p>
<pre><code class="language-python"># Allocate from beginning of free list
block_id = self.free_block_list.pop(0)
</code></pre>
<p>For production systems, more sophisticated strategies might include:</p>
<ul>
<li>Best-fit to minimize fragmentation</li>
<li>Coalescing strategies to combine blocks</li>
<li>Preallocation to reduce allocation overhead</li>
</ul>
<h2 id="memory-safety-and-management"><a class="header" href="#memory-safety-and-management">Memory Safety and Management</a></h2>
<h3 id="gpu-memory-management"><a class="header" href="#gpu-memory-management">GPU Memory Management</a></h3>
<p>The system ensures proper GPU memory allocation:</p>
<pre><code class="language-python">def allocate(self):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    self.keys = torch.zeros(
        self.size, self.num_heads, self.head_dim, 
        dtype=self.dtype, device=device
    )
    self.values = torch.zeros(
        self.size, self.num_heads, self.head_dim, 
        dtype=self.dtype, device=device
    )
</code></pre>
<h3 id="memory-cleanup"><a class="header" href="#memory-cleanup">Memory Cleanup</a></h3>
<p>Proper cleanup prevents memory leaks:</p>
<ul>
<li>Free blocks when requests complete</li>
<li>Clear GPU tensors to release memory</li>
<li>Maintain consistent state in block tables</li>
</ul>
<h2 id="advanced-features"><a class="header" href="#advanced-features">Advanced Features</a></h2>
<h3 id="dynamic-block-resizing"><a class="header" href="#dynamic-block-resizing">Dynamic Block Resizing</a></h3>
<p>For requests that need to extend beyond their initial allocation:</p>
<ul>
<li>Allocate additional blocks as needed</li>
<li>Maintain logical sequence continuity</li>
<li>Update block tables accordingly</li>
</ul>
<h3 id="memory-pool-management"><a class="header" href="#memory-pool-management">Memory Pool Management</a></h3>
<p>Advanced implementations might include:</p>
<ul>
<li>Block migration to reduce fragmentation</li>
<li>Eviction policies for memory-constrained scenarios</li>
<li>Prefetching strategies for better performance</li>
</ul>
<h2 id="error-handling-1"><a class="header" href="#error-handling-1">Error Handling</a></h2>
<h3 id="out-of-memory-conditions"><a class="header" href="#out-of-memory-conditions">Out of Memory Conditions</a></h3>
<p>The system handles memory exhaustion gracefully:</p>
<pre><code class="language-python">if len(self.free_block_list) &lt; num_blocks_needed:
    raise RuntimeError(f"Not enough free blocks. Need {num_blocks_needed}, have {len(self.free_block_list)}")
</code></pre>
<h3 id="block-validation"><a class="header" href="#block-validation">Block Validation</a></h3>
<p>Before operations, the system validates block states:</p>
<ul>
<li>Verify blocks are allocated before accessing</li>
<li>Check for proper tensor dimensions</li>
<li>Validate request associations</li>
</ul>
<h2 id="future-enhancements"><a class="header" href="#future-enhancements">Future Enhancements</a></h2>
<h3 id="memory-optimization-1"><a class="header" href="#memory-optimization-1">Memory Optimization</a></h3>
<p>Potential improvements include:</p>
<ul>
<li>Compressed KV-cache storage</li>
<li>Offloading to CPU memory when possible</li>
<li>Cache eviction policies for long-running requests</li>
</ul>
<h3 id="performance-optimization-1"><a class="header" href="#performance-optimization-1">Performance Optimization</a></h3>
<p>Advanced techniques might include:</p>
<ul>
<li>Block prefetching for better cache performance</li>
<li>Heterogeneous memory management (different memory types)</li>
<li>Asynchronous memory operations</li>
</ul>
<h2 id="implementation-variations"><a class="header" href="#implementation-variations">Implementation Variations</a></h2>
<h3 id="sglang-style-memory-management"><a class="header" href="#sglang-style-memory-management">SGLang-Style Memory Management</a></h3>
<p>For SGLang-specific optimizations:</p>
<ul>
<li>Prefix sharing memory management</li>
<li>Radix tree integration for shared computation</li>
<li>Advanced scheduling based on memory access patterns</li>
</ul>
<h3 id="integration-points-2"><a class="header" href="#integration-points-2">Integration Points</a></h3>
<p>The memory manager connects with other components:</p>
<ul>
<li>Scheduler: Provides memory availability information</li>
<li>Attention modules: Access KV-cache through block tables</li>
<li>Model execution: Uses paged cache for efficient attention computation</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="python-kernels-guide"><a class="header" href="#python-kernels-guide">Python Kernels Guide</a></h1>
<h2 id="overview-4"><a class="header" href="#overview-4">Overview</a></h2>
<p>The Python kernels in Mini-YAIE implement the core computational components that enable SGLang-style inference optimization. These kernels provide the foundational functionality for attention mechanisms, memory management, and token sampling that make efficient LLM inference possible.</p>
<h2 id="kernel-architecture"><a class="header" href="#kernel-architecture">Kernel Architecture</a></h2>
<h3 id="core-components-2"><a class="header" href="#core-components-2">Core Components</a></h3>
<p>The kernel system consists of several interconnected modules:</p>
<ol>
<li><strong>Radix Tree</strong>: Implements prefix matching for shared computation</li>
<li><strong>KV Cache Manager</strong>: Manages paged key-value storage</li>
<li><strong>Radix Attention Module</strong>: Implements radial attention with shared computation</li>
<li><strong>Sampling Module</strong>: Provides token selection algorithms</li>
</ol>
<h3 id="sglang-style-optimization-1"><a class="header" href="#sglang-style-optimization-1">SGLang-Style Optimization</a></h3>
<p>The kernels are designed to support SGLang’s key optimization strategies:</p>
<ul>
<li><strong>Prefix Sharing</strong>: Share computation for requests with common prefixes</li>
<li><strong>Continuous Batching</strong>: Dynamically batch requests at different processing stages</li>
<li><strong>Paged Memory Management</strong>: Efficiently manage KV-cache memory using fixed-size blocks</li>
<li><strong>Radial Attention</strong>: Optimize attention computation for shared prefixes</li>
</ul>
<h2 id="python-kernel-implementation"><a class="header" href="#python-kernel-implementation">Python Kernel Implementation</a></h2>
<h3 id="design-philosophy"><a class="header" href="#design-philosophy">Design Philosophy</a></h3>
<p>The Python kernels follow these design principles:</p>
<h4 id="1-educational-focus"><a class="header" href="#1-educational-focus">1. Educational Focus</a></h4>
<ul>
<li>Clean, well-documented code</li>
<li>Clear algorithm implementation</li>
<li>Comprehensive comments explaining concepts</li>
</ul>
<h4 id="2-sglang-compatibility"><a class="header" href="#2-sglang-compatibility">2. SGLang Compatibility</a></h4>
<ul>
<li>Implement SGLang-style optimization techniques</li>
<li>Support for radial attention and prefix sharing</li>
<li>Continuous batching integration</li>
</ul>
<h4 id="3-modularity"><a class="header" href="#3-modularity">3. Modularity</a></h4>
<ul>
<li>Independent components that can be tested individually</li>
<li>Clean interfaces between components</li>
<li>Easy to extend and modify</li>
</ul>
<h4 id="4-performance-considerations"><a class="header" href="#4-performance-considerations">4. Performance Considerations</a></h4>
<ul>
<li>Efficient data structures</li>
<li>Proper memory management</li>
<li>Optimized algorithm implementations</li>
</ul>
<h3 id="implementation-structure"><a class="header" href="#implementation-structure">Implementation Structure</a></h3>
<p>Each kernel follows a similar pattern:</p>
<pre><code class="language-python">class KernelName:
    def __init__(self, parameters):
        # Initialize kernel with configuration
        pass
    
    def process(self, input_data):
        # Core processing logic
        pass
    
    def update_state(self, new_data):
        # State management for ongoing requests
        pass
</code></pre>
<h2 id="integration-with-system-components"><a class="header" href="#integration-with-system-components">Integration with System Components</a></h2>
<h3 id="engine-integration-1"><a class="header" href="#engine-integration-1">Engine Integration</a></h3>
<p>The kernels integrate seamlessly with the main inference engine:</p>
<pre><code class="language-python"># Engine uses kernels for computation
self.radix_attention = RadixAttentionWithPagedKVCache(...)
self.kv_cache_manager = KVCacheManager(...)
self.sampling_kernel = SamplingKernel()
</code></pre>
<h3 id="scheduler-coordination"><a class="header" href="#scheduler-coordination">Scheduler Coordination</a></h3>
<p>Kernels work with the SGLang scheduler:</p>
<ul>
<li>Provide computation sharing opportunities</li>
<li>Manage memory allocation and deallocation</li>
<li>Coordinate with scheduling policies</li>
</ul>
<h3 id="memory-management-2"><a class="header" href="#memory-management-2">Memory Management</a></h3>
<p>Kernels connect with the paged memory system:</p>
<ul>
<li>Request memory allocation through the manager</li>
<li>Manage KV-cache blocks efficiently</li>
<li>Support for shared memory blocks</li>
</ul>
<h2 id="performance-characteristics"><a class="header" href="#performance-characteristics">Performance Characteristics</a></h2>
<h3 id="computational-efficiency"><a class="header" href="#computational-efficiency">Computational Efficiency</a></h3>
<p>The Python kernels provide:</p>
<ul>
<li>Efficient attention computation</li>
<li>Optimized memory access patterns</li>
<li>Shared computation for common prefixes</li>
</ul>
<h3 id="memory-usage"><a class="header" href="#memory-usage">Memory Usage</a></h3>
<p>Optimized memory management includes:</p>
<ul>
<li>Paged cache allocation</li>
<li>Block-level memory sharing</li>
<li>Efficient reuse of allocated blocks</li>
</ul>
<h3 id="scalability"><a class="header" href="#scalability">Scalability</a></h3>
<p>The kernel design supports:</p>
<ul>
<li>Variable batch sizes</li>
<li>Multiple concurrent requests</li>
<li>Scaled performance with more requests</li>
</ul>
<h2 id="advanced-features-1"><a class="header" href="#advanced-features-1">Advanced Features</a></h2>
<h3 id="computation-sharing-1"><a class="header" href="#computation-sharing-1">Computation Sharing</a></h3>
<p>The radix tree and attention modules enable:</p>
<ul>
<li>Shared prefix identification</li>
<li>Computation reuse across requests</li>
<li>Efficient memory utilization</li>
</ul>
<h3 id="adaptive-processing"><a class="header" href="#adaptive-processing">Adaptive Processing</a></h3>
<p>Kernels adapt to:</p>
<ul>
<li>Different request patterns</li>
<li>Variable sequence lengths</li>
<li>Changing memory requirements</li>
</ul>
<h2 id="testing-and-validation"><a class="header" href="#testing-and-validation">Testing and Validation</a></h2>
<h3 id="unit-testing"><a class="header" href="#unit-testing">Unit Testing</a></h3>
<p>Each kernel includes:</p>
<ul>
<li>Comprehensive unit tests</li>
<li>Edge case validation</li>
<li>Performance benchmarking</li>
</ul>
<h3 id="integration-testing"><a class="header" href="#integration-testing">Integration Testing</a></h3>
<p>Kernels are tested as part of:</p>
<ul>
<li>Full inference pipeline</li>
<li>SGLang-style optimization scenarios</li>
<li>Memory management validation</li>
</ul>
<h2 id="extensibility"><a class="header" href="#extensibility">Extensibility</a></h2>
<h3 id="adding-new-kernels"><a class="header" href="#adding-new-kernels">Adding New Kernels</a></h3>
<p>The system supports:</p>
<ul>
<li>Easy addition of new kernel types</li>
<li>Pluggable architecture for kernel replacement</li>
<li>Backwards compatibility</li>
</ul>
<h3 id="customization"><a class="header" href="#customization">Customization</a></h3>
<p>Kernels can be customized for:</p>
<ul>
<li>Specific model architectures</li>
<li>Hardware optimization</li>
<li>Performance tuning</li>
</ul>
<p>This Python kernel system forms the computational backbone of Mini-YAIE, implementing SGLang-style optimization techniques in an educational and accessible way.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="radix-tree-kernelsradix_treepy"><a class="header" href="#radix-tree-kernelsradix_treepy">Radix Tree (<code>kernels/radix_tree.py</code>)</a></h1>
<h2 id="1-concept-the-prefix-tree"><a class="header" href="#1-concept-the-prefix-tree">1. Concept: The Prefix Tree</a></h2>
<p>A <strong>Radix Tree</strong> (or Compressed Trie) is a data structure that succinctly stores sequences of tokens. Unlike a standard trie where each edge is a single character (or token), a Radix Tree allows edges to be <strong>sequences</strong> of tokens.</p>
<h3 id="optimization-goal"><a class="header" href="#optimization-goal">Optimization Goal</a></h3>
<p>When two requests start with <code>"The quick brown fox"</code>, we want to store that sequence <strong>once</strong>.</p>
<ul>
<li><strong>Request A</strong>: <code>"The quick brown fox jumps"</code></li>
<li><strong>Request B</strong>: <code>"The quick brown fox sleeps"</code></li>
</ul>
<p>In our tree, we should have a shared node for <code>"The quick brown fox"</code>, which then branches into <code>"jumps"</code> and <code>"sleeps"</code>.</p>
<pre><code class="language-mermaid">flowchart TD
    Root((Root)) --&gt; Shared["The quick brown fox"]
    Shared --&gt; Branch1["jumps"]
    Shared --&gt; Branch2["sleeps"]

    style Shared fill:#aaffaa
</code></pre>
<hr>
<h2 id="2-implementation-guide"><a class="header" href="#2-implementation-guide">2. Implementation Guide</a></h2>
<p>Open <code>src/kernels/radix_tree.py</code>. You will implement the <code>RadixTree</code> class step-by-step.</p>
<h3 id="step-1-define-the-tree-node"><a class="header" href="#step-1-define-the-tree-node">Step 1: Define the Tree Node</a></h3>
<p>First, we need a node structure. Unlike a binary tree, a Radix Node can have many children.</p>
<pre><code class="language-python">class RadixTreeNode:
    def __init__(self, prefix: List[int]):
        self.prefix = prefix               # The sequence of tokens on this edge
        self.children: Dict[int, RadixTreeNode] = {} # Map: first_token -&gt; Child Node
        self.request_id: Optional[str] = None # If a request ends here, store its ID
        self.lock_count = 0                # Reference counting (how many requests use this?)
</code></pre>
<p><strong>Task</strong>: Locate the <code>RadixTreeNode</code> class and ensure it has these fields.</p>
<hr>
<h3 id="step-2-implement-match_prefix"><a class="header" href="#step-2-implement-match_prefix">Step 2: Implement <code>match_prefix</code></a></h3>
<p>Before inserting, we need a way to see how much of a new prompt <em>already exists</em> in the tree.</p>
<p><strong>Algorithm</strong>:</p>
<ol>
<li>Start at <code>self.root</code>.</li>
<li>Compare the input <code>token_ids</code> with the edges in the tree.</li>
<li>Traverse down as long as the tokens match exactly.</li>
<li>Return the last matching Node and the number of matching tokens.</li>
</ol>
<p><strong>Your Turn</strong>:
Implement <code>find_shared_prefixes(token_ids)</code> in <code>RadixTree</code>.</p>
<blockquote>
<p><em>Hint</em>: Use a while loop. At each node, look at <code>node.children[token_ids[current_idx]]</code>. If it exists, check if the full edge <code>child.prefix</code> matches the next chunk of your input.</p>
</blockquote>
<hr>
<h3 id="step-3-implement-insert_request-the-hard-part"><a class="header" href="#step-3-implement-insert_request-the-hard-part">Step 3: Implement <code>insert_request</code> (The Hard Part)</a></h3>
<p>Now, inserting a new request. This involves <strong>splitting</strong> nodes if a partial match is found.</p>
<p><strong>Scenario</strong>:</p>
<ul>
<li>Tree has edge <code>[1, 2, 3, 4]</code>.</li>
<li>You insert <code>[1, 2, 5]</code>.</li>
</ul>
<p><strong>Algorithm</strong>:</p>
<ol>
<li>Trace the path like in Step 2.</li>
<li>If you differ in the <em>middle</em> of an edge (e.g., matched <code>1, 2</code> but tree has <code>3</code>, you have <code>5</code>):
<ul>
<li><strong>Split</strong>: Create a new parent node for <code>[1, 2]</code>.</li>
<li>Make the old node <code>[3, 4]</code> a child of this new parent.</li>
<li>Create your new node <code>[5]</code> as another child.</li>
</ul>
</li>
</ol>
<pre><code class="language-mermaid">flowchart TD
    Start([Insert 1, 2, 5]) --&gt; Match{Match 1, 2?}
    Match --&gt;|Yes| Diverge{Next is 3 vs 5}
    Diverge --&gt; Split[Split Edge]
    Split --&gt; Old[Child: 3, 4]
    Split --&gt; New[Child: 5]
</code></pre>
<p><strong>Your Turn</strong>:
Implement <code>insert_request(request_id, token_ids)</code>.</p>
<ul>
<li>Use your <code>match_prefix</code> logic helper.</li>
<li>Handle the 3 cases: Exact match, New Branch, or Split Edge.</li>
</ul>
<hr>
<h3 id="step-4-verify"><a class="header" href="#step-4-verify">Step 4: Verify</a></h3>
<p>Create a test script <code>tests/test_radix_manual.py</code>:</p>
<pre><code class="language-python">tree = RadixTree()
tree.insert_request("req1", [1, 2, 3])
match, count = tree.find_shared_prefixes([1, 2, 3, 4])
print(f"Matched {count} tokens") # Should be 3!
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="kv-cache-manager-kernelskv_cachepy"><a class="header" href="#kv-cache-manager-kernelskv_cachepy">KV Cache Manager (<code>kernels/kv_cache.py</code>)</a></h1>
<h2 id="1-concept-paged-attention"><a class="header" href="#1-concept-paged-attention">1. Concept: Paged Attention</a></h2>
<p>In a standard implementation, KV Cache is a huge contiguous tensor <code>[MAX_SEQ_LEN, HEADS, DIM]</code>. This wastes memory because most prompts are shorter than <code>MAX_SEQ_LEN</code>.</p>
<p><strong>Paged Attention</strong> breaks this tensor into small fixed-size blocks (e.g., size 16).</p>
<ul>
<li><strong>Physical Memory</strong>: A big pool of blocks <code>[NUM_BLOCKS, 16, HEADS, DIM]</code>.</li>
<li><strong>Logical Memory</strong>: For each request, we just keep a list of block indices <code>[0, 5, 12]</code>.</li>
</ul>
<p>Your job is to write the <strong>Allocator</strong> (like <code>malloc</code> in C).</p>
<hr>
<h2 id="2-implementation-guide-1"><a class="header" href="#2-implementation-guide-1">2. Implementation Guide</a></h2>
<p>Open <code>src/kernels/kv_cache.py</code>.</p>
<h3 id="step-1-initialization"><a class="header" href="#step-1-initialization">Step 1: Initialization</a></h3>
<p>We need to track which blocks are free and which are used.</p>
<p><strong>Task</strong>: In <code>__init__</code>:</p>
<ol>
<li>Create a list <code>self.free_blocks</code>. Initially, it should contain <em>all</em> integers from <code>0</code> to <code>num_blocks - 1</code>.</li>
<li>Create a dictionary <code>self.block_tables</code>. This will map <code>request_id -&gt; List[int]</code> (the list of blocks owned by that request).</li>
</ol>
<pre><code class="language-python"># Hint
self.free_blocks = list(range(num_blocks))
</code></pre>
<hr>
<h3 id="step-2-the-allocate_blocks-method"><a class="header" href="#step-2-the-allocate_blocks-method">Step 2: The <code>allocate_blocks</code> Method</a></h3>
<p>When a request comes in (or generates new tokens), it needs memory.</p>
<p><strong>Signature</strong>:</p>
<pre><code class="language-python">def allocate_blocks(self, request_id: str, num_tokens: int) -&gt; List[int]:
</code></pre>
<p><strong>Algorithm</strong>:</p>
<ol>
<li>Calculate how many blocks are needed.
<ul>
<li>$N_{blocks} = \lceil num_tokens / block_size \rceil$</li>
</ul>
</li>
<li>Check if we have enough <code>free_blocks</code>.
<ul>
<li>If <code>len(free_blocks) &lt; needed</code>, raise an Error (or handle OOM).</li>
</ul>
</li>
<li><strong>Pop</strong> the blocks from <code>free_blocks</code>.</li>
<li><strong>Assign</strong> them to <code>self.block_tables[request_id]</code>.</li>
<li>Return the list of allocated block indices.</li>
</ol>
<p><strong>Your Turn</strong>: Implement this logic. Watch out for integer division!</p>
<hr>
<h3 id="step-3-the-free_blocks-method"><a class="header" href="#step-3-the-free_blocks-method">Step 3: The <code>free_blocks</code> Method</a></h3>
<p>When a request finishes, we must reclaim memory.</p>
<p><strong>Algorithm</strong>:</p>
<ol>
<li>Look up the blocks for <code>request_id</code>.</li>
<li><strong>Append</strong> them back to <code>self.free_blocks</code>.</li>
<li>Delete the entry from <code>self.block_tables</code>.</li>
</ol>
<p><strong>Critical</strong>: Do not double-free! (Though Python sets make this easier, a list is faster for standard stacks).</p>
<hr>
<h3 id="step-4-connecting-to-the-engine"><a class="header" href="#step-4-connecting-to-the-engine">Step 4: Connecting to the Engine</a></h3>
<p>The <code>get_kv_tensors</code> method is checking if you can translate the “Logical” view to the “Physical” view.</p>
<p><strong>Task</strong>: Implement <code>get_kv_tensors</code>.</p>
<ul>
<li>It should presumably return the specific GPU tensors for the blocks.</li>
<li><em>Note</em>: In this Python simulation, just returning the indices is often enough for the Scheduler to know mapping. The actual <em>Tensor</em> access happens in the CUDA kernel.</li>
</ul>
<hr>
<h3 id="step-5-verify"><a class="header" href="#step-5-verify">Step 5: Verify</a></h3>
<p>Create <code>tests/test_kv_manual.py</code>:</p>
<pre><code class="language-python">manager = KVCacheManager(num_blocks=10, block_size=16, ...)
# Alloc 20 tokens -&gt; needs 2 blocks (indices 0, 1)
blocks = manager.allocate_blocks("req1", 20)
print(blocks)
# Free
manager.free_blocks("req1")
print(len(manager.free_blocks)) # Should be 10 again
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="radix-attention-module-kernelsradix_attentionpy"><a class="header" href="#radix-attention-module-kernelsradix_attentionpy">Radix Attention Module (<code>kernels/radix_attention.py</code>)</a></h1>
<h2 id="1-concept-connecting-the-dots"><a class="header" href="#1-concept-connecting-the-dots">1. Concept: Connecting the Dots</a></h2>
<p>We have a <strong>Radix Tree</strong> (prefix matching) and a <strong>Paged KV Cache</strong> (memory management). The <code>RadixAttentionWithPagedKVCache</code> class is the glue that runs on the CPU (Python side) to manage these resources before we launch the GPU kernels.</p>
<p>It doesn’t run the attention <em>math</em> (that’s the CUDA kernel’s job). Instead, it manages the <strong>metadata</strong>:</p>
<ul>
<li>“Request A needs to append ‘cat’ to its sequence.”</li>
<li>“Does ‘cat’ already exist in the Radix Tree?”</li>
<li>“If yes, reuse the block.”</li>
<li>“If no, allocate a new block.”</li>
</ul>
<hr>
<h2 id="2-implementation-guide-2"><a class="header" href="#2-implementation-guide-2">2. Implementation Guide</a></h2>
<p>Open <code>src/kernels/radix_attention.py</code>.</p>
<h3 id="step-1-initialization-1"><a class="header" href="#step-1-initialization-1">Step 1: Initialization</a></h3>
<p>You need to initialize the two sub-components we built earlier.</p>
<pre><code class="language-python">class RadixAttentionWithPagedKVCache:
    def __init__(self, ...):
        # ...
        self.radix_tree = RadixTree()
        self.kv_cache_manager = KVCacheManager(...)
</code></pre>
<h3 id="step-2-append_slot-the-critical-logic"><a class="header" href="#step-2-append_slot-the-critical-logic">Step 2: <code>append_slot</code> (The Critical Logic)</a></h3>
<p>This method is called when we want to add a new token (or tokens) to a request.</p>
<p><strong>Signature</strong>:</p>
<pre><code class="language-python">def append_slot(self, key: torch.Tensor, value: torch.Tensor, request_id: str):
</code></pre>
<ul>
<li><code>key</code>/<code>value</code>: The computed K/V tensors for the <em>new</em> token(s).</li>
</ul>
<p><strong>Algorithm</strong>:</p>
<ol>
<li><strong>Check Tree</strong>: Use <code>self.radix_tree</code> to see if this <code>(request_id + new_token)</code> path already exists?
<ul>
<li><em>Note</em>: In a real system, we check <em>before</em> computing K/V. Here, we might just be managing the cache storage.</li>
</ul>
</li>
<li><strong>Allocate</strong>: If we need new space, call <code>self.kv_cache_manager.allocate_blocks()</code>.</li>
<li><strong>Store</strong>: We need to perform the copy.
<ul>
<li>Ideally, we just return the <em>indices</em> of where to write, and the GPU kernel does the writing.</li>
<li>For this Python simulation, you might simulate the copy or just track the metadata.</li>
</ul>
</li>
</ol>
<h3 id="step-3-get_kv_cache"><a class="header" href="#step-3-get_kv_cache">Step 3: <code>get_kv_cache</code></a></h3>
<p>The scheduler asks: “I am about to run requests <code>[R1, R2]</code>. Where is their data?”</p>
<p><strong>Algorithm</strong>:</p>
<ol>
<li>Loop through <code>request_ids</code>.</li>
<li>For each, ask <code>self.kv_cache_manager</code> for its block table (list of integers).</li>
<li>Pack these lists into a single Tensor <code>block_tables</code>.</li>
<li>Return <code>block_tables</code> to the Engine.</li>
</ol>
<h3 id="step-4-free_request"><a class="header" href="#step-4-free_request">Step 4: <code>free_request</code></a></h3>
<p>When a request is done:</p>
<ol>
<li><code>self.radix_tree.remove_request(request_id)</code> (Decrement ref counts).</li>
<li><code>self.kv_cache_manager.free_blocks(request_id)</code> (Reclaim memory).</li>
</ol>
<hr>
<h2 id="3-the-radixattentionblock-model-layer"><a class="header" href="#3-the-radixattentionblock-model-layer">3. The <code>RadixAttentionBlock</code> (Model Layer)</a></h2>
<p>The class <code>RadixAttentionBlock</code> is the PyTorch module that sits in the model.</p>
<p><strong>Task</strong>:
In <code>forward()</code>:</p>
<ol>
<li>Compute Q, K, V projections.</li>
<li>Compute RoPE (Rotary Embeddings).</li>
<li><strong>If Prefill</strong>: Use Flash Attention (or a standard attention) on the new tokens.</li>
<li><strong>If Decode</strong>:
<ul>
<li>Call <code>append_slot</code> to save the new K/V.</li>
<li>Call <code>paged_attention_kernel</code> (the CUDA op) to attend to the <em>entire</em> history using the block tables.</li>
</ul>
</li>
</ol>
<p><strong>Exercise</strong>:
Since we don’t have the full model weight loading for this specific block, focus on the <strong>logic flow</strong> in the comments.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="sampling-kernelssamplingpy"><a class="header" href="#sampling-kernelssamplingpy">Sampling (<code>kernels/sampling.py</code>)</a></h1>
<h2 id="1-concept-from-logits-to-tokens"><a class="header" href="#1-concept-from-logits-to-tokens">1. Concept: From Logits to Tokens</a></h2>
<p>The model outputs <strong>logits</strong>: a vector of size <code>[VOCAB_SIZE]</code> (e.g., 50,000) containing raw scores for the next token.
We need to pick <strong>one</strong> token ID.</p>
<ul>
<li><strong>Greedy</strong>: Just pick <code>argmax()</code>. Boring, repetitive.</li>
<li><strong>Sampling</strong>: Pick randomly based on probability. Creative!</li>
</ul>
<p>We control the randomness with <strong>Temperature</strong>, <strong>Top-P</strong> (Nucleus), and <strong>Top-K</strong>.</p>
<hr>
<h2 id="2-implementation-guide-3"><a class="header" href="#2-implementation-guide-3">2. Implementation Guide</a></h2>
<p>Open <code>src/kernels/sampling.py</code>.</p>
<h3 id="step-1-temperature-scaling"><a class="header" href="#step-1-temperature-scaling">Step 1: Temperature Scaling</a></h3>
<p><strong>Temperature</strong> ($T$) controls confidence.</p>
<ul>
<li>$T &lt; 1$: Makes peakier (more confident).</li>
<li>$T &gt; 1$: Makes flatter (more random).</li>
</ul>
<p><strong>Algorithm</strong>:</p>
<pre><code class="language-python">logits = logits / temperature
</code></pre>
<ul>
<li><strong>Watch out</strong>: If $T$ is very close to 0, just do <code>argmax</code> to avoid division by zero!</li>
</ul>
<hr>
<h3 id="step-2-softmax"><a class="header" href="#step-2-softmax">Step 2: Softmax</a></h3>
<p>Convert logits to probabilities (0.0 to 1.0).</p>
<pre><code class="language-python">probs = torch.softmax(logits, dim=-1)
</code></pre>
<hr>
<h3 id="step-3-top-k-filtering"><a class="header" href="#step-3-top-k-filtering">Step 3: Top-K Filtering</a></h3>
<p>Keep only the $K$ most likely tokens. Zero out the rest.</p>
<p><strong>Algorithm</strong>:</p>
<ol>
<li>Find the value of the $K$-th highest score.</li>
<li>Mask (set to $-\infty$) anything below that value in <code>logits</code> (or 0 in <code>probs</code>).</li>
<li>Re-normalize probabilities.</li>
</ol>
<hr>
<h3 id="step-4-top-p-nucleus-filtering-the-tricky-one"><a class="header" href="#step-4-top-p-nucleus-filtering-the-tricky-one">Step 4: Top-P (Nucleus) Filtering (The Tricky One)</a></h3>
<p>Keep the smallest set of tokens whose cumulative probability adds up to $P$ (e.g., 0.9). This dynamically truncates the long tail of “nonsense” words.</p>
<p><strong>Algorithm</strong>:</p>
<ol>
<li>Sort probabilities in descending order: <code>sorted_probs, sorted_indices = torch.sort(probs, descending=True)</code>.</li>
<li>Compute cumulative sum: <code>cumulative_probs = torch.cumsum(sorted_probs, dim=-1)</code>.</li>
<li>Find cut-off: Mask where <code>cumulative_probs &gt; top_p</code>.
<ul>
<li><em>Tip</em>: You want to include the <em>first</em> token that crosses the threshold. So shift the mask right by one.</li>
</ul>
</li>
<li>Scatter the mask back to the original ordering.</li>
<li>Re-normalize.</li>
</ol>
<hr>
<h3 id="step-5-the-final-selection"><a class="header" href="#step-5-the-final-selection">Step 5: The Final Selection</a></h3>
<p>Once you have your clean probability distribution:</p>
<pre><code class="language-python">next_token = torch.multinomial(probs, num_samples=1)
</code></pre>
<p><strong>Your Turn</strong>: Implement <code>sample</code> in <code>SamplingKernel</code>. Start simple (just Temperature) and verify, then add Top-P.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="build-system-and-cuda-kernels"><a class="header" href="#build-system-and-cuda-kernels">Build System and CUDA Kernels</a></h1>
<h2 id="overview-5"><a class="header" href="#overview-5">Overview</a></h2>
<p>The Mini-YAIE project includes a comprehensive build system for compiling custom CUDA kernels that provide optimized performance for SGLang-style inference operations. The build system is designed to be both educational and production-ready, allowing users to learn CUDA kernel development while achieving high performance.</p>
<h2 id="build-system-architecture"><a class="header" href="#build-system-architecture">Build System Architecture</a></h2>
<h3 id="build-scripts"><a class="header" href="#build-scripts">Build Scripts</a></h3>
<p>The project provides multiple ways to build kernels:</p>
<h4 id="shell-script-build_kernelssh"><a class="header" href="#shell-script-build_kernelssh">Shell Script: <code>build_kernels.sh</code></a></h4>
<pre><code class="language-bash">#!/bin/bash
# Comprehensive build script for CUDA kernels
./build_kernels.sh
</code></pre>
<h4 id="makefile-integration"><a class="header" href="#makefile-integration">Makefile Integration</a></h4>
<pre><code class="language-makefile"># Build kernels using make
make build-kernels
</code></pre>
<h4 id="direct-python-build"><a class="header" href="#direct-python-build">Direct Python Build</a></h4>
<pre><code class="language-bash"># Direct build using Python setup
python setup_kernels.py build_ext --inplace
</code></pre>
<h3 id="build-dependencies"><a class="header" href="#build-dependencies">Build Dependencies</a></h3>
<p>The build system requires:</p>
<ul>
<li><strong>CUDA Toolkit</strong>: Version 11.0 or higher</li>
<li><strong>PyTorch with CUDA Support</strong>: For CUDA extensions</li>
<li><strong>NVIDIA GPU</strong>: With compute capability &gt;= 6.0</li>
<li><strong>System Compiler</strong>: GCC/Clang with C++14 support</li>
<li><strong>Python Development Headers</strong>: For Python C API</li>
</ul>
<h2 id="cuda-kernel-design"><a class="header" href="#cuda-kernel-design">CUDA Kernel Design</a></h2>
<h3 id="sglang-style-optimization-focus"><a class="header" href="#sglang-style-optimization-focus">SGLang-Style Optimization Focus</a></h3>
<p>The CUDA kernels are specifically designed to optimize SGLang-style inference operations:</p>
<ol>
<li><strong>Radix Tree Operations</strong>: Efficient prefix matching on GPU</li>
<li><strong>Paged Attention</strong>: Optimized attention for paged KV-cache</li>
<li><strong>Memory Operations</strong>: High-performance memory management</li>
<li><strong>Radix Attention</strong>: GPU-optimized radial attention with prefix sharing</li>
</ol>
<h3 id="performance-goals"><a class="header" href="#performance-goals">Performance Goals</a></h3>
<p>The kernels target SGLang-specific optimizations:</p>
<ul>
<li><strong>Memory Bandwidth Optimization</strong>: Minimize memory access overhead</li>
<li><strong>Computation Sharing</strong>: Implement efficient prefix sharing on GPU</li>
<li><strong>Batch Processing</strong>: Optimize for variable-length batch processing</li>
<li><strong>Cache Efficiency</strong>: Optimize for GPU cache hierarchies</li>
</ul>
<h2 id="cuda-kernel-components"><a class="header" href="#cuda-kernel-components">CUDA Kernel Components</a></h2>
<h3 id="1-memory-operations-kernels"><a class="header" href="#1-memory-operations-kernels">1. Memory Operations Kernels</a></h3>
<h4 id="paged-kv-cache-operations"><a class="header" href="#paged-kv-cache-operations">Paged KV-Cache Operations</a></h4>
<pre><code class="language-cuda">// Efficient operations on paged key-value cache
__global__ void copy_blocks_kernel(
    float* dst_keys, float* dst_values,
    float* src_keys, float* src_values,
    int* block_mapping, int num_blocks
);
</code></pre>
<h4 id="block-management"><a class="header" href="#block-management">Block Management</a></h4>
<ul>
<li>Block allocation and deallocation</li>
<li>Memory copying between blocks</li>
<li>Block state management</li>
</ul>
<h3 id="2-flash-attention-kernels"><a class="header" href="#2-flash-attention-kernels">2. Flash Attention Kernels</a></h3>
<h4 id="optimized-attention-computation"><a class="header" href="#optimized-attention-computation">Optimized Attention Computation</a></h4>
<pre><code class="language-cuda">// Optimized attention for both prefill and decode phases
template&lt;typename T&gt;
__global__ void flash_attention_kernel(
    const T* q, const T* k, const T* v,
    T* output, float* lse,  // logsumexp for numerical stability
    int num_heads, int head_dim, int seq_len
);
</code></pre>
<h4 id="features"><a class="header" href="#features">Features</a></h4>
<ul>
<li>Memory-efficient attention computation</li>
<li>Numerical stability with logsumexp</li>
<li>Support for variable sequence lengths</li>
<li>Optimized memory access patterns</li>
</ul>
<h3 id="3-paged-attention-kernels"><a class="header" href="#3-paged-attention-kernels">3. Paged Attention Kernels</a></h3>
<h4 id="paged-memory-access"><a class="header" href="#paged-memory-access">Paged Memory Access</a></h4>
<pre><code class="language-cuda">// Attention with paged key-value cache support
__global__ void paged_attention_kernel(
    float* output,
    const float* query,
    const float* key_cache,
    const float* value_cache,
    const int* block_tables,
    const int* context_lens,
    int num_kv_heads, int head_dim, int block_size
);
</code></pre>
<h4 id="features-1"><a class="header" href="#features-1">Features</a></h4>
<ul>
<li>Direct paged cache access patterns</li>
<li>Efficient block index computation</li>
<li>Memory coalescing optimization</li>
<li>Support for shared prefix computation</li>
</ul>
<h3 id="4-radix-operations-kernels"><a class="header" href="#4-radix-operations-kernels">4. Radix Operations Kernels</a></h3>
<h4 id="prefix-matching-operations"><a class="header" href="#prefix-matching-operations">Prefix Matching Operations</a></h4>
<pre><code class="language-cuda">// GPU-accelerated prefix matching for SGLang-style sharing
__global__ void radix_tree_lookup_kernel(
    int* token_ids, int* request_ids,
    int* prefix_matches, int batch_size
);
</code></pre>
<h4 id="features-2"><a class="header" href="#features-2">Features</a></h4>
<ul>
<li>Parallel prefix matching</li>
<li>Efficient tree traversal on GPU</li>
<li>Shared computation identification</li>
<li>Batch processing optimization</li>
</ul>
<h2 id="build-process-details"><a class="header" href="#build-process-details">Build Process Details</a></h2>
<h3 id="setup-configuration"><a class="header" href="#setup-configuration">Setup Configuration</a></h3>
<p>The build system uses PyTorch’s <code>setup.py</code> for CUDA extension compilation:</p>
<pre><code class="language-python"># setup_kernels.py
from torch.utils.cpp_extension import setup, CUDAExtension

setup(
    name="yaie_kernels",
    ext_modules=[
        CUDAExtension(
            name="yaie_kernels",
            sources=[
                "kernels/cuda/radix_attention.cu",
                "kernels/cuda/paged_attention.cu",
                "kernels/cuda/memory_ops.cu",
                "kernels/cuda/radix_ops.cu"
            ],
            extra_compile_args={
                "cxx": ["-O3"],
                "nvcc": ["-O3", "--use_fast_math"]
            }
        )
    ],
    cmdclass={"build_ext": torch.utils.cpp_extension.BuildExtension}
)
</code></pre>
<h3 id="compilation-flags"><a class="header" href="#compilation-flags">Compilation Flags</a></h3>
<p>The build system uses optimization flags for performance:</p>
<ul>
<li><code>-O3</code>: Maximum optimization</li>
<li><code>--use_fast_math</code>: Fast math operations</li>
<li><code>-arch=sm_60</code>: Target specific GPU architectures</li>
<li><code>-lineinfo</code>: Include debug line information</li>
</ul>
<h3 id="architecture-targeting"><a class="header" href="#architecture-targeting">Architecture Targeting</a></h3>
<p>The system supports multiple GPU architectures:</p>
<pre><code class="language-bash"># Specify target architecture during build
python setup_kernels.py build_ext --inplace --arch=75  # Turing GPUs
python setup_kernels.py build_ext --inplace --arch=80  # Ampere GPUs
</code></pre>
<h2 id="cuda-kernel-implementation-guidelines"><a class="header" href="#cuda-kernel-implementation-guidelines">CUDA Kernel Implementation Guidelines</a></h2>
<h3 id="memory-management-3"><a class="header" href="#memory-management-3">Memory Management</a></h3>
<h4 id="unified-memory-vs-regular-memory"><a class="header" href="#unified-memory-vs-regular-memory">Unified Memory vs Regular Memory</a></h4>
<pre><code class="language-cuda">// Use unified memory for easier management (if available)
cudaMallocManaged(&amp;ptr, size);

// Or regular device memory for better performance
cudaMalloc(&amp;ptr, size);
</code></pre>
<h4 id="memory-pooling"><a class="header" href="#memory-pooling">Memory Pooling</a></h4>
<ul>
<li>Implement memory pooling for frequently allocated objects</li>
<li>Reuse memory blocks across operations</li>
<li>Batch memory operations when possible</li>
</ul>
<h3 id="thread-organization"><a class="header" href="#thread-organization">Thread Organization</a></h3>
<h4 id="block-and-grid-sizing"><a class="header" href="#block-and-grid-sizing">Block and Grid Sizing</a></h4>
<pre><code class="language-cuda">// Optimize for your specific algorithm
dim3 blockSize(256);
dim3 gridSize((N + blockSize.x - 1) / blockSize.x);
</code></pre>
<h4 id="warp-level-primitives"><a class="header" href="#warp-level-primitives">Warp-Level Primitives</a></h4>
<ul>
<li>Use warp-level operations for better efficiency</li>
<li>Align memory accesses with warp boundaries</li>
<li>Minimize warp divergence</li>
</ul>
<h3 id="synchronization"><a class="header" href="#synchronization">Synchronization</a></h3>
<h4 id="cooperative-groups"><a class="header" href="#cooperative-groups">Cooperative Groups</a></h4>
<pre><code class="language-cuda">#include &lt;cooperative_groups.h&gt;
using namespace cooperative_groups;

// Use cooperative groups for complex synchronization
thread_block block = this_thread_block();
</code></pre>
<h4 id="memory-barriers"><a class="header" href="#memory-barriers">Memory Barriers</a></h4>
<ul>
<li>Use appropriate memory barriers for consistency</li>
<li>Minimize unnecessary synchronization overhead</li>
</ul>
<h2 id="performance-optimization-strategies"><a class="header" href="#performance-optimization-strategies">Performance Optimization Strategies</a></h2>
<h3 id="memory-bandwidth-optimization"><a class="header" href="#memory-bandwidth-optimization">Memory Bandwidth Optimization</a></h3>
<h4 id="coalesced-access"><a class="header" href="#coalesced-access">Coalesced Access</a></h4>
<pre><code class="language-cuda">// Ensure memory accesses are coalesced
int tid = blockIdx.x * blockDim.x + threadIdx.x;
// Access data[tid] by threads in order for coalescing
</code></pre>
<h4 id="shared-memory-usage"><a class="header" href="#shared-memory-usage">Shared Memory Usage</a></h4>
<ul>
<li>Use shared memory for frequently accessed data</li>
<li>Implement tiling strategies for large operations</li>
<li>Minimize global memory access</li>
</ul>
<h3 id="computation-optimization"><a class="header" href="#computation-optimization">Computation Optimization</a></h3>
<h4 id="warp-level-operations"><a class="header" href="#warp-level-operations">Warp-Level Operations</a></h4>
<ul>
<li>Leverage warp-level primitives when possible</li>
<li>Use vectorized operations (float4, int4)</li>
<li>Minimize thread divergence within warps</li>
</ul>
<h3 id="kernel-fusion"><a class="header" href="#kernel-fusion">Kernel Fusion</a></h3>
<h4 id="combined-operations"><a class="header" href="#combined-operations">Combined Operations</a></h4>
<ul>
<li>Fuse multiple operations into single kernels</li>
<li>Reduce kernel launch overhead</li>
<li>Improve memory locality</li>
</ul>
<h2 id="integration-with-python"><a class="header" href="#integration-with-python">Integration with Python</a></h2>
<h3 id="pytorch-extensions"><a class="header" href="#pytorch-extensions">PyTorch Extensions</a></h3>
<p>The CUDA kernels integrate with PyTorch using extensions:</p>
<pre><code class="language-python">import torch
import yaie_kernels  # Compiled extension

# Use kernel from Python
result = yaie_kernels.radix_attention_forward(
    query, key, value, 
    radix_tree_info, 
    attention_mask
)
</code></pre>
<h3 id="automatic-gpu-management"><a class="header" href="#automatic-gpu-management">Automatic GPU Management</a></h3>
<p>The integration handles:</p>
<ul>
<li>GPU memory allocation</li>
<li>Device synchronization</li>
<li>Error propagation</li>
<li>Backpropagation support</li>
</ul>
<h2 id="error-handling-and-debugging"><a class="header" href="#error-handling-and-debugging">Error Handling and Debugging</a></h2>
<h3 id="build-time-errors"><a class="header" href="#build-time-errors">Build-Time Errors</a></h3>
<p>Common build issues and solutions:</p>
<pre><code class="language-bash"># CUDA toolkit not found
export CUDA_HOME=/usr/local/cuda

# Architecture mismatch
# Check GPU compute capability and adjust flag

# Missing PyTorch
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
</code></pre>
<h3 id="runtime-error-checking"><a class="header" href="#runtime-error-checking">Runtime Error Checking</a></h3>
<p>Kernels should include error checking:</p>
<pre><code class="language-cuda">#define CUDA_CHECK(call) \
    do { \
        cudaError_t err = call; \
        if (err != cudaSuccess) { \
            fprintf(stderr, "CUDA error at %s:%d - %s\n", __FILE__, __LINE__, \
                    cudaGetErrorString(err)); \
            exit(1); \
        } \
    } while(0)
</code></pre>
<h2 id="testing-and-validation-1"><a class="header" href="#testing-and-validation-1">Testing and Validation</a></h2>
<h3 id="kernel-testing"><a class="header" href="#kernel-testing">Kernel Testing</a></h3>
<p>Test kernels with:</p>
<ul>
<li>Unit tests for individual functions</li>
<li>Integration tests with Python interface</li>
<li>Performance benchmarks</li>
<li>Memory correctness validation</li>
</ul>
<h3 id="sglang-specific-tests"><a class="header" href="#sglang-specific-tests">SGLang-Specific Tests</a></h3>
<p>Test SGLang optimization features:</p>
<ul>
<li>Prefix sharing correctness</li>
<li>Memory management validation</li>
<li>Performance gain verification</li>
<li>Edge case handling</li>
</ul>
<h2 id="development-workflow"><a class="header" href="#development-workflow">Development Workflow</a></h2>
<h3 id="iterative-development"><a class="header" href="#iterative-development">Iterative Development</a></h3>
<p>The development process includes:</p>
<ol>
<li><strong>Kernel Design</strong>: Design algorithm for GPU execution</li>
<li><strong>Implementation</strong>: Write CUDA kernel code</li>
<li><strong>Building</strong>: Compile with build system</li>
<li><strong>Testing</strong>: Validate correctness and performance</li>
<li><strong>Optimization</strong>: Profile and optimize based on results</li>
</ol>
<h3 id="profiling-and-optimization"><a class="header" href="#profiling-and-optimization">Profiling and Optimization</a></h3>
<p>Use NVIDIA tools for optimization:</p>
<ul>
<li><strong>Nsight Systems</strong>: Overall system profiling</li>
<li><strong>Nsight Compute</strong>: Detailed kernel analysis</li>
<li><strong>nvprof</strong>: Legacy profiling tool</li>
</ul>
<h2 id="future-extensions-3"><a class="header" href="#future-extensions-3">Future Extensions</a></h2>
<h3 id="advanced-features-2"><a class="header" href="#advanced-features-2">Advanced Features</a></h3>
<p>Potential kernel enhancements:</p>
<ul>
<li>Quantized attention operations</li>
<li>Sparse attention kernels</li>
<li>Custom activation functions</li>
<li>Advanced memory management</li>
</ul>
<h3 id="hardware-support"><a class="header" href="#hardware-support">Hardware Support</a></h3>
<p>Expand support for:</p>
<ul>
<li>Different GPU architectures</li>
<li>Multi-GPU operations</li>
<li>Heterogeneous computing</li>
<li>Tensor Core optimizations</li>
</ul>
<p>This build system and CUDA kernel architecture enables Mini-YAIE to achieve SGLang-style performance optimizations while maintaining educational value and extensibility.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="memory-operations-kernelscudamemory_opscu"><a class="header" href="#memory-operations-kernelscudamemory_opscu">Memory Operations (<code>kernels/cuda/memory_ops.cu</code>)</a></h1>
<h2 id="concept"><a class="header" href="#concept">Concept</a></h2>
<p>Moving data between different GPU memory locations is a frequent operation in Paged Attention.</p>
<h2 id="implementation-goal"><a class="header" href="#implementation-goal">Implementation Goal</a></h2>
<p>Implement <code>copy_blocks_kernel</code>:</p>
<h3 id="signature"><a class="header" href="#signature">Signature</a></h3>
<pre><code class="language-cpp">void copy_blocks_kernel(
    torch::Tensor key_cache,      // [num_blocks, block_size, head_dim]
    torch::Tensor value_cache,    // [num_blocks, block_size, head_dim]
    torch::Tensor block_mapping,  // [num_mappings, 2] (src, dst)
    int num_mappings
);
</code></pre>
<h3 id="logic"><a class="header" href="#logic">Logic</a></h3>
<ol>
<li><strong>Parallelism</strong>: Launch one thread per token to copy.</li>
<li><strong>Indexing</strong>:
<ul>
<li><code>mapping_idx = blockIdx.x</code></li>
<li><code>src_block = block_mapping[mapping_idx][0]</code></li>
<li><code>dst_block = block_mapping[mapping_idx][1]</code></li>
</ul>
</li>
<li><strong>Copy</strong>:
<ul>
<li>Read <code>key/value</code> from <code>src_block</code> at <code>threadIdx</code> offset.</li>
<li>Write to <code>dst_block</code>.</li>
</ul>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="flash-attention-kernelscudaflash_attentioncu"><a class="header" href="#flash-attention-kernelscudaflash_attentioncu">Flash Attention (<code>kernels/cuda/flash_attention.cu</code>)</a></h1>
<h2 id="1-concept-memory-bandwidth"><a class="header" href="#1-concept-memory-bandwidth">1. Concept: Memory Bandwidth</a></h2>
<p>The main bottleneck in Attention is reading the huge $N \times N$ matrix from memory.
<strong>Flash Attention</strong> breaks the problem into small “tiles” that fit into the GPU’s fast <strong>SRAM</strong> (Shared Memory). We compute everything for that tile without going back to slow Global Memory.</p>
<pre><code class="language-mermaid">graph TB
    subgraph GlobalMemory[Global Memory HBM]
        Q[Matrix Q]
        K[Matrix K]
        V[Matrix V]
    end

    subgraph SRAM[Shared Memory SRAM]
        TileQ[Tile Q]
        TileK[Tile K]
        TileV[Tile V]
        Comp(("Compute QK^T * V"))
    end

    Q --&gt; TileQ
    K --&gt; TileK
    V --&gt; TileV

    TileQ --&gt; Comp
    TileK --&gt; Comp
    TileV --&gt; Comp
</code></pre>
<hr>
<h2 id="2-implementation-guide-4"><a class="header" href="#2-implementation-guide-4">2. Implementation Guide</a></h2>
<p>We will implement a <strong>simplified</strong> version. Doing full FlashAttention v2 is extremely complex. We aim for “Tiled Attention”.</p>
<h3 id="step-0-the-setup"><a class="header" href="#step-0-the-setup">Step 0: The Setup</a></h3>
<p>Open <code>src/kernels/cuda/flash_attention.cu</code>.
Identify the <code>flash_attention_forward</code> function.</p>
<p>You have pointers to:</p>
<ul>
<li><code>query</code> (Q), <code>key</code> (K), <code>value</code> (V) residing in Global Memory.</li>
</ul>
<h3 id="step-1-define-thread-layout"><a class="header" href="#step-1-define-thread-layout">Step 1: Define Thread Layout</a></h3>
<p>We want to process tiles.</p>
<ul>
<li><strong>Grid</strong>: One block per query chunk.</li>
<li><strong>Block</strong>: Threads within the block handle individual heads or elements.</li>
</ul>
<pre><code class="language-cpp">// Example
dim3 grid(num_batches, num_heads);
dim3 block(128); // 128 threads work together on one head
</code></pre>
<h3 id="step-2-load-tiles-to-shared-memory"><a class="header" href="#step-2-load-tiles-to-shared-memory">Step 2: Load Tiles to Shared Memory</a></h3>
<p>You need <code>__shared__</code> memory arrays.</p>
<pre><code class="language-cpp">__shared__ float s_Q[TILE_SIZE][HEAD_DIM];
__shared__ float s_K[TILE_SIZE][HEAD_DIM];
</code></pre>
<p>Use <code>threadIdx.x</code> to cooperatively load data from Global <code>Q</code> to Shared <code>s_Q</code>.
<strong>Remember</strong>: call <code>__syncthreads()</code> after loading!</p>
<h3 id="step-3-compute-qkt-scores"><a class="header" href="#step-3-compute-qkt-scores">Step 3: Compute $QK^T$ (Scores)</a></h3>
<p>Iterate over your shared Q and K.
Calculate the dot product.
Store in a register (local variable).</p>
<h3 id="step-4-softmax-the-online-trick"><a class="header" href="#step-4-softmax-the-online-trick">Step 4: Softmax (The “Online” Trick)</a></h3>
<p>In standard softmax, you need the max of the <em>entire</em> row. Here we only see a tile!
<strong>Trick</strong>: Keep a running max ($m$) and running sum ($l$). Update them as you see new tiles.</p>
<ul>
<li>$m_{new} = \max(m_{old}, \max(current_tile))$</li>
<li>Adjust previous sums by multiplying by $e^{m_{old} - m_{new}}$.</li>
</ul>
<h3 id="step-5-compute-score-times-v"><a class="header" href="#step-5-compute-score-times-v">Step 5: Compute Score $\times$ V</a></h3>
<p>Once you have the probabilities for the tile, multiply by <code>s_V</code> (which you also loaded).
Accumulate into <code>output</code>.</p>
<hr>
<h2 id="3-hints"><a class="header" href="#3-hints">3. Hints</a></h2>
<ul>
<li>Start with a <strong>Naive</strong> kernel first! Forget shared memory. Just loops.
<ul>
<li>Thread per query token.</li>
<li>Loop over all key tokens.</li>
<li>Compute.</li>
<li>This is $O(N^2)$ memory reads but verifies your logic is correct.</li>
</ul>
</li>
<li>Only optimize to Shared Memory once logic works.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="paged-attention-kernelscudapaged_attentioncu"><a class="header" href="#paged-attention-kernelscudapaged_attentioncu">Paged Attention (<code>kernels/cuda/paged_attention.cu</code>)</a></h1>
<h2 id="1-concept-indirection"><a class="header" href="#1-concept-indirection">1. Concept: Indirection</a></h2>
<p>Paged Attention is just standard attention, but <code>K</code> and <code>V</code> are not contiguous.
We have to “gather” them using a Page Table.</p>
<pre><code class="language-mermaid">graph LR
    Thread --&gt;|1. Get Logical idx| Logic[Token #42]
    Logic --&gt;|2. Lookup Table| Table[Block 2, Offset 10]
    Table --&gt;|3. Get Physical Addr| Phys[0xA000...]
    Phys --&gt;|4. Read| Data[Value]
</code></pre>
<hr>
<h2 id="2-implementation-guide-5"><a class="header" href="#2-implementation-guide-5">2. Implementation Guide</a></h2>
<h3 id="step-1-understand-the-block-table"><a class="header" href="#step-1-understand-the-block-table">Step 1: Understand the Block Table</a></h3>
<p>You are passed <code>block_tables</code> tensor of shape <code>[num_seqs, max_blocks]</code>.</p>
<ul>
<li>It holds integer indices of physical blocks.</li>
<li><code>block_tables[req_id][0]</code> is the first block of that request.</li>
</ul>
<h3 id="step-2-calculate-physical-address"><a class="header" href="#step-2-calculate-physical-address">Step 2: Calculate Physical Address</a></h3>
<p>Inside your kernel, you want the Key vector for token <code>t</code> of request <code>r</code>.</p>
<pre><code class="language-cpp">int block_idx = t / BLOCK_SIZE;
int block_offset = t % BLOCK_SIZE;
int physical_block_number = block_tables[r][block_idx];

// Pointer arithmetic
float* k_ptr = key_cache_base
             + physical_block_number * (BLOCK_SIZE * HEAD_DIM * NUM_HEADS)
             + ... // navigate to specific head and offset
</code></pre>
<h3 id="step-3-load-data"><a class="header" href="#step-3-load-data">Step 3: Load Data</a></h3>
<p>Using the pointer <code>k_ptr</code>, load the vector into registers or shared memory.</p>
<h3 id="step-4-compute-attention"><a class="header" href="#step-4-compute-attention">Step 4: Compute Attention</a></h3>
<p>Once loaded, the math is identical to standard Attention or Flash Attention.
$Q \cdot K^T$, Softmax, $\cdot V$.</p>
<hr>
<h2 id="3-your-task"><a class="header" href="#3-your-task">3. Your Task</a></h2>
<p>Implement <code>paged_attention_kernel</code> in <code>src/kernels/cuda/paged_attention.cu</code>.</p>
<ol>
<li>Focus on the <strong>address calculation</strong> logic. That is the only difference!</li>
<li>Use the <code>copy_blocks</code> kernel (Memory Ops) to help set up test data if needed.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="radix-operations-kernelscudaradix_opscu"><a class="header" href="#radix-operations-kernelscudaradix_opscu">Radix Operations (<code>kernels/cuda/radix_ops.cu</code>)</a></h1>
<h2 id="concept-1"><a class="header" href="#concept-1">Concept</a></h2>
<p>If we have a Radix Tree, we can optimize attention even further by knowing exactly which tokens are shared.</p>
<h2 id="implementation-goal-1"><a class="header" href="#implementation-goal-1">Implementation Goal</a></h2>
<p>This is an advanced extension.</p>
<h3 id="logic-1"><a class="header" href="#logic-1">Logic</a></h3>
<ol>
<li><strong>Tree Traversal on GPU</strong>: Mapping the Radix Tree structure to a GPU-friendly format (e.g., flattened arrays).</li>
<li><strong>Prefix Matching</strong>: A kernel that takes a batch of prompts and quickly identifies the longest common prefix node ID for each.</li>
</ol>
<p><em>Note: In the simplified version, this logic is often kept in CPU (Python) and only the KV indices are passed to the GPU.</em></p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="kernels-implementation-guide"><a class="header" href="#kernels-implementation-guide">Kernels Implementation Guide</a></h1>
<h2 id="overview-6"><a class="header" href="#overview-6">Overview</a></h2>
<p>This guide provides a comprehensive walkthrough for implementing the core kernels in Mini-YAIE that enable SGLang-style inference optimization. The implementation focuses on three key areas:</p>
<ol>
<li><strong>Python Implementations</strong>: Educational implementations of core algorithms</li>
<li><strong>CUDA Kernels</strong>: Performance-optimized GPU implementations</li>
<li><strong>Integration</strong>: Connecting kernels with the main inference engine</li>
</ol>
<h2 id="implementation-roadmap"><a class="header" href="#implementation-roadmap">Implementation Roadmap</a></h2>
<h3 id="phase-1-core-python-kernels"><a class="header" href="#phase-1-core-python-kernels">Phase 1: Core Python Kernels</a></h3>
<p>Implement the educational Python versions first:</p>
<ol>
<li>Radix tree for prefix matching</li>
<li>Basic attention mechanisms</li>
<li>KV-cache management</li>
<li>Sampling algorithms</li>
</ol>
<h3 id="phase-2-cuda-kernel-development"><a class="header" href="#phase-2-cuda-kernel-development">Phase 2: CUDA Kernel Development</a></h3>
<p>Develop optimized GPU versions:</p>
<ol>
<li>Memory operations kernels</li>
<li>Paged attention implementation</li>
<li>Flash attention optimization</li>
<li>Radix operations acceleration</li>
</ol>
<h3 id="phase-3-integration-and-optimization"><a class="header" href="#phase-3-integration-and-optimization">Phase 3: Integration and Optimization</a></h3>
<p>Connect kernels to the main system:</p>
<ol>
<li>Engine integration</li>
<li>Performance validation</li>
<li>Correctness verification</li>
</ol>
<h2 id="python-kernel-implementation-1"><a class="header" href="#python-kernel-implementation-1">Python Kernel Implementation</a></h2>
<h3 id="1-radix-tree-implementation"><a class="header" href="#1-radix-tree-implementation">1. Radix Tree Implementation</a></h3>
<p>Start with the radix tree that enables prefix sharing:</p>
<p><strong>File</strong>: <code>src/kernels/radix_tree.py</code></p>
<pre><code class="language-python">class RadixTreeNode:
    def __init__(self, token_id: Optional[int] = None):
        self.token_id = token_id
        self.children: Dict[int, "RadixTreeNode"] = {}
        self.request_ids: List[str] = []
        self.kv_cache_refs: List[str] = []
        self.is_terminal = False

class RadixTree:
    def __init__(self):
        self.root = RadixTreeNode()
        self.request_to_path: Dict[str, List[int]] = {}
        self.path_to_node: Dict[str, RadixTreeNode] = {}
    
    def insert_request(self, request_id: str, token_ids: List[int]):
        """Insert a request into the radix tree based on its token sequence"""
        current = self.root
        for token_id in token_ids:
            if token_id not in current.children:
                current.children[token_id] = RadixTreeNode(token_id)
            current = current.children[token_id]
            if request_id not in current.request_ids:
                current.request_ids.append(request_id)
        current.is_terminal = True
        self.request_to_path[request_id] = token_ids
        path_str = self._path_to_string(token_ids)
        self.path_to_node[path_str] = current
    
    def find_shared_prefixes(self, token_ids: List[int]) -&gt; Tuple[List[str], int]:
        """Find requests that share prefixes with the given token sequence"""
        current = self.root
        matched_requests = []
        prefix_length = 0
        
        for i, token_id in enumerate(token_ids):
            if token_id in current.children:
                current = current.children[token_id]
                matched_requests.extend(current.request_ids)
                prefix_length = i + 1
            else:
                break
        return list(set(matched_requests)), prefix_length
</code></pre>
<h3 id="2-kv-cache-management"><a class="header" href="#2-kv-cache-management">2. KV-Cache Management</a></h3>
<p>Implement the paged KV-cache system:</p>
<p><strong>File</strong>: <code>src/kernels/kv_cache.py</code></p>
<pre><code class="language-python">class KVCacheBlock:
    def __init__(self, block_id: int, size: int, num_heads: int, head_dim: int, dtype=torch.float16):
        self.block_id = block_id
        self.size = size
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dtype = dtype
        self.keys = None
        self.values = None
    
    def allocate(self):
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.keys = torch.zeros(self.size, self.num_heads, self.head_dim, dtype=self.dtype, device=device)
        self.values = torch.zeros(self.size, self.num_heads, self.head_dim, dtype=self.dtype, device=device)

class KVCacheManager:
    def __init__(self, num_blocks: int, block_size: int, num_heads: int, head_dim: int, dtype=torch.float16):
        self.num_blocks = num_blocks
        self.block_size = block_size
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dtype = dtype
        
        self.blocks: List[KVCacheBlock] = []
        for i in range(num_blocks):
            block = KVCacheBlock(i, block_size, num_heads, head_dim, dtype)
            self.blocks.append(block)
        
        self.free_block_list: List[int] = list(range(num_blocks))
        self.block_tables: dict = {}
    
    def allocate_blocks(self, request_id: str, num_tokens: int) -&gt; List[int]:
        num_blocks_needed = (num_tokens + self.block_size - 1) // self.block_size
        if len(self.free_block_list) &lt; num_blocks_needed:
            raise RuntimeError(f"Not enough free blocks. Need {num_blocks_needed}, have {len(self.free_block_list)}")
        
        allocated_block_ids = []
        for _ in range(num_blocks_needed):
            block_id = self.free_block_list.pop(0)
            allocated_block_ids.append(block_id)
            self.blocks[block_id].allocate()
        
        self.block_tables[request_id] = allocated_block_ids
        return allocated_block_ids
</code></pre>
<h3 id="3-radix-attention-implementation"><a class="header" href="#3-radix-attention-implementation">3. Radix Attention Implementation</a></h3>
<p>Implement the radial attention mechanism:</p>
<p><strong>File</strong>: <code>src/kernels/radix_attention.py</code></p>
<pre><code class="language-python">import math
from typing import List, Optional, Tuple
import torch
import torch.nn as nn
import torch.nn.functional as F

def rotate_half(x):
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)

def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None):
    cos = cos.unsqueeze(0).unsqueeze(0)
    sin = sin.unsqueeze(0).unsqueeze(0)
    
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed

class RadixAttentionBlock(nn.Module):
    def __init__(self, hidden_size: int, num_heads: int, head_dim: int, max_position_embeddings: int = 2048):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.max_position_embeddings = max_position_embeddings
        
        total_hidden_dim = num_heads * head_dim
        
        self.q_proj = nn.Linear(hidden_size, total_hidden_dim, bias=False)
        self.k_proj = nn.Linear(hidden_size, total_hidden_dim, bias=False)
        self.v_proj = nn.Linear(hidden_size, total_hidden_dim, bias=False)
        self.o_proj = nn.Linear(total_hidden_dim, hidden_size, bias=False)
        
        self.register_buffer(
            "cos_cached",
            torch.ones((max_position_embeddings, head_dim), dtype=torch.float32),
            persistent=False,
        )
        self.register_buffer(
            "sin_cached",
            torch.ones((max_position_embeddings, head_dim), dtype=torch.float32),
            persistent=False,
        )
        
        self._setup_rope_embeddings()
    
    def _setup_rope_embeddings(self):
        position_ids = torch.arange(self.max_position_embeddings, dtype=torch.float32)
        inv_freq = 1.0 / (10000.0 ** (torch.arange(0, self.head_dim, 2, dtype=torch.float32) / self.head_dim))
        
        freqs = torch.einsum("i,j-&gt;ij", position_ids, inv_freq)
        emb = torch.cat((freqs, freqs), dim=-1)
        
        self.cos_cached = emb.cos().to(dtype=torch.float16)
        self.sin_cached = emb.sin().to(dtype=torch.float16)
    
    def forward(self, hidden_states: torch.Tensor, position_ids: Optional[torch.Tensor] = None, 
                past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None) -&gt; torch.Tensor:
        batch_size, seq_len, _ = hidden_states.shape
        
        query = self.q_proj(hidden_states)
        key = self.k_proj(hidden_states)
        value = self.v_proj(hidden_states)
        
        query = query.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        key = key.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        value = value.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        cos_to_use = self.cos_cached[:seq_len].to(query.dtype)
        sin_to_use = self.sin_cached[:seq_len].to(query.dtype)
        
        query, key = apply_rotary_pos_emb(query, key, cos_to_use, sin_to_use, position_ids)
        
        if past_key_value is not None:
            key = torch.cat([past_key_value[0], key], dim=2)
            value = torch.cat([past_key_value[1], value], dim=2)
        
        attn_weights = torch.matmul(query, key.transpose(2, 3)) / math.sqrt(self.head_dim)
        attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
        
        attn_output = torch.matmul(attn_weights, value)
        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.reshape(batch_size, seq_len, self.num_heads * self.head_dim)
        
        output = self.o_proj(attn_output)
        return output
</code></pre>
<h3 id="4-sampling-kernel-implementation"><a class="header" href="#4-sampling-kernel-implementation">4. Sampling Kernel Implementation</a></h3>
<p>Implement the token sampling system:</p>
<p><strong>File</strong>: <code>src/kernels/sampling.py</code></p>
<pre><code class="language-python">import torch

class SamplingKernel:
    def sample(self, logits: torch.Tensor, temperature: float = 1.0, top_p: float = 1.0, top_k: int = -1) -&gt; torch.Tensor:
        if temperature != 1.0:
            logits = logits / temperature
        
        probs = torch.softmax(logits, dim=-1)
        batch_size, vocab_size = probs.shape
        
        if top_k &gt; 0:
            top_k = min(top_k, vocab_size)
            top_k_probs, top_k_indices = torch.topk(probs, top_k, dim=-1)
            
            new_probs = torch.zeros_like(probs)
            new_probs.scatter_(1, top_k_indices, top_k_probs)
            new_probs = new_probs / new_probs.sum(dim=-1, keepdim=True)
            probs = new_probs
        
        if 0 &lt; top_p &lt; 1.0:
            sorted_probs, sorted_indices = torch.sort(probs, dim=-1, descending=True)
            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
            
            mask = cumulative_probs &lt;= top_p
            if mask.shape[-1] &gt; 0:
                mask[..., 0] = True
            
            filtered_probs = torch.zeros_like(probs)
            filtered_probs.scatter_(1, sorted_indices, mask.float() * sorted_probs)
            filtered_probs = filtered_probs / filtered_probs.sum(dim=-1, keepdim=True)
            probs = filtered_probs
        
        sampled_ids = torch.multinomial(probs, num_samples=1).squeeze(-1)
        return sampled_ids
</code></pre>
<h2 id="cuda-kernel-implementation"><a class="header" href="#cuda-kernel-implementation">CUDA Kernel Implementation</a></h2>
<h3 id="1-memory-operations-kernels-1"><a class="header" href="#1-memory-operations-kernels-1">1. Memory Operations Kernels</a></h3>
<p><strong>File</strong>: <code>src/kernels/cuda/memory_ops.cu</code></p>
<pre><code class="language-cuda">#include &lt;torch/extension.h&gt;
#include &lt;cuda.h&gt;
#include &lt;cuda_runtime.h&gt;

__global__ void copy_blocks_kernel(
    float* key_cache, float* value_cache,
    float* new_key_cache, float* new_value_cache,
    int* block_mapping,  // [src_block_id, dst_block_id] pairs
    int num_heads, int head_dim, int block_size,
    int num_mappings
) {
    int mapping_idx = blockIdx.x;
    if (mapping_idx &gt;= num_mappings) return;
    
    int src_block_id = block_mapping[mapping_idx * 2];
    int dst_block_id = block_mapping[mapping_idx * 2 + 1];
    
    int total_elements_per_block = block_size * num_heads * head_dim;
    
    int tid = threadIdx.x;
    if (tid &lt; total_elements_per_block) {
        int src_idx = src_block_id * total_elements_per_block + tid;
        int dst_idx = dst_block_id * total_elements_per_block + tid;
        
        new_key_cache[dst_idx] = key_cache[src_idx];
        new_value_cache[dst_idx] = value_cache[src_idx];
    }
}

torch::Tensor copy_blocks_cuda(
    torch::Tensor key_cache, torch::Tensor value_cache,
    torch::Tensor block_mapping,
    int num_heads, int head_dim, int block_size
) {
    int num_mappings = block_mapping.size(0);
    
    auto options = key_cache.options();
    auto new_key_cache = torch::zeros_like(key_cache);
    auto new_value_cache = torch::zeros_like(value_cache);
    
    dim3 grid(num_mappings);
    dim3 block(256);  // Use 256 threads per block
    
    copy_blocks_kernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;(
        key_cache.data_ptr&lt;float&gt;(),
        value_cache.data_ptr&lt;float&gt;(),
        new_key_cache.data_ptr&lt;float&gt;(),
        new_value_cache.data_ptr&lt;float&gt;(),
        block_mapping.data_ptr&lt;int&gt;(),
        num_heads, head_dim, block_size,
        num_mappings
    );
    
    cudaDeviceSynchronize();
    return std::make_tuple(new_key_cache, new_value_cache);
}
</code></pre>
<h3 id="2-paged-attention-kernels"><a class="header" href="#2-paged-attention-kernels">2. Paged Attention Kernels</a></h3>
<p><strong>File</strong>: <code>src/kernels/cuda/paged_attention.cu</code></p>
<pre><code class="language-cuda">#include &lt;torch/extension.h&gt;
#include &lt;cuda.h&gt;
#include &lt;cuda_runtime.h&gt;

__global__ void paged_attention_kernel(
    float* output,           // [num_seqs, seq_len, num_heads, head_dim]
    const float* query,      // [num_seqs, seq_len, num_heads, head_dim]
    const float* key_cache,  // [num_blocks, block_size, num_kv_heads, head_dim]
    const float* value_cache,// [num_blocks, block_size, num_kv_heads, head_dim]
    const int* block_tables, // [num_seqs, max_blocks_per_seq]
    const int* context_lens, // [num_seqs]
    const int num_kv_heads,
    const int num_queries_per_kv,
    const int head_dim,
    const int block_size,
    const int max_num_blocks_per_seq
) {
    int seq_idx = blockIdx.x;
    int q_head_idx = blockIdx.y;
    int token_idx = blockIdx.z * blockDim.x + threadIdx.x;
    
    if (seq_idx &gt;= gridDim.x || q_head_idx &gt;= gridDim.y || token_idx &gt;= context_lens[seq_idx]) {
        return;
    }
    
    // Get corresponding KV head index
    int kv_head_idx = q_head_idx / num_queries_per_kv;
    
    // Get query vector
    int query_idx = seq_idx * context_lens[seq_idx] * gridDim.y * head_dim +
                    token_idx * gridDim.y * head_dim +
                    q_head_idx * head_dim;
    
    // Shared memory for the current query
    extern __shared__ float shared_mem[];
    float* query_vec = shared_mem;
    
    // Load query vector to shared memory
    for (int d = 0; d &lt; head_dim; d++) {
        query_vec[d] = query[query_idx + d];
    }
    
    // Calculate which block and offset for this token
    int block_idx = token_idx / block_size;
    int block_offset = token_idx % block_size;
    
    // Get physical block number from block table
    int physical_block = block_tables[seq_idx * max_num_blocks_per_seq + block_idx];
    
    // Calculate the actual index in the cache
    int cache_idx = physical_block * block_size * num_kv_heads * head_dim +
                    block_offset * num_kv_heads * head_dim +
                    kv_head_idx * head_dim;
    
    // Perform attention computation
    float sum = 0.0f;
    for (int d = 0; d &lt; head_dim; d++) {
        sum += query_vec[d] * key_cache[cache_idx + d];
    }
    
    // Apply softmax and multiply with value
    float attention_weight = __expf(sum);  // Simplified (real softmax needs normalization)
    for (int d = 0; d &lt; head_dim; d++) {
        int output_idx = seq_idx * context_lens[seq_idx] * gridDim.y * head_dim +
                         token_idx * gridDim.y * head_dim +
                         q_head_idx * head_dim + d;
        output[output_idx] += attention_weight * value_cache[cache_idx + d];
    }
}

torch::Tensor paged_attention_cuda(
    torch::Tensor query, torch::Tensor key_cache, torch::Tensor value_cache,
    torch::Tensor block_tables, torch::Tensor context_lens,
    int num_kv_heads, int num_queries_per_kv
) {
    int num_seqs = query.size(0);
    int seq_len = query.size(1);
    int num_heads = query.size(2);
    int head_dim = query.size(3);
    int block_size = key_cache.size(1);
    int max_blocks_per_seq = block_tables.size(1);
    
    auto output = torch::zeros_like(query);
    
    dim3 grid(num_seqs, num_heads, (seq_len + 255) / 256);  // 256 threads per block
    dim3 block(256);
    
    // Allocate shared memory for query vector
    int shared_mem_size = head_dim * sizeof(float);
    
    paged_attention_kernel&lt;&lt;&lt;grid, block, shared_mem_size&gt;&gt;&gt;(
        output.data_ptr&lt;float&gt;(),
        query.data_ptr&lt;float&gt;(),
        key_cache.data_ptr&lt;float&gt;(),
        value_cache.data_ptr&lt;float&gt;(),
        block_tables.data_ptr&lt;int&gt;(),
        context_lens.data_ptr&lt;int&gt;(),
        num_kv_heads,
        num_queries_per_kv,
        head_dim,
        block_size,
        max_blocks_per_seq
    );
    
    cudaDeviceSynchronize();
    return output;
}
</code></pre>
<h3 id="3-flash-attention-kernels-simplified"><a class="header" href="#3-flash-attention-kernels-simplified">3. Flash Attention Kernels (Simplified)</a></h3>
<p><strong>File</strong>: <code>src/kernels/cuda/flash_attention.cu</code></p>
<pre><code class="language-cuda">#include &lt;torch/extension.h&gt;
#include &lt;cuda.h&gt;
#include &lt;cuda_runtime.h&gt;

#define TILE_SIZE 32  // Small tile for educational purposes

__global__ void flash_attention_kernel(
    float* output,
    const float* query,
    const float* key,
    const float* value,
    const int* seq_lens,
    const int num_seqs,
    const int num_heads,
    const int head_dim,
    const int max_seq_len
) {
    int seq_idx = blockIdx.x;
    int head_idx = blockIdx.y;
    
    if (seq_idx &gt;= num_seqs || head_idx &gt;= num_heads) return;
    
    int current_seq_len = seq_lens[seq_idx];
    
    // Shared memory for tiles
    extern __shared__ float shared_mem[];
    float* s_Q = shared_mem;
    float* s_K = s_Q + TILE_SIZE * head_dim;
    float* s_V = s_K + TILE_SIZE * head_dim;
    float* s_scores = s_V + TILE_SIZE * head_dim;
    
    // Process the sequence in tiles
    for (int q_tile_start = 0; q_tile_start &lt; current_seq_len; q_tile_start += TILE_SIZE) {
        // Load Q tile to shared memory
        for (int i = threadIdx.x; i &lt; TILE_SIZE * head_dim; i += blockDim.x) {
            int q_row = q_tile_start + i / head_dim;
            int q_col = i % head_dim;
            
            if (q_row &lt; current_seq_len) {
                int q_idx = seq_idx * max_seq_len * num_heads * head_dim +
                           q_row * num_heads * head_dim +
                           head_idx * head_dim + q_col;
                s_Q[i] = query[q_idx];
            } else {
                s_Q[i] = 0.0f;
            }
        }
        
        __syncthreads();
        
        // For each K/V tile
        for (int k_tile_start = 0; k_tile_start &lt; current_seq_len; k_tile_start += TILE_SIZE) {
            // Load K and V tiles
            for (int i = threadIdx.x; i &lt; TILE_SIZE * head_dim; i += blockDim.x) {
                int k_row = k_tile_start + i / head_dim;
                int k_col = i % head_dim;
                
                if (k_row &lt; current_seq_len) {
                    int k_idx = seq_idx * max_seq_len * num_heads * head_dim +
                               k_row * num_heads * head_dim +
                               head_idx * head_dim + k_col;
                    s_K[i] = key[k_idx];
                    s_V[i] = value[k_idx];
                } else {
                    s_K[i] = 0.0f;
                    s_V[i] = 0.0f;
                }
            }
            
            __syncthreads();
            
            // Compute attention scores for this tile
            for (int q_local = threadIdx.x; q_local &lt; TILE_SIZE; q_local += blockDim.x) {
                int q_global = q_tile_start + q_local;
                if (q_global &gt;= current_seq_len) continue;
                
                float score_sum = 0.0f;
                float max_score = -INFINITY;
                
                // Compute scores against K tile
                for (int k_local = 0; k_local &lt; TILE_SIZE; k_local++) {
                    int k_global = k_tile_start + k_local;
                    if (k_global &gt;= current_seq_len) continue;
                    
                    float score = 0.0f;
                    for (int d = 0; d &lt; head_dim; d++) {
                        int q_offset = q_local * head_dim + d;
                        int k_offset = k_local * head_dim + d;
                        score += s_Q[q_offset] * s_K[k_offset];
                    }
                    
                    // Apply causal mask
                    if (k_global &gt; q_global) score = -INFINITY;
                    
                    // Update max for numerical stability
                    if (score &gt; max_score) max_score = score;
                    
                    s_scores[q_local * TILE_SIZE + k_local] = score;
                }
                
                // Apply softmax with numerical stability
                float exp_sum = 0.0f;
                for (int k_local = 0; k_local &lt; TILE_SIZE; k_local++) {
                    int k_global = k_tile_start + k_local;
                    if (k_global &gt;= current_seq_len || k_global &gt; q_global) {
                        s_scores[q_local * TILE_SIZE + k_local] = 0.0f;
                    } else {
                        float score = s_scores[q_local * TILE_SIZE + k_local];
                        float exp_score = __expf(score - max_score);
                        s_scores[q_local * TILE_SIZE + k_local] = exp_score;
                        exp_sum += exp_score;
                    }
                }
                
                // Normalize scores
                if (exp_sum &gt; 0.0f) {
                    for (int k_local = 0; k_local &lt; TILE_SIZE; k_local++) {
                        s_scores[q_local * TILE_SIZE + k_local] /= exp_sum;
                    }
                }
                
                // Compute output = scores @ V
                for (int d = 0; d &lt; head_dim; d++) {
                    float output_val = 0.0f;
                    for (int k_local = 0; k_local &lt; TILE_SIZE; k_local++) {
                        int v_offset = k_local * head_dim + d;
                        output_val += s_scores[q_local * TILE_SIZE + k_local] * s_V[v_offset];
                    }
                    
                    int out_idx = seq_idx * max_seq_len * num_heads * head_dim +
                                 q_global * num_heads * head_dim +
                                 head_idx * head_dim + d;
                    output[out_idx] = output_val;
                }
            }
            
            __syncthreads();
        }
    }
}

torch::Tensor flash_attention_cuda(
    torch::Tensor query, torch::Tensor key, torch::Tensor value,
    torch::Tensor seq_lens
) {
    int num_seqs = query.size(0);
    int num_heads = query.size(2);
    int head_dim = query.size(3);
    int max_seq_len = query.size(1);
    
    auto output = torch::zeros_like(query);
    
    dim3 grid(num_seqs, num_heads);
    dim3 block(256);
    
    // Shared memory for 3 tiles + scores matrix
    int shared_mem_size = 3 * TILE_SIZE * head_dim * sizeof(float) +
                          TILE_SIZE * TILE_SIZE * sizeof(float);
    
    flash_attention_kernel&lt;&lt;&lt;grid, block, shared_mem_size&gt;&gt;&gt;(
        output.data_ptr&lt;float&gt;(),
        query.data_ptr&lt;float&gt;(),
        key.data_ptr&lt;float&gt;(),
        value.data_ptr&lt;float&gt;(),
        seq_lens.data_ptr&lt;int&gt;(),
        num_seqs,
        num_heads,
        head_dim,
        max_seq_len
    );
    
    cudaDeviceSynchronize();
    return output;
}
</code></pre>
<h2 id="integration-and-testing"><a class="header" href="#integration-and-testing">Integration and Testing</a></h2>
<h3 id="1-python-cuda-interface"><a class="header" href="#1-python-cuda-interface">1. Python-CUDA Interface</a></h3>
<p>Create the Python interface for CUDA kernels:</p>
<p><strong>File</strong>: <code>src/kernels/api.py</code></p>
<pre><code class="language-python">"""
API for accessing both Python and CUDA implementations of kernels
"""

import torch
from typing import Optional

# Try to import CUDA extensions
try:
    from . import yaie_kernels  # This will be built from CUDA sources
    CUDA_AVAILABLE = True
except ImportError:
    print("CUDA extensions not available. Using Python implementations.")
    CUDA_AVAILABLE = False

def attention_forward(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,
                     use_cuda: bool = True, **kwargs):
    """Unified attention interface that can use CUDA or Python implementation"""
    if CUDA_AVAILABLE and use_cuda and query.is_cuda:
        return yaie_kernels.flash_attention_cuda(query, key, value, 
                                               kwargs.get('seq_lens', None))
    else:
        # Fallback to Python implementation
        from .radix_attention import RadixAttentionBlock
        attention_block = RadixAttentionBlock(
            hidden_size=query.shape[-1],
            num_heads=query.shape[-2],
            head_dim=query.shape[-1] // query.shape[-2]
        )
        return attention_block(query)

def paged_attention_forward(query: torch.Tensor, key_cache: torch.Tensor, 
                           value_cache: torch.Tensor, block_tables: torch.Tensor,
                           context_lens: torch.Tensor, use_cuda: bool = True, **kwargs):
    """Paged attention interface"""
    if CUDA_AVAILABLE and use_cuda and query.is_cuda:
        return yaie_kernels.paged_attention_cuda(
            query, key_cache, value_cache, block_tables, context_lens,
            kwargs.get('num_kv_heads', 1),
            kwargs.get('num_queries_per_kv', 1)
        )
    else:
        # Python fallback would go here
        raise NotImplementedError("Paged attention Python fallback not implemented")

def copy_blocks(key_cache: torch.Tensor, value_cache: torch.Tensor, 
                block_mapping: torch.Tensor, use_cuda: bool = True, **kwargs):
    """Memory copy interface"""
    if CUDA_AVAILABLE and use_cuda and key_cache.is_cuda:
        return yaie_kernels.copy_blocks_cuda(
            key_cache, value_cache, block_mapping,
            kwargs.get('num_heads', 1),
            kwargs.get('head_dim', 64),
            kwargs.get('block_size', 16)
        )
    else:
        # Python fallback would go here
        raise NotImplementedError("Copy blocks Python fallback not implemented")
</code></pre>
<h3 id="2-testing-framework"><a class="header" href="#2-testing-framework">2. Testing Framework</a></h3>
<p>Create comprehensive tests:</p>
<p><strong>File</strong>: <code>tests/test_kernels.py</code></p>
<pre><code class="language-python">import pytest
import torch
import numpy as np

from src.kernels.radix_tree import RadixTree
from src.kernels.kv_cache import KVCacheManager
from src.kernels.radix_attention import RadixAttentionBlock
from src.kernels.sampling import SamplingKernel
from src.kernels.api import attention_forward, paged_attention_forward, copy_blocks

class TestRadixTree:
    def test_basic_insertion_and_search(self):
        tree = RadixTree()
        
        # Insert requests
        tree.insert_request("req1", [1, 2, 3])
        tree.insert_request("req2", [1, 2, 4])  # Shares prefix [1, 2]
        tree.insert_request("req3", [5, 6, 7])  # No shared prefix
        
        # Test shared prefixes
        shared, length = tree.find_shared_prefixes([1, 2, 5])
        assert "req1" in shared
        assert "req2" in shared
        assert length == 2  # Common prefix [1, 2]
    
    def test_prefix_sharing_graph(self):
        tree = RadixTree()
        tree.insert_request("req1", [1, 2, 3])
        tree.insert_request("req2", [1, 2, 4])
        tree.insert_request("req3", [1, 5, 6])
        
        graph = tree.get_shared_computation_graph()
        # Should show shared computation at token [1]
        assert graph["request_count"] == 3  # All requests start with root
        
class TestKVCacheManager:
    def test_basic_allocation(self):
        cache_manager = KVCacheManager(
            num_blocks=100,
            block_size=16,
            num_heads=8,
            head_dim=64,
            dtype=torch.float16
        )
        
        # Allocate blocks for a request
        blocks = cache_manager.allocate_blocks("req1", 20)  # Need 2 blocks (20/16 = 2)
        assert len(blocks) == 2
        
        # Verify the blocks exist and have proper tensors
        for block_id in blocks:
            block = cache_manager.blocks[block_id]
            assert block.keys is not None
            assert block.values is not None
            assert block.keys.shape == (16, 8, 64)  # block_size, num_heads, head_dim
    
    def test_block_reuse(self):
        cache_manager = KVCacheManager(
            num_blocks=10,
            block_size=16,
            num_heads=8,
            head_dim=64
        )
        
        # Allocate all blocks
        req_ids = [f"req{i}" for i in range(10)]
        for req_id in req_ids:
            cache_manager.allocate_blocks(req_id, 10)
        
        assert len(cache_manager.free_block_list) == 0
        
        # Free some blocks
        cache_manager.free_blocks("req0")
        cache_manager.free_blocks("req1")
        
        assert len(cache_manager.free_block_list) == 2
        assert 0 in cache_manager.free_block_list
        assert 1 in cache_manager.free_block_list

class TestRadixAttention:
    def test_basic_attention_forward(self):
        hidden_size = 512
        num_heads = 8
        head_dim = hidden_size // num_heads
        
        attention = RadixAttentionBlock(
            hidden_size=hidden_size,
            num_heads=num_heads,
            head_dim=head_dim,
            max_position_embeddings=256
        )
        
        batch_size = 2
        seq_len = 10
        x = torch.randn(batch_size, seq_len, hidden_size, dtype=torch.float16)
        
        output = attention(x)
        assert output.shape == (batch_size, seq_len, hidden_size)
        assert not torch.isnan(output).any()
    
    def test_attention_with_past_key_value(self):
        hidden_size = 256
        num_heads = 4
        head_dim = hidden_size // num_heads
        
        attention = RadixAttentionBlock(
            hidden_size=hidden_size,
            num_heads=num_heads,
            head_dim=head_dim
        )
        
        batch_size = 1
        seq_len = 5
        x = torch.randn(batch_size, seq_len, hidden_size)
        
        # First forward pass
        output1, _, past_kv = attention(x, use_cache=True)
        
        # Second forward pass with past key-value
        next_token = torch.randn(batch_size, 1, hidden_size)
        output2, _, _ = attention(next_token, past_key_value=past_kv)
        
        assert output2.shape == (batch_size, 1, hidden_size)

class TestSamplingKernel:
    def test_temperature_sampling(self):
        sampling = SamplingKernel()
        
        # Create logits with one clear winner
        logits = torch.tensor([[10.0, 1.0, 1.0, 1.0]])  # First token is dominant
        
        # High temperature should allow other tokens
        sampled_high_temp = sampling.sample(logits, temperature=2.0)
        assert sampled_high_temp.shape == (1,)
        
        # Low temperature should favor dominant token
        sampled_low_temp = sampling.sample(logits, temperature=0.1)
        assert sampled_low_temp[0] == 0  # Should pick the dominant token
    
    def test_top_p_nucleus_sampling(self):
        sampling = SamplingKernel()
        
        # Create logits where first 3 tokens account for ~90% of probability
        logits = torch.tensor([[2.0, 1.5, 1.0, -10.0, -10.0]])
        
        # Top-p = 0.8 should exclude the last two tokens
        sampled = sampling.sample(logits, top_p=0.8)
        # Should be one of the first 3 tokens
        assert sampled[0] in [0, 1, 2]
    
    def test_top_k_sampling(self):
        sampling = SamplingKernel()
        
        # Create logits with clear ordering
        logits = torch.tensor([[5.0, 4.0, 3.0, 2.0, 1.0]])
        
        # Top-k = 2 should only consider first 2 tokens
        sampled = sampling.sample(logits, top_k=2)
        assert sampled[0] in [0, 1]  # Should be one of top 2 tokens

class TestIntegration:
    def test_full_inference_pipeline(self):
        """Test integration of all kernels in a simple pipeline"""
        # This test would simulate a full inference step
        batch_size = 2
        seq_len = 10
        hidden_size = 256
        num_heads = 4
        head_dim = hidden_size // num_heads
        
        # Create input
        x = torch.randn(batch_size, seq_len, hidden_size)
        
        # Apply attention
        attention = RadixAttentionBlock(
            hidden_size=hidden_size,
            num_heads=num_heads,
            head_dim=head_dim
        )
        attn_output = attention(x)
        assert attn_output.shape == (batch_size, seq_len, hidden_size)
        
        # Apply sampling (on logits that would come from LM head)
        logits = torch.randn(batch_size, 1000)  # vocab_size = 1000
        sampling = SamplingKernel()
        sampled_tokens = sampling.sample(logits, temperature=0.7)
        assert sampled_tokens.shape == (batch_size,)

if __name__ == "__main__":
    pytest.main([__file__])
</code></pre>
<h2 id="building-and-running"><a class="header" href="#building-and-running">Building and Running</a></h2>
<h3 id="setup-configuration-1"><a class="header" href="#setup-configuration-1">Setup Configuration</a></h3>
<p><strong>File</strong>: <code>setup_kernels.py</code></p>
<pre><code class="language-python">from setuptools import setup
from torch.utils.cpp_extension import BuildExtension, CUDAExtension
import os

# Check if CUDA is available
def get_extensions():
    extensions = []
    
    # Check if CUDA is available
    try:
        import torch
        if torch.cuda.is_available():
            extensions.append(
                CUDAExtension(
                    name='yaie_kernels',
                    sources=[
                        'src/kernels/cuda/memory_ops.cu',
                        'src/kernels/cuda/paged_attention.cu', 
                        'src/kernels/cuda/flash_attention.cu',
                        'src/kernels/cuda/radix_ops.cu',
                        'src/kernels/cuda/pybind.cpp',  # Python bindings
                    ],
                    extra_compile_args={
                        'cxx': ['-O3'],
                        'nvcc': ['-O3', '--use_fast_math', '-arch=sm_70']
                    }
                )
            )
    except:
        print("CUDA not available, building without CUDA extensions")
    
    return extensions

setup(
    name='yaie_kernels',
    ext_modules=get_extensions(),
    cmdclass={'build_ext': BuildExtension},
    zip_safe=False,
)
</code></pre>
<h2 id="performance-optimization-guidelines"><a class="header" href="#performance-optimization-guidelines">Performance Optimization Guidelines</a></h2>
<h3 id="cuda-optimization-tips"><a class="header" href="#cuda-optimization-tips">CUDA Optimization Tips</a></h3>
<ol>
<li><strong>Memory Coalescing</strong>: Ensure threads in a warp access consecutive memory</li>
<li><strong>Shared Memory</strong>: Use for frequently accessed data</li>
<li><strong>Occupancy</strong>: Maximize number of active warps</li>
<li><strong>Reduction Operations</strong>: Use efficient parallel reduction algorithms</li>
</ol>
<h3 id="profiling-and-benchmarking"><a class="header" href="#profiling-and-benchmarking">Profiling and Benchmarking</a></h3>
<p>Create benchmarking tools:</p>
<pre><code class="language-python">import torch
import time
from torch.profiler import profile, record_function, ProfilerActivity

def benchmark_kernel(kernel_func, *args, **kwargs):
    """Benchmark a kernel function"""
    # Warmup
    for _ in range(3):
        result = kernel_func(*args, **kwargs)
    
    # Actual timing
    torch.cuda.synchronize()
    start_time = time.time()
    
    for _ in range(10):  # Run multiple times for average
        result = kernel_func(*args, **kwargs)
    
    torch.cuda.synchronize()
    end_time = time.time()
    
    avg_time = (end_time - start_time) / 10
    return avg_time, result

def profile_kernel(kernel_func, *args, **kwargs):
    """Profile a kernel function"""
    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:
        result = kernel_func(*args, **kwargs)
    
    print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))
    return result
</code></pre>
<p>This comprehensive implementation guide provides everything needed to implement the core kernels for Mini-YAIE, following SGLang-style optimization principles while maintaining educational value.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="api--serving-openai-compatible-server"><a class="header" href="#api--serving-openai-compatible-server">API &amp; Serving: OpenAI-Compatible Server</a></h1>
<h2 id="overview-7"><a class="header" href="#overview-7">Overview</a></h2>
<p>The API server in Mini-YAIE implements an OpenAI-compatible interface, allowing the engine to be used with existing applications and tools designed for OpenAI’s API. The server uses FastAPI to provide RESTful endpoints with proper request/response handling, streaming support, and health monitoring.</p>
<h2 id="api-design-philosophy"><a class="header" href="#api-design-philosophy">API Design Philosophy</a></h2>
<p>The server follows OpenAI’s API specification to ensure compatibility with existing tools and applications while leveraging the advanced features of the SGLang-style inference engine. This approach allows users to:</p>
<ul>
<li>Use existing OpenAI clients without modification</li>
<li>Take advantage of Mini-YAIE’s performance optimizations</li>
<li>Integrate with tools built for OpenAI’s API format</li>
<li>Maintain familiar request/response patterns</li>
</ul>
<h2 id="core-architecture"><a class="header" href="#core-architecture">Core Architecture</a></h2>
<h3 id="fastapi-application"><a class="header" href="#fastapi-application">FastAPI Application</a></h3>
<p>The server is built using FastAPI for high-performance web serving:</p>
<pre><code class="language-python">def create_app(model_name: str) -&gt; FastAPI:
    app = FastAPI(title="YAIE API", version="0.1.0")
    engine = InferenceEngine(model_name)
    return app
</code></pre>
<h3 id="main-components"><a class="header" href="#main-components">Main Components</a></h3>
<ol>
<li><strong>Inference Engine Integration</strong>: Connects API endpoints to the core inference engine</li>
<li><strong>Request Validation</strong>: Pydantic models for request/response validation</li>
<li><strong>Streaming Support</strong>: Server-sent events for real-time token streaming</li>
<li><strong>Error Handling</strong>: Proper HTTP error codes and message formatting</li>
<li><strong>Health Monitoring</strong>: Endpoints for system status and availability</li>
</ol>
<h2 id="api-endpoints"><a class="header" href="#api-endpoints">API Endpoints</a></h2>
<h3 id="1-chat-completions-endpoint"><a class="header" href="#1-chat-completions-endpoint">1. Chat Completions Endpoint</a></h3>
<p>The main endpoint follows OpenAI’s <code>/v1/chat/completions</code> specification:</p>
<pre><code class="language-python">@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    # Implementation handles both streaming and non-streaming responses
</code></pre>
<h4 id="request-schema"><a class="header" href="#request-schema">Request Schema</a></h4>
<pre><code class="language-python">class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    temperature: Optional[float] = 1.0
    top_p: Optional[float] = 1.0
    max_tokens: Optional[int] = None
    stream: Optional[bool] = False
</code></pre>
<h4 id="response-schema"><a class="header" href="#response-schema">Response Schema</a></h4>
<pre><code class="language-python">class ChatCompletionResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[Choice]
    usage: Dict[str, int]
</code></pre>
<h4 id="supported-parameters"><a class="header" href="#supported-parameters">Supported Parameters</a></h4>
<ul>
<li><strong>model</strong>: Model identifier (passed during server startup)</li>
<li><strong>messages</strong>: List of message objects with role and content</li>
<li><strong>temperature</strong>: Sampling temperature (0.0-2.0 recommended)</li>
<li><strong>top_p</strong>: Nucleus sampling threshold (0.0-1.0)</li>
<li><strong>max_tokens</strong>: Maximum tokens to generate</li>
<li><strong>stream</strong>: Whether to stream responses (true/false)</li>
</ul>
<h3 id="2-model-listing-endpoint"><a class="header" href="#2-model-listing-endpoint">2. Model Listing Endpoint</a></h3>
<p>Lists the available model:</p>
<pre><code class="language-python">@app.get("/v1/models")
async def list_models():
    return {
        "object": "list",
        "data": [
            {
                "id": model_name,
                "object": "model",
                "owned_by": "user",
                "created": int(time.time()),
            }
        ],
    }
</code></pre>
<h3 id="3-health-check-endpoint"><a class="header" href="#3-health-check-endpoint">3. Health Check Endpoint</a></h3>
<p>Simple health monitoring:</p>
<pre><code class="language-python">@app.get("/health")
async def health_check():
    return {"status": "healthy", "model": model_name}
</code></pre>
<h2 id="streaming-implementation"><a class="header" href="#streaming-implementation">Streaming Implementation</a></h2>
<h3 id="streaming-vs-non-streaming"><a class="header" href="#streaming-vs-non-streaming">Streaming vs Non-Streaming</a></h3>
<p>The server supports both response formats using the same endpoint:</p>
<pre><code class="language-python">if request.stream:
    # Return streaming response
    return StreamingResponse(generate_stream(), media_type="text/event-stream")
else:
    # Return non-streaming response
    response = engine.chat_completion(messages_dicts, **kwargs)
    return response
</code></pre>
<h3 id="streaming-response-format"><a class="header" href="#streaming-response-format">Streaming Response Format</a></h3>
<p>The streaming implementation generates Server-Sent Events (SSE):</p>
<pre><code class="language-python">def generate_stream():
    for chunk in engine.chat_completion_stream(messages_dicts, **kwargs):
        yield f"data: {json.dumps(chunk)}\n\n"
    yield "data: [DONE]\n\n"
</code></pre>
<p>Each chunk follows OpenAI’s streaming format:</p>
<pre><code class="language-json">{
  "id": "chatcmpl-...",
  "object": "chat.completion.chunk",
  "created": 1234567890,
  "model": "model-name",
  "choices": [{
    "index": 0,
    "delta": {"content": "token"},
    "finish_reason": null
  }]
}
</code></pre>
<h2 id="integration-with-inference-engine"><a class="header" href="#integration-with-inference-engine">Integration with Inference Engine</a></h2>
<h3 id="request-processing-flow-1"><a class="header" href="#request-processing-flow-1">Request Processing Flow</a></h3>
<ol>
<li><strong>API Request</strong>: Received through FastAPI endpoints</li>
<li><strong>Validation</strong>: Pydantic models validate request format</li>
<li><strong>Parameter Extraction</strong>: Convert API parameters to engine format</li>
<li><strong>Engine Processing</strong>: Call appropriate engine methods</li>
<li><strong>Response Formatting</strong>: Convert engine output to API format</li>
<li><strong>API Response</strong>: Return properly formatted responses</li>
</ol>
<h3 id="parameter-mapping"><a class="header" href="#parameter-mapping">Parameter Mapping</a></h3>
<p>API parameters are mapped to engine capabilities:</p>
<pre><code class="language-python">kwargs = {}
if request.max_tokens is not None:
    kwargs["max_tokens"] = request.max_tokens
if request.temperature is not None:
    kwargs["temperature"] = request.temperature
if request.top_p is not None:
    kwargs["top_p"] = request.top_p
</code></pre>
<h3 id="message-formatting"><a class="header" href="#message-formatting">Message Formatting</a></h3>
<p>The server handles OpenAI-style messages by converting them to a format the engine understands:</p>
<pre><code class="language-python">messages_dicts = [
    {"role": msg.role, "content": msg.content}
    for msg in request.messages
]

# Apply chat template if available
if hasattr(self.tokenizer, 'apply_chat_template'):
    formatted_prompt = self.tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
else:
    # Fallback formatting
    formatted_prompt = ""
    for message in messages:
        formatted_prompt += f"{message['role'].capitalize()}: {message['content']}\n"
    formatted_prompt += "\nAssistant:"
</code></pre>
<h2 id="error-handling-2"><a class="header" href="#error-handling-2">Error Handling</a></h2>
<h3 id="http-error-codes"><a class="header" href="#http-error-codes">HTTP Error Codes</a></h3>
<p>The server properly handles various error conditions:</p>
<ul>
<li><strong>400 Bad Request</strong>: Invalid request parameters</li>
<li><strong>429 Too Many Requests</strong>: Rate limiting (not implemented in basic version)</li>
<li><strong>500 Internal Server Error</strong>: Server-side errors during processing</li>
</ul>
<h3 id="error-response-format"><a class="header" href="#error-response-format">Error Response Format</a></h3>
<p>Standard OpenAI-compatible error format:</p>
<pre><code class="language-python">{
  "error": {
    "message": "Error description",
    "type": "server_error",
    "param": null,
    "code": null
  }
}
</code></pre>
<h3 id="exception-handling"><a class="header" href="#exception-handling">Exception Handling</a></h3>
<p>The server wraps all processing in try-catch blocks:</p>
<pre><code class="language-python">try:
    # Process request
    response = engine.chat_completion(messages_dicts, **kwargs)
    return response
except Exception as e:
    traceback.print_exc()
    raise HTTPException(status_code=500, detail=str(e))
</code></pre>
<h2 id="performance-considerations-2"><a class="header" href="#performance-considerations-2">Performance Considerations</a></h2>
<h3 id="request-batching"><a class="header" href="#request-batching">Request Batching</a></h3>
<p>The API integrates with the engine’s batching system:</p>
<ul>
<li>Multiple API requests can be batched together in the engine</li>
<li>Continuous batching maintains high throughput</li>
<li>Batch size limited by engine configuration</li>
</ul>
<h3 id="memory-management-4"><a class="header" href="#memory-management-4">Memory Management</a></h3>
<p>The server shares memory with the inference engine:</p>
<ul>
<li>KV-cache is shared across API requests</li>
<li>Efficient memory reuse through paged cache system</li>
<li>Memory limits enforced by engine configuration</li>
</ul>
<h3 id="concurrency"><a class="header" href="#concurrency">Concurrency</a></h3>
<p>FastAPI provides automatic concurrency handling:</p>
<ul>
<li>Async request processing</li>
<li>Connection pooling</li>
<li>Efficient handling of multiple simultaneous requests</li>
</ul>
<h2 id="security-considerations"><a class="header" href="#security-considerations">Security Considerations</a></h2>
<h3 id="input-validation"><a class="header" href="#input-validation">Input Validation</a></h3>
<ul>
<li>Pydantic models validate all request parameters</li>
<li>Type checking prevents injection attacks</li>
<li>Length limits prevent excessive resource consumption</li>
</ul>
<h3 id="rate-limiting"><a class="header" href="#rate-limiting">Rate Limiting</a></h3>
<p>While not implemented in the basic version, can be added:</p>
<ul>
<li>Per-IP rate limiting</li>
<li>Request quota management</li>
<li>Usage monitoring</li>
</ul>
<h2 id="deployment-configuration"><a class="header" href="#deployment-configuration">Deployment Configuration</a></h2>
<h3 id="server-startup"><a class="header" href="#server-startup">Server Startup</a></h3>
<p>The server can be started with a specific model:</p>
<pre><code class="language-bash">uvicorn server.api:app --host 0.0.0.0 --port 8000
</code></pre>
<h3 id="environment-configuration"><a class="header" href="#environment-configuration">Environment Configuration</a></h3>
<p>The server supports environment-based configuration:</p>
<ul>
<li>Model name via environment variables</li>
<li>Port and host configuration</li>
<li>Resource limits</li>
</ul>
<h2 id="sglang-style-features-integration"><a class="header" href="#sglang-style-features-integration">SGLang-Style Features Integration</a></h2>
<h3 id="continuous-batching-1"><a class="header" href="#continuous-batching-1">Continuous Batching</a></h3>
<p>The API benefits from the engine’s continuous batching:</p>
<ul>
<li>Requests are automatically batched</li>
<li>High throughput maintained</li>
<li>Low latency for individual requests</li>
</ul>
<h3 id="prefix-sharing"><a class="header" href="#prefix-sharing">Prefix Sharing</a></h3>
<p>API requests with similar prefixes benefit from:</p>
<ul>
<li>Shared computation in radial attention</li>
<li>Reduced memory usage</li>
<li>Improved efficiency</li>
</ul>
<h3 id="multi-step-processing"><a class="header" href="#multi-step-processing">Multi-Step Processing</a></h3>
<p>The API leverages the engine’s multi-step capabilities:</p>
<ul>
<li>Efficient prefill and decode phases</li>
<li>Optimized request scheduling</li>
<li>Memory-aware processing</li>
</ul>
<h2 id="usage-examples"><a class="header" href="#usage-examples">Usage Examples</a></h2>
<h3 id="basic-chat-request"><a class="header" href="#basic-chat-request">Basic Chat Request</a></h3>
<pre><code class="language-bash">curl -X POST "http://localhost:8000/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "model-name",
    "messages": [
      {"role": "user", "content": "Hello, how are you?"}
    ],
    "temperature": 0.7
  }'
</code></pre>
<h3 id="streaming-request"><a class="header" href="#streaming-request">Streaming Request</a></h3>
<pre><code class="language-bash">curl -X POST "http://localhost:8000/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "model-name",
    "messages": [
      {"role": "user", "content": "Write a short story"}
    ],
    "stream": true
  }'
</code></pre>
<h3 id="programmatic-usage"><a class="header" href="#programmatic-usage">Programmatic Usage</a></h3>
<pre><code class="language-python">import openai

# Configure client to use local server
client = openai.OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="dummy"  # Required but ignored
)

# Use standard OpenAI SDK
response = client.chat.completions.create(
    model="model-name",
    messages=[{"role": "user", "content": "Hello"}],
    temperature=0.7
)
</code></pre>
<h2 id="monitoring-and-logging"><a class="header" href="#monitoring-and-logging">Monitoring and Logging</a></h2>
<h3 id="health-endpoints"><a class="header" href="#health-endpoints">Health Endpoints</a></h3>
<p>The health endpoint can be used for monitoring:</p>
<pre><code class="language-bash">curl http://localhost:8000/health
</code></pre>
<h3 id="performance-metrics"><a class="header" href="#performance-metrics">Performance Metrics</a></h3>
<p>The server can be extended with:</p>
<ul>
<li>Response time tracking</li>
<li>Request volume monitoring</li>
<li>Error rate monitoring</li>
<li>Resource utilization metrics</li>
</ul>
<h2 id="advanced-features-3"><a class="header" href="#advanced-features-3">Advanced Features</a></h2>
<h3 id="model-loading-1"><a class="header" href="#model-loading-1">Model Loading</a></h3>
<p>The server handles model loading automatically:</p>
<ul>
<li>Lazy loading when first accessed</li>
<li>Caching for subsequent requests</li>
<li>HuggingFace model integration</li>
</ul>
<h3 id="response-caching"><a class="header" href="#response-caching">Response Caching</a></h3>
<p>The system supports response caching for:</p>
<ul>
<li>Repeated identical requests</li>
<li>Common prompt patterns</li>
<li>Improved response times for cached content</li>
</ul>
<h3 id="logging-and-debugging"><a class="header" href="#logging-and-debugging">Logging and Debugging</a></h3>
<p>Comprehensive logging can be added:</p>
<ul>
<li>Request/response logging</li>
<li>Performance metrics</li>
<li>Error tracing</li>
<li>Usage analytics</li>
</ul>
<p>This OpenAI-compatible API server enables Mini-YAIE to be integrated into existing ecosystems while providing the performance benefits of SGLang-style inference optimization.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="cli-usage-interactive-and-server-modes"><a class="header" href="#cli-usage-interactive-and-server-modes">CLI Usage: Interactive and Server Modes</a></h1>
<h2 id="overview-8"><a class="header" href="#overview-8">Overview</a></h2>
<p>Mini-YAIE provides a comprehensive command-line interface (CLI) that serves as the primary entry point for users. The CLI supports both interactive chat mode and server mode, making it suitable for both direct interaction and production deployment scenarios.</p>
<h2 id="cli-architecture"><a class="header" href="#cli-architecture">CLI Architecture</a></h2>
<h3 id="entry-point-structure"><a class="header" href="#entry-point-structure">Entry Point Structure</a></h3>
<p>The CLI is organized around different command verbs:</p>
<pre><code>yaie &lt;command&gt; [options] [arguments]

Commands:
- serve: Start an OpenAI-compatible API server
- chat: Start an interactive chat session
</code></pre>
<h3 id="core-components-3"><a class="header" href="#core-components-3">Core Components</a></h3>
<ol>
<li><strong>Argument Parsing</strong>: Uses argparse for command-line option handling</li>
<li><strong>Model Integration</strong>: Connects CLI commands to the inference engine</li>
<li><strong>Interactive Interface</strong>: Provides user-friendly chat experience</li>
<li><strong>Server Integration</strong>: Launches API server with proper configuration</li>
</ol>
<h2 id="server-mode"><a class="header" href="#server-mode">Server Mode</a></h2>
<h3 id="basic-server-usage"><a class="header" href="#basic-server-usage">Basic Server Usage</a></h3>
<p>Start the API server with a specific model:</p>
<pre><code class="language-bash">yaie serve microsoft/DialoGPT-medium --host localhost --port 8000
</code></pre>
<h3 id="server-options"><a class="header" href="#server-options">Server Options</a></h3>
<h4 id="model-selection-1"><a class="header" href="#model-selection-1">Model Selection</a></h4>
<pre><code class="language-bash">--model MODEL_NAME          # Specify the model to use (required)
</code></pre>
<h4 id="network-configuration"><a class="header" href="#network-configuration">Network Configuration</a></h4>
<pre><code class="language-bash">--host HOST                 # Server host (default: localhost)
--port PORT                 # Server port (default: 8000)
--workers WORKERS           # Number of server workers
</code></pre>
<h4 id="performance-options"><a class="header" href="#performance-options">Performance Options</a></h4>
<pre><code class="language-bash">--max-batch-size N          # Maximum batch size
--max-prefill-batch-size N  # Maximum prefill batch size
--max-decode-batch-size N   # Maximum decode batch size
--num-blocks N              # Number of KV-cache blocks
--block-size N              # Size of each cache block
</code></pre>
<h3 id="server-startup-process"><a class="header" href="#server-startup-process">Server Startup Process</a></h3>
<ol>
<li><strong>Model Loading</strong>: Download and load model from HuggingFace if not cached</li>
<li><strong>Engine Initialization</strong>: Create inference engine with specified parameters</li>
<li><strong>API Server Creation</strong>: Initialize FastAPI application with engine</li>
<li><strong>Server Launch</strong>: Start the web server on specified host/port</li>
</ol>
<h3 id="example-server-commands"><a class="header" href="#example-server-commands">Example Server Commands</a></h3>
<h4 id="basic-server"><a class="header" href="#basic-server">Basic Server</a></h4>
<pre><code class="language-bash">yaie serve microsoft/DialoGPT-medium
</code></pre>
<h4 id="production-server"><a class="header" href="#production-server">Production Server</a></h4>
<pre><code class="language-bash">yaie serve microsoft/DialoGPT-medium --host 0.0.0.0 --port 8000 --max-batch-size 16
</code></pre>
<h4 id="resource-constrained-server"><a class="header" href="#resource-constrained-server">Resource-Constrained Server</a></h4>
<pre><code class="language-bash">yaie serve microsoft/DialoGPT-medium --num-blocks 1000 --max-batch-size 4
</code></pre>
<h2 id="chat-mode"><a class="header" href="#chat-mode">Chat Mode</a></h2>
<h3 id="basic-chat-usage"><a class="header" href="#basic-chat-usage">Basic Chat Usage</a></h3>
<p>Start an interactive chat session:</p>
<pre><code class="language-bash">yaie chat microsoft/DialoGPT-medium
</code></pre>
<h3 id="chat-options"><a class="header" href="#chat-options">Chat Options</a></h3>
<h4 id="generation-parameters"><a class="header" href="#generation-parameters">Generation Parameters</a></h4>
<pre><code class="language-bash">--temperature TEMP          # Sampling temperature (default: 1.0)
--top-p TOP_P               # Nucleus sampling threshold (default: 1.0)
--max-tokens N              # Maximum tokens to generate (default: 512)
--stream                    # Stream responses in real-time (default: true)
</code></pre>
<h4 id="model-configuration-2"><a class="header" href="#model-configuration-2">Model Configuration</a></h4>
<pre><code class="language-bash">--model MODEL_NAME          # Specify the model to use (required)
</code></pre>
<h3 id="interactive-chat-experience"><a class="header" href="#interactive-chat-experience">Interactive Chat Experience</a></h3>
<h4 id="session-flow"><a class="header" href="#session-flow">Session Flow</a></h4>
<ol>
<li><strong>Model Loading</strong>: Model is loaded if not cached</li>
<li><strong>Chat Initialization</strong>: Engine and tokenizer are set up</li>
<li><strong>Conversation Loop</strong>: User inputs are processed and responses generated</li>
<li><strong>Session Termination</strong>: Exit with Ctrl+C or quit command</li>
</ol>
<h4 id="user-interaction"><a class="header" href="#user-interaction">User Interaction</a></h4>
<p>The chat interface provides a conversational experience:</p>
<pre><code>$ yaie chat microsoft/DialoGPT-medium
Model loaded successfully!
Starting chat session (press Ctrl+C to exit)...

User: Hello, how are you?
AI: I'm doing well, thank you for asking!

User: What can you help me with?
AI: I can have conversations, answer questions, and assist with various tasks.
</code></pre>
<h3 id="example-chat-commands"><a class="header" href="#example-chat-commands">Example Chat Commands</a></h3>
<h4 id="basic-chat"><a class="header" href="#basic-chat">Basic Chat</a></h4>
<pre><code class="language-bash">yaie chat microsoft/DialoGPT-medium
</code></pre>
<h4 id="creative-chat"><a class="header" href="#creative-chat">Creative Chat</a></h4>
<pre><code class="language-bash">yaie chat microsoft/DialoGPT-medium --temperature 1.2 --top-p 0.9
</code></pre>
<h4 id="focused-chat"><a class="header" href="#focused-chat">Focused Chat</a></h4>
<pre><code class="language-bash">yaie chat microsoft/DialoGPT-medium --temperature 0.7 --max-tokens 128
</code></pre>
<h2 id="model-selection-1-1"><a class="header" href="#model-selection-1-1">Model Selection</a></h2>
<h3 id="supported-model-formats"><a class="header" href="#supported-model-formats">Supported Model Formats</a></h3>
<p>The CLI supports any HuggingFace-compatible model:</p>
<h4 id="pre-trained-models"><a class="header" href="#pre-trained-models">Pre-trained Models</a></h4>
<pre><code class="language-bash">yaie serve microsoft/DialoGPT-medium
yaie serve gpt2
yaie serve facebook/opt-1.3b
</code></pre>
<h4 id="local-models-1"><a class="header" href="#local-models-1">Local Models</a></h4>
<pre><code class="language-bash">yaie serve /path/to/local/model
yaie serve ./models/my-custom-model
</code></pre>
<h3 id="model-caching"><a class="header" href="#model-caching">Model Caching</a></h3>
<p>Models are automatically downloaded and cached:</p>
<ul>
<li>First run: Download from HuggingFace Hub</li>
<li>Subsequent runs: Use cached version</li>
<li>Cache location: Standard HuggingFace cache directory</li>
</ul>
<h2 id="performance-tuning-1"><a class="header" href="#performance-tuning-1">Performance Tuning</a></h2>
<h3 id="memory-configuration"><a class="header" href="#memory-configuration">Memory Configuration</a></h3>
<p>Adjust memory settings based on available GPU memory:</p>
<pre><code class="language-bash"># For 24GB+ GPU
yaie serve model --num-blocks 4000 --max-batch-size 32

# For 8-16GB GPU  
yaie serve model --num-blocks 1500 --max-batch-size 8

# For 4-8GB GPU
yaie serve model --num-blocks 800 --max-batch-size 4
</code></pre>
<h3 id="batch-size-optimization-1"><a class="header" href="#batch-size-optimization-1">Batch Size Optimization</a></h3>
<p>Tune batch sizes for optimal throughput:</p>
<pre><code class="language-bash"># High throughput (more memory)
yaie serve model --max-batch-size 32 --max-prefill-batch-size 64

# Memory efficient (lower batch sizes)
yaie serve model --max-batch-size 4 --max-prefill-batch-size 8
</code></pre>
<h2 id="error-handling-and-troubleshooting"><a class="header" href="#error-handling-and-troubleshooting">Error Handling and Troubleshooting</a></h2>
<h3 id="common-errors"><a class="header" href="#common-errors">Common Errors</a></h3>
<h4 id="model-loading-errors"><a class="header" href="#model-loading-errors">Model Loading Errors</a></h4>
<pre><code class="language-bash"># If model name is invalid
Error: Model not found on HuggingFace Hub

# If network is unavailable during first load
Error: Failed to download model
</code></pre>
<h4 id="memory-errors"><a class="header" href="#memory-errors">Memory Errors</a></h4>
<pre><code class="language-bash"># If not enough GPU memory
CUDA out of memory error

# If KV-cache is too large
Memory allocation failed
</code></pre>
<h3 id="debugging-options"><a class="header" href="#debugging-options">Debugging Options</a></h3>
<h4 id="verbose-output"><a class="header" href="#verbose-output">Verbose Output</a></h4>
<pre><code class="language-bash">yaie serve model --verbose  # Show detailed startup information
</code></pre>
<h4 id="configuration-validation-1"><a class="header" href="#configuration-validation-1">Configuration Validation</a></h4>
<pre><code class="language-bash">yaie serve model --debug    # Enable debugging features
</code></pre>
<h2 id="advanced-cli-features"><a class="header" href="#advanced-cli-features">Advanced CLI Features</a></h2>
<h3 id="configuration-files"><a class="header" href="#configuration-files">Configuration Files</a></h3>
<p>The CLI supports configuration files for complex setups:</p>
<pre><code class="language-bash">yaie serve --config config.yaml model
</code></pre>
<h3 id="environment-variables"><a class="header" href="#environment-variables">Environment Variables</a></h3>
<p>Several environment variables can customize behavior:</p>
<pre><code class="language-bash"># Set default host
export YAIE_HOST=0.0.0.0

# Set default port  
export YAIE_PORT=9000

# Set memory limits
export YAIE_MAX_BLOCKS=2000
</code></pre>
<h3 id="logging-configuration"><a class="header" href="#logging-configuration">Logging Configuration</a></h3>
<p>Control logging verbosity and output:</p>
<pre><code class="language-bash"># Enable detailed logging
yaie serve model --log-level DEBUG

# Log to file
yaie serve model --log-file server.log
</code></pre>
<h2 id="integration-with-sglang-features-1"><a class="header" href="#integration-with-sglang-features-1">Integration with SGLang Features</a></h2>
<h3 id="batching-optimization"><a class="header" href="#batching-optimization">Batching Optimization</a></h3>
<p>The CLI exposes SGLang batching parameters:</p>
<pre><code class="language-bash">yaie serve model \
  --max-prefill-batch-size 16 \
  --max-decode-batch-size 256
</code></pre>
<h3 id="prefix-sharing-control"><a class="header" href="#prefix-sharing-control">Prefix Sharing Control</a></h3>
<p>Parameters that affect prefix sharing efficiency:</p>
<pre><code class="language-bash">yaie serve model \
  --max-seq-len 2048 \
  --block-size 16
</code></pre>
<h2 id="production-deployment"><a class="header" href="#production-deployment">Production Deployment</a></h2>
<h3 id="server-management"><a class="header" href="#server-management">Server Management</a></h3>
<h4 id="process-control"><a class="header" href="#process-control">Process Control</a></h4>
<pre><code class="language-bash"># Start server in background
nohup yaie serve model &gt; server.log 2&gt;&amp;1 &amp;

# Kill server process
pkill -f "yaie serve"
</code></pre>
<h4 id="process-monitoring"><a class="header" href="#process-monitoring">Process Monitoring</a></h4>
<pre><code class="language-bash"># Monitor server with systemd
systemctl start yaie-server

# Monitor with supervisor
supervisorctl start yaie-server
</code></pre>
<h3 id="health-checks"><a class="header" href="#health-checks">Health Checks</a></h3>
<p>The server provides health status:</p>
<pre><code class="language-bash"># Check server status
curl http://localhost:8000/health

# Integrate with monitoring tools
# Health check interval and thresholds
</code></pre>
<h2 id="examples-and-use-cases"><a class="header" href="#examples-and-use-cases">Examples and Use Cases</a></h2>
<h3 id="development-usage"><a class="header" href="#development-usage">Development Usage</a></h3>
<pre><code class="language-bash"># Quick test with small model
yaie chat gpt2

# Interactive development with verbose output
yaie serve gpt2 --port 8000 --verbose
</code></pre>
<h3 id="production-usage"><a class="header" href="#production-usage">Production Usage</a></h3>
<pre><code class="language-bash"># High-performance server for production
yaie serve microsoft/DialoGPT-medium \
  --host 0.0.0.0 \
  --port 8000 \
  --max-batch-size 16 \
  --num-blocks 2000

# Low-resource server for edge deployment
yaie serve gpt2 \
  --max-batch-size 2 \
  --num-blocks 500
</code></pre>
<h3 id="testing-and-evaluation"><a class="header" href="#testing-and-evaluation">Testing and Evaluation</a></h3>
<pre><code class="language-bash"># Test with various parameters
yaie chat model --temperature 0.5 --top-p 0.9

# Evaluate different models
yaie serve model1 --port 8001 &amp;
yaie serve model2 --port 8002 &amp;
</code></pre>
<p>The CLI provides a comprehensive interface to access all of Mini-YAIE’s features, from simple interactive chat to high-performance API serving with SGLang-style optimizations.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="production-deployment-1"><a class="header" href="#production-deployment-1">Production Deployment</a></h1>
<p>While Mini-YAIE is primarily educational, understanding production considerations helps bridge the gap between learning and real-world deployment.</p>
<h2 id="deployment-architecture"><a class="header" href="#deployment-architecture">Deployment Architecture</a></h2>
<pre><code class="language-mermaid">graph TD
    LoadBalancer --&gt;|HTTP| API1
    LoadBalancer --&gt;|HTTP| API2
    LoadBalancer --&gt;|HTTP| API3
    API1 --&gt;|gRPC| Inference1
    API2 --&gt;|gRPC| Inference2
    API3 --&gt;|gRPC| Inference3
    Inference1 --&gt;|GPU| Model1
    Inference2 --&gt;|GPU| Model2
    Inference3 --&gt;|GPU| Model3
</code></pre>
<h2 id="1-performance-optimization"><a class="header" href="#1-performance-optimization">1. Performance Optimization</a></h2>
<h3 id="batching--latency-management"><a class="header" href="#batching--latency-management">Batching &amp; Latency Management</a></h3>
<ul>
<li><strong>Request Timeouts</strong>: Implement configurable timeouts for requests waiting in queue</li>
<li><strong>Priority Queues</strong>: Support different priority levels for requests</li>
<li><strong>Preemption</strong>: Pause low-priority requests and swap their KV-cache to CPU when high-priority requests arrive</li>
<li><strong>Adaptive Batching</strong>: Dynamically adjust batch sizes based on current load and latency requirements</li>
</ul>
<h3 id="continuous-batching-strategies"><a class="header" href="#continuous-batching-strategies">Continuous Batching Strategies</a></h3>
<pre><code class="language-python"># Advanced scheduling policies
class SchedulingPolicy(Enum):
    FCFS = "fcfs"  # First-come-first-served
    PRIORITY = "priority"  # Priority-based
    FAIR = "fair"  # Fair sharing
    LATENCY_OPTIMIZED = "latency"  # Minimize latency
</code></pre>
<h2 id="2-distributed-inference"><a class="header" href="#2-distributed-inference">2. Distributed Inference</a></h2>
<h3 id="tensor-parallelism"><a class="header" href="#tensor-parallelism">Tensor Parallelism</a></h3>
<pre><code class="language-python"># Split model weights across multiple GPUs
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map={
        "layer.0": "cuda:0",
        "layer.1": "cuda:1",
        "layer.2": "cuda:2",
        "layer.3": "cuda:3"
    }
)
</code></pre>
<h3 id="pipeline-parallelism"><a class="header" href="#pipeline-parallelism">Pipeline Parallelism</a></h3>
<pre><code class="language-python"># Split layers across different stages
# Stage 1: Layers 0-10 on GPU 0
# Stage 2: Layers 11-20 on GPU 1
# Stage 3: Layers 21-30 on GPU 2
</code></pre>
<h3 id="model-parallelism"><a class="header" href="#model-parallelism">Model Parallelism</a></h3>
<ul>
<li><strong>Expert Parallelism</strong>: For mixture-of-experts models</li>
<li><strong>Sequence Parallelism</strong>: For very long sequences</li>
<li><strong>Hybrid Parallelism</strong>: Combining multiple strategies</li>
</ul>
<h2 id="3-memory-optimization"><a class="header" href="#3-memory-optimization">3. Memory Optimization</a></h2>
<h3 id="quantization"><a class="header" href="#quantization">Quantization</a></h3>
<pre><code class="language-python"># Load model with quantization
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_8bit=True,  # 8-bit quantization
    device_map="auto"
)

# Or use 4-bit quantization
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_4bit=True,
    device_map="auto"
)
</code></pre>
<h3 id="memory-management-strategies"><a class="header" href="#memory-management-strategies">Memory Management Strategies</a></h3>
<ul>
<li><strong>Paged Attention</strong>: Efficient memory management for KV-cache</li>
<li><strong>Memory Pooling</strong>: Reuse memory blocks across requests</li>
<li><strong>Swapping</strong>: Move inactive KV-cache blocks to CPU memory</li>
<li><strong>Compression</strong>: Compress KV-cache values with minimal quality loss</li>
</ul>
<h2 id="4-scalability--reliability"><a class="header" href="#4-scalability--reliability">4. Scalability &amp; Reliability</a></h2>
<h3 id="horizontal-scaling"><a class="header" href="#horizontal-scaling">Horizontal Scaling</a></h3>
<pre><code class="language-python"># Multiple inference workers behind load balancer
workers = [
    InferenceWorker(model_name, gpu_id=0),
    InferenceWorker(model_name, gpu_id=1),
    InferenceWorker(model_name, gpu_id=2),
    InferenceWorker(model_name, gpu_id=3)
]
</code></pre>
<h3 id="load-balancing"><a class="header" href="#load-balancing">Load Balancing</a></h3>
<ul>
<li><strong>Round Robin</strong>: Simple distribution across workers</li>
<li><strong>Least Connections</strong>: Send to least busy worker</li>
<li><strong>Latency-based</strong>: Send to worker with lowest current latency</li>
<li><strong>Content-based</strong>: Route based on request characteristics</li>
</ul>
<h3 id="health-monitoring"><a class="header" href="#health-monitoring">Health Monitoring</a></h3>
<pre><code class="language-python"># Health check endpoints
@app.get("/health")
def health_check():
    return {
        "status": "healthy",
        "gpu_memory": get_gpu_memory_usage(),
        "active_requests": scheduler.get_active_request_count(),
        "queue_length": scheduler.get_queue_status()
    }
</code></pre>
<h2 id="5-observability"><a class="header" href="#5-observability">5. Observability</a></h2>
<h3 id="metrics-collection"><a class="header" href="#metrics-collection">Metrics Collection</a></h3>
<pre><code class="language-python"># Prometheus metrics
REQUEST_LATENCY = Histogram(
    'request_latency_seconds',
    'Request latency in seconds',
    buckets=[0.1, 0.5, 1.0, 2.5, 5.0, 10.0]
)

TOKENS_PER_SECOND = Counter(
    'tokens_per_second',
    'Tokens generated per second'
)
</code></pre>
<h3 id="logging"><a class="header" href="#logging">Logging</a></h3>
<pre><code class="language-python"># Structured logging
logger.info(
    "Request completed",
    request_id=request.id,
    latency=latency_seconds,
    tokens_generated=token_count,
    model=model_name
)
</code></pre>
<h3 id="tracing"><a class="header" href="#tracing">Tracing</a></h3>
<pre><code class="language-python"># Distributed tracing
with tracer.start_span("generate_response") as span:
    span.set_attribute("model", model_name)
    span.set_attribute("request_id", request.id)
    response = engine.generate(prompt)
</code></pre>
<h2 id="6-security"><a class="header" href="#6-security">6. Security</a></h2>
<h3 id="authentication"><a class="header" href="#authentication">Authentication</a></h3>
<pre><code class="language-python"># API key authentication
@app.post("/v1/chat/completions")
async def chat_completions(
    request: ChatCompletionRequest,
    api_key: str = Header(None)
):
    if not validate_api_key(api_key):
        raise HTTPException(status_code=401, detail="Unauthorized")
</code></pre>
<h3 id="rate-limiting-1"><a class="header" href="#rate-limiting-1">Rate Limiting</a></h3>
<pre><code class="language-python"># Rate limiting per API key
limiter = RateLimiter(
    requests_per_minute=1000,
    burst_capacity=100
)

@app.post("/v1/chat/completions")
@limiter.limit("1000/minute")
async def chat_completions(request: ChatCompletionRequest):
    # Process request
</code></pre>
<h3 id="input-validation-1"><a class="header" href="#input-validation-1">Input Validation</a></h3>
<pre><code class="language-python"># Validate and sanitize inputs
@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    # Validate prompt length
    if len(request.prompt) &gt; MAX_PROMPT_LENGTH:
        raise HTTPException(status_code=400, detail="Prompt too long")
    
    # Sanitize inputs
    sanitized_prompt = sanitize_input(request.prompt)
</code></pre>
<h2 id="7-configuration-management"><a class="header" href="#7-configuration-management">7. Configuration Management</a></h2>
<h3 id="environment-variables-1"><a class="header" href="#environment-variables-1">Environment Variables</a></h3>
<pre><code class="language-bash"># Configure through environment variables
export MODEL_NAME="gpt2"
export MAX_BATCH_SIZE="16"
export GPU_MEMORY_UTIL="0.9"
export ENABLE_RADIX_CACHE="true"
</code></pre>
<h3 id="configuration-files-1"><a class="header" href="#configuration-files-1">Configuration Files</a></h3>
<pre><code class="language-yaml"># YAML configuration
model:
  name: "gpt2"
  dtype: "float16"
  tensor_parallel_size: 1

scheduler:
  max_batch_size: 16
  max_prefill_batch_size: 32
  max_decode_batch_size: 256

memory:
  gpu_blocks: 2000
  cpu_blocks: 1000
  block_size: 16
</code></pre>
<h2 id="8-deployment-strategies"><a class="header" href="#8-deployment-strategies">8. Deployment Strategies</a></h2>
<h3 id="containerization"><a class="header" href="#containerization">Containerization</a></h3>
<pre><code class="language-dockerfile"># Dockerfile for Mini-YAIE
FROM python:3.9-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
RUN pip install -e .

CMD ["yaie", "serve", "gpt2", "--host", "0.0.0.0", "--port", "8000"]
</code></pre>
<h3 id="kubernetes-deployment"><a class="header" href="#kubernetes-deployment">Kubernetes Deployment</a></h3>
<pre><code class="language-yaml"># Kubernetes deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: yaie-inference
spec:
  replicas: 3
  selector:
    matchLabels:
      app: yaie-inference
  template:
    metadata:
      labels:
        app: yaie-inference
    spec:
      containers:
      - name: yaie
        image: yaie:latest
        ports:
        - containerPort: 8000
        resources:
          limits:
            nvidia.com/gpu: 1
</code></pre>
<h2 id="9-performance-monitoring"><a class="header" href="#9-performance-monitoring">9. Performance Monitoring</a></h2>
<h3 id="key-metrics-to-track"><a class="header" href="#key-metrics-to-track">Key Metrics to Track</a></h3>
<ul>
<li><strong>Latency</strong>: Time from request to first token (TTFT) and time per token</li>
<li><strong>Throughput</strong>: Tokens generated per second</li>
<li><strong>GPU Utilization</strong>: Percentage of GPU time spent on computation</li>
<li><strong>Memory Usage</strong>: GPU and CPU memory consumption</li>
<li><strong>Queue Length</strong>: Number of requests waiting</li>
<li><strong>Error Rates</strong>: Percentage of failed requests</li>
</ul>
<h3 id="alerting"><a class="header" href="#alerting">Alerting</a></h3>
<pre><code class="language-python"># Set up alerts for critical conditions
if gpu_memory_usage &gt; 0.95:
    alert("High GPU memory usage")

if request_latency &gt; 5.0:  # 5 seconds
    alert("High request latency")

if error_rate &gt; 0.01:  # 1%
    alert("High error rate")
</code></pre>
<h2 id="10-cost-optimization"><a class="header" href="#10-cost-optimization">10. Cost Optimization</a></h2>
<h3 id="resource-management"><a class="header" href="#resource-management">Resource Management</a></h3>
<ul>
<li><strong>Auto-scaling</strong>: Scale workers based on demand</li>
<li><strong>Spot Instances</strong>: Use cheaper spot instances for non-critical workloads</li>
<li><strong>Right-sizing</strong>: Choose appropriate instance types for workload</li>
<li><strong>Batch Processing</strong>: Process offline requests in batches during low-traffic periods</li>
</ul>
<h3 id="model-selection-2"><a class="header" href="#model-selection-2">Model Selection</a></h3>
<pre><code class="language-python"># Choose appropriate model size for use case
small_models = ["gpt2", "DialoGPT-small"]  # Fast, low cost
medium_models = ["gpt2-medium", "DialoGPT-medium"]  # Balanced
large_models = ["gpt2-large", "DialoGPT-large"]  # High quality, expensive
</code></pre>
<h2 id="educational-focus-1"><a class="header" href="#educational-focus-1">Educational Focus</a></h2>
<p>Understanding production considerations helps you:</p>
<ol>
<li><strong>Bridge the gap</strong> between educational implementations and real-world systems</li>
<li><strong>Appreciate the complexity</strong> of production-grade inference engines</li>
<li><strong>Make informed decisions</strong> about trade-offs in your implementations</li>
<li><strong>Design for scalability</strong> from the beginning</li>
</ol>
<h2 id="from-mini-yaie-to-production"><a class="header" href="#from-mini-yaie-to-production">From Mini-YAIE to Production</a></h2>
<p>Mini-YAIE provides the foundation for understanding key concepts:</p>
<ul>
<li><strong>Continuous Batching</strong>: The core of efficient inference</li>
<li><strong>Memory Management</strong>: Critical for handling multiple requests</li>
<li><strong>Prefix Sharing</strong>: Advanced optimization for similar requests</li>
<li><strong>API Design</strong>: Standard interfaces for integration</li>
</ul>
<p>Production systems build on these foundations with:</p>
<ul>
<li><strong>Scalability</strong>: Handling thousands of concurrent requests</li>
<li><strong>Reliability</strong>: High availability and fault tolerance</li>
<li><strong>Observability</strong>: Comprehensive monitoring and logging</li>
<li><strong>Security</strong>: Authentication, authorization, and input validation</li>
</ul>
<p>By mastering the concepts in Mini-YAIE, you’ll be well-prepared to understand and contribute to production-grade inference systems like vLLM, SGLang, and TensorRT-LLM.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="references"><a class="header" href="#references">References</a></h1>
<ol>
<li><strong>SGLang</strong>: Efficient Execution of Structured Language Model Programs. <a href="https://github.com/sgl-project/sglang">Link</a></li>
<li><strong>vLLM</strong>: Easy, Fast, and Cheap LLM Serving with PagedAttention. <a href="https://github.com/vllm-project/vllm">Link</a></li>
<li><strong>FlashAttention</strong>: Fast and Memory-Efficient Exact Attention with IO-Awareness. <a href="https://github.com/Dao-AILab/flash-attention">Link</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="troubleshooting-1"><a class="header" href="#troubleshooting-1">Troubleshooting</a></h1>
<h2 id="cuda-kernel-not-found"><a class="header" href="#cuda-kernel-not-found">“CUDA kernel not found”</a></h2>
<ul>
<li>Ensure you ran <code>pip install -e .</code>.</li>
<li>Check if <code>nvcc</code> is in your path: <code>nvcc --version</code>.</li>
</ul>
<h2 id="outofmemoryerror"><a class="header" href="#outofmemoryerror">“OutOfMemoryError”</a></h2>
<ul>
<li>Decrease <code>max_batch_size</code>.</li>
<li>Decrease <code>kv_cache_manager</code> block count.</li>
</ul>
<h2 id="importerror-attempted-relative-import"><a class="header" href="#importerror-attempted-relative-import">“ImportError: attempted relative import…”</a></h2>
<ul>
<li>Ensure you are running the <code>yaie</code> command, or running python as a module <code>python -m src.cli.main</code>.</li>
<li>Do not run scripts directly like <code>python src/engine.py</code>.</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr-ef4e11c1.min.js"></script>
        <script src="mark-09e88c2c.min.js"></script>
        <script src="searcher-c2a407aa.js"></script>

        <script src="clipboard-1626706a.min.js"></script>
        <script src="highlight-abc7f01d.js"></script>
        <script src="book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>


    </div>
    </body>
</html>
