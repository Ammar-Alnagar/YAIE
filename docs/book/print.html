<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Mini-YAIE: Educational LLM Inference Engine</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Mini-YAIE: Educational LLM Inference Engine</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/yourusername/YAIE" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="welcome-to-mini-yaie"><a class="header" href="#welcome-to-mini-yaie">Welcome to Mini-YAIE</a></h1>
<p><strong>Mini-YAIE</strong> (Yet Another Inference Engine) is an educational project designed to demystify modern Large Language Model (LLM) inference engines.</p>
<p>Driven by the need for efficiency, modern engines like <strong>SGLang</strong>, <strong>vLLM</strong>, and <strong>TensorRT-LLM</strong> use sophisticated techniques to maximize GPU throughput and minimize latency. Mini-YAIE provides a simplified, clean implementation of these concepts, focusing on:</p>
<ul>
<li><strong>Continuous Batching</strong></li>
<li><strong>Paged KV Caching</strong></li>
<li><strong>Radix Attention (Prefix Sharing)</strong></li>
</ul>
<h2 id="how-to-use-this-guide"><a class="header" href="#how-to-use-this-guide">How to use this guide</a></h2>
<p>This documentation is structured to take you from high-level concepts to low-level implementation.</p>
<ol>
<li><strong>Core Concepts</strong>: Start here to understand the <em>why</em> and <em>what</em> of inference optimization.</li>
<li><strong>Architecture</strong>: Understand how the system components fit together.</li>
<li><strong>Implementation Guides</strong>: Step-by-step guides to implementing the missing "kernels" in Python and CUDA.</li>
</ol>
<h2 id="your-mission"><a class="header" href="#your-mission">Your Mission</a></h2>
<p>The codebase contains <strong>placeholders</strong> (<code>NotImplementedError</code>) for critical components. Your goal is to implement these components following this guide, turning Mini-YAIE from a skeleton into a fully functional inference engine.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h1>
<p>To successfully implement the kernels in Mini-YAIE, you should be familiar with:</p>
<h2 id="programming-languages"><a class="header" href="#programming-languages">Programming Languages</a></h2>
<ul>
<li><strong>Python (Intermediate)</strong>: Understanding of classes, inheritance, type hinting, and PyTorch tensors.</li>
<li><strong>C++ (Basic)</strong>: For reading and writing the CUDA kernels (though much of the boilerplate is provided).</li>
<li><strong>CUDA (Basic)</strong>: Understanding of the GPU execution model (blocks, threads, shared memory).</li>
</ul>
<h2 id="machine-learning-concepts"><a class="header" href="#machine-learning-concepts">Machine Learning Concepts</a></h2>
<ul>
<li><strong>Transformer Architecture</strong>: Queries, Keys, Values, Attention mechanism.</li>
<li><strong>Tensors</strong>: Shapes, dimensions, matrix multiplication.</li>
</ul>
<h2 id="tools"><a class="header" href="#tools">Tools</a></h2>
<ul>
<li><strong>Git</strong>: For version control.</li>
<li><strong>Linux/Unix Shell</strong>: For running commands.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="environment-setup"><a class="header" href="#environment-setup">Environment Setup</a></h1>
<h2 id="1-clone-the-repository"><a class="header" href="#1-clone-the-repository">1. Clone the Repository</a></h2>
<pre><code class="language-bash">git clone https://github.com/yourusername/YAIE.git
cd YAIE
</code></pre>
<h2 id="2-python-environment"><a class="header" href="#2-python-environment">2. Python Environment</a></h2>
<p>It is highly recommended to use a virtual environment.</p>
<pre><code class="language-bash">python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
pip install -e .
</code></pre>
<h2 id="3-cuda-requirements-optional"><a class="header" href="#3-cuda-requirements-optional">3. CUDA Requirements (Optional)</a></h2>
<p>To build and run the CUDA kernels, you need:</p>
<ul>
<li>NVIDIA GPU (Compute Capability 7.0+)</li>
<li>CUDA Toolkit 11.8+</li>
<li>PyTorch with CUDA support</li>
</ul>
<p>If you do not have a GPU, you can still implement the Python logic and the CPU fallback kernels.</p>
<h2 id="4-documentation-setup"><a class="header" href="#4-documentation-setup">4. Documentation Setup</a></h2>
<p>To serve this documentation locally:</p>
<ol>
<li>
<p><strong>Install mdbook</strong>:</p>
<pre><code class="language-bash"># If you have Rust/Cargo installed:
cargo install mdbook

# Or download the binary from their GitHub releases.
</code></pre>
</li>
<li>
<p><strong>Serve the docs</strong>:</p>
<pre><code class="language-bash">mdbook serve docs
</code></pre>
<p>Navigate to <code>http://localhost:3000</code> in your browser.</p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="llm-inference-the-basics"><a class="header" href="#llm-inference-the-basics">LLM Inference: The Basics</a></h1>
<p>Large Language Model (LLM) inference is the process of generating text from a trained model. It consists of two distinct phases.</p>
<h2 id="1-prefill-phase-the-prompt"><a class="header" href="#1-prefill-phase-the-prompt">1. Prefill Phase (The "Prompt")</a></h2>
<ul>
<li><strong>Input</strong>: The user's prompt (e.g., "Write a poem about cats").</li>
<li><strong>Operation</strong>: The model processes all input tokens in parallel.</li>
<li><strong>Output</strong>: The KV (Key-Value) cache for the prompt and the first generated token.</li>
<li><strong>Characteristic</strong>: Compute-bound. We maximize parallelism here.</li>
</ul>
<h2 id="2-decode-phase-the-generation"><a class="header" href="#2-decode-phase-the-generation">2. Decode Phase (The "Generation")</a></h2>
<ul>
<li><strong>Input</strong>: The previously generated token.</li>
<li><strong>Operation</strong>: The model generates one token at a time, autoregressively.</li>
<li><strong>Output</strong>: The next token and an updated KV cache.</li>
<li><strong>Characteristic</strong>: Memory-bound. We are limited by how fast we can move weights and KV cache from memory to the compute units.</li>
</ul>
<h2 id="the-kv-cache"><a class="header" href="#the-kv-cache">The KV Cache</a></h2>
<p>State management is crucial. Instead of re-computing the attention for all previous tokens at every step, we cache the <strong>Key</strong> and <strong>Value</strong> vectors for every token in the sequence. This is the <strong>KV Cache</strong>. Managing this cache efficiently is the main challenge of high-performance inference engines.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="continuous-batching"><a class="header" href="#continuous-batching">Continuous Batching</a></h1>
<h2 id="the-problem-static-batching"><a class="header" href="#the-problem-static-batching">The Problem: Static Batching</a></h2>
<p>In traditional deep learning (like training), we use static batches: all sequences in a batch must have the same length (padded to the max length).</p>
<ul>
<li><strong>Waste</strong>: Padding wastes computation and memory.</li>
<li><strong>Latency</strong>: We must wait for the longest sequence to finish generating before finishing the batch.</li>
</ul>
<h2 id="the-solution-continuous-batching-orca"><a class="header" href="#the-solution-continuous-batching-orca">The Solution: Continuous Batching (Orca)</a></h2>
<p>Introduced by the Orca paper, Continuous Batching (or Iteration-level Batching) decouples the implementation of a batch from the user's view.</p>
<ol>
<li><strong>Iteration Level</strong>: The engine runs one iteration (one forward pass) at a time.</li>
<li><strong>Dynamic Insertion</strong>: As soon as a request finishes, it enters the "Completed" state. A new request from the queue can immediately take its place in the next iteration.</li>
<li><strong>No Padding</strong>: We process only the valid tokens for each request.</li>
</ol>
<p>This significantly improves <strong>throughput</strong> (requests per second) without hurting <strong>latency</strong> (time per token) for individual requests.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="radix-attention-sglang"><a class="header" href="#radix-attention-sglang">Radix Attention (SGLang)</a></h1>
<p><strong>Radix Attention</strong> is the core innovation of SGLang. It optimizes the <strong>Prefill Phase</strong> by reusing computation from previous requests.</p>
<h2 id="the-intuition"><a class="header" href="#the-intuition">The Intuition</a></h2>
<p>If two users ask:</p>
<ol>
<li>"Write a Python script to <strong>scrape a website</strong>."</li>
<li>"Write a Python script to <strong>sort a list</strong>."</li>
</ol>
<p>They share the prefix "Write a Python script to ". In a standard engine, we would compute the KV cache for this prefix twice.</p>
<h2 id="the-radix-tree"><a class="header" href="#the-radix-tree">The Radix Tree</a></h2>
<p>SGLang maintains a <strong>Radix Tree</strong> (Trie) of all token sequences currently in the KV cache.</p>
<ul>
<li><strong>Nodes</strong>: Sequences of tokens.</li>
<li><strong>Edges</strong>: Transitions to new tokens.</li>
</ul>
<p>When a new request arrives, we map its prompt to the longest matching path in the Radix Tree.</p>
<ul>
<li><strong>Hit</strong>: We reuse the KV Cache for the matched part. The prefill only needs to compute the <em>new</em> suffix.</li>
<li><strong>Miss</strong>: We compute from scratch.</li>
</ul>
<h2 id="benefits"><a class="header" href="#benefits">Benefits</a></h2>
<ul>
<li><strong>Reduced Latency</strong>: "Time To First Token" (TTFT) is nearly zero for cached prefixes.</li>
<li><strong>Higher Throughput</strong>: Less computation required per request.</li>
<li><strong>Complex Workflows</strong>: Enables efficient multi-turn chat, few-shot learning, and tree-of-thought prompting.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="paged-attention-vllm"><a class="header" href="#paged-attention-vllm">Paged Attention (vLLM)</a></h1>
<p><strong>Paged Attention</strong> is the core innovation of vLLM. It optimizes the <strong>Decode Phase</strong> by managing memory like an Operating System.</p>
<h2 id="the-problem-memory-fragmentation"><a class="header" href="#the-problem-memory-fragmentation">The Problem: Memory Fragmentation</a></h2>
<p>Before vLLM, engines allocated contiguous memory for the maximum possible length of a request.</p>
<ul>
<li><strong>Internal Fragmentation</strong>: If a request was shorter than max length, memory was wasted.</li>
<li><strong>External Fragmentation</strong>: We couldn't fit a new request even if total free memory was sufficient, because no single contiguous block was large enough.</li>
</ul>
<h2 id="the-solution-pagiing"><a class="header" href="#the-solution-pagiing">The Solution: Pagiing</a></h2>
<p>Inspired by virtual memory in OS:</p>
<ol>
<li><strong>Blocks</strong>: Divide KV Cache into fixed-size blocks (e.g., 16 tokens per block).</li>
<li><strong>Non-Contiguous</strong>: Blocks can be stored anywhere in physical GPU memory.</li>
<li><strong>Mapping</strong>: A "Block Table" maps logical token positions to physical block addresses.</li>
</ol>
<h2 id="the-kernel"><a class="header" href="#the-kernel">The Kernel</a></h2>
<p>The Paged Attention kernel allows the Attention mechanism to read keys and values from these non-contiguous blocks on the fly, enabling near-zero memory waste.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="system-overview"><a class="header" href="#system-overview">System Overview</a></h1>
<p>Mini-YAIE follows a modular architecture similar to vLLM.</p>
<h2 id="high-level-components"><a class="header" href="#high-level-components">High-Level Components</a></h2>
<pre><code class="language-mermaid">graph TD
    User[User / API Client] --&gt; API[FastAPI Server (server/api.py)]
    API --&gt; Engine[Inference Engine (engine.py)]

    subgraph Core Logic
        Engine --&gt; Scheduler[SGLang Scheduler (core/sglang_scheduler.py)]
        Engine --&gt; MM[Memory Manager (kernels/kv_cache.py)]
        Engine --&gt; Model[LLM Model (HuggingFace)]
    end

    subgraph Kernels
        Scheduler --&gt; RadixTree[Radix Tree (kernels/radix_tree.py)]
        Model --&gt; RadixAttn[Radix Attention (kernels/radix_attention.py)]
        Model --&gt; PagedAttn[Paged Attention (kernels/cuda/paged_attention.cu)]
    end
</code></pre>
<h2 id="data-flow"><a class="header" href="#data-flow">Data Flow</a></h2>
<ol>
<li><strong>Request</strong>: User sends a prompt to the API.</li>
<li><strong>Scheduling</strong>: Scheduler analyzes the prompt, checks the Radix Tree for cached prefixes, and assigns a Request ID.</li>
<li><strong>Batching</strong>: <code>schedule_step</code> groups requests into Prefill (new) and Decode (running) batches.</li>
<li><strong>Execution</strong>: The Engine runs the model.
<ul>
<li><strong>Prefill</strong>: Computes initial KV cache for new prompts.</li>
<li><strong>Decode</strong>: Generates one token for running requests.</li>
</ul>
</li>
<li><strong>Memory</strong>: The Memory Manager allocates/frees GPU blocks as needed.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="the-engine-enginepy"><a class="header" href="#the-engine-enginepy">The Engine (<code>engine.py</code>)</a></h1>
<p>The <code>InferenceEngine</code> class is the conductor of the orchestra. It ties everything together.</p>
<h2 id="responsibilities"><a class="header" href="#responsibilities">Responsibilities</a></h2>
<ul>
<li><strong>Initialization</strong>: Loads the Model, Tokenizer, Scheduler, and Memory Manager.</li>
<li><strong>Request Entry</strong>: Accepts <code>generate()</code> calls and pushes them to the Scheduler.</li>
<li><strong>The Loop</strong>: Runs the continuous batching loop <code>_run_generation_loop</code>.</li>
</ul>
<h2 id="key-method-_run_generation_loop"><a class="header" href="#key-method-_run_generation_loop">Key Method: <code>_run_generation_loop</code></a></h2>
<p>This is the heart of continuous batching.</p>
<pre><code class="language-python">while has_active_requests:
    # 1. Ask Scheduler for work
    prefill_batch, decode_batch = scheduler.schedule_step()

    # 2. Run Prefill (Compute-bound)
    if prefill_batch:
        run_prefill(prefill_batch)

    # 3. Run Decode (Memory-bound)
    if decode_batch:
        run_decode(decode_batch)
</code></pre>
<p>In Mini-YAIE, the loop logic is simplified for education, but the structure mirrors production engines.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="the-scheduler-coresglang_schedulerpy"><a class="header" href="#the-scheduler-coresglang_schedulerpy">The Scheduler (<code>core/sglang_scheduler.py</code>)</a></h1>
<p>The <code>SGLangScheduler</code> is the "Brain". It decides <em>what</em> runs <em>when</em>.</p>
<h2 id="sglang-features"><a class="header" href="#sglang-features">SGLang Features</a></h2>
<p>Unlike a simple FIFO queue, this scheduler is aware of <strong>Radix Attention</strong>.</p>
<h3 id="prefix-hashing"><a class="header" href="#prefix-hashing">Prefix Hashing</a></h3>
<p>When a request arrives, <code>_calculate_prefix_hash</code> identifies its common prefix.</p>
<pre><code class="language-python"># Simplified Logic
prefix_hash = sha256(prompt)
</code></pre>
<p>In a full implementation, this uses the Radix Tree to find the precise token match.</p>
<h3 id="scheduling-policy"><a class="header" href="#scheduling-policy">Scheduling Policy</a></h3>
<ol>
<li><strong>Prioritize Decode</strong>: To minimize latency, we always try to run pending decode steps first.</li>
<li><strong>Fill with Prefill</strong>: If there is leftover GPU memory (or batch size capacity), we pull new requests from the queue for prefill.</li>
</ol>
<h2 id="state-machine"><a class="header" href="#state-machine">State Machine</a></h2>
<p>Requests transition through states:
<code>PENDING</code> -&gt; <code>RUNNING_PREFILL</code> -&gt; <code>SCHEDULED_DECODE</code> -&gt; <code>RUNNING_DECODE</code> -&gt; <code>COMPLETED</code></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="memory-manager-kernelskv_cachepy"><a class="header" href="#memory-manager-kernelskv_cachepy">Memory Manager (<code>kernels/kv_cache.py</code>)</a></h1>
<p>The <code>KVCacheManager</code> is the "Allocator". It manages the Paged KV Cache.</p>
<h2 id="the-block-table"><a class="header" href="#the-block-table">The Block Table</a></h2>
<p>Just like an OS manages RAM pages, this manager tracks GPU memory blocks.</p>
<ul>
<li><strong>Physical Blocks</strong>: Fixed-size tensors in GPU memory (e.g., <code>[num_blocks, block_size, head_dim]</code>).</li>
<li><strong>Logical Slots</strong>: The token positions in a sequence.</li>
</ul>
<h2 id="key-methods"><a class="header" href="#key-methods">Key Methods</a></h2>
<ul>
<li><code>allocate_blocks(request)</code>: Finds free physical blocks and assigns them to a request.</li>
<li><code>free_blocks(request)</code>: Returns blocks to the free pool.</li>
<li><code>get_block_table(request)</code>: Returns the mapping <code>[logical_idx -&gt; physical_idx]</code> for the attention kernel.</li>
</ul>
<p><strong>Student Task</strong>: You will implement the allocation logic in the Python Kernels phase.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="python-kernels-guide"><a class="header" href="#python-kernels-guide">Python Kernels Guide</a></h1>
<p>This section guides you through implementing the core Python logic "kernels". These are not CUDA kernels, but critical algorithmic components.</p>
<h2 id="your-tasks"><a class="header" href="#your-tasks">Your Tasks</a></h2>
<ol>
<li><strong>Radix Tree</strong>: Implement the Trie data structure for prefix matching.</li>
<li><strong>KV Cache Manager</strong>: Implement the block allocation strategy.</li>
<li><strong>Sampling</strong>: Implement the token sampling logic.</li>
</ol>
<p><strong>Why Python?</strong>
While computation happens in CUDA, the <em>logic</em> of memory management and prefix matching is complex and best handled in Python (or C++ CPU code) before dispatching to the GPU.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="radix-tree-kernelsradix_treepy"><a class="header" href="#radix-tree-kernelsradix_treepy">Radix Tree (<code>kernels/radix_tree.py</code>)</a></h1>
<h2 id="concept"><a class="header" href="#concept">Concept</a></h2>
<p>A <strong>Radix Tree</strong> (or Compressed Trie) is a space-optimized prefix tree.</p>
<h2 id="implementation-goal"><a class="header" href="#implementation-goal">Implementation Goal</a></h2>
<p>You need to implement the <code>RadixTree</code> class with:</p>
<h3 id="1-inserttoken_ids-request_id"><a class="header" href="#1-inserttoken_ids-request_id">1. <code>insert(token_ids, request_id)</code></a></h3>
<ul>
<li>Traverse the tree with <code>token_ids</code>.</li>
<li>If a path exists, follow it.</li>
<li>If tokens diverge, <strong>split</strong> the edge and create a new node.</li>
<li>Store the <code>request_id</code> at the leaf.</li>
</ul>
<h3 id="2-match_prefixtoken_ids"><a class="header" href="#2-match_prefixtoken_ids">2. <code>match_prefix(token_ids)</code></a></h3>
<ul>
<li>Traverse the tree to find the longest common prefix.</li>
<li>Return the <code>node_id</code> and the length of the match.</li>
<li>This tells the scheduler how many tokens we can skip computing!</li>
</ul>
<h3 id="3-removerequest_id"><a class="header" href="#3-removerequest_id">3. <code>remove(request_id)</code></a></h3>
<ul>
<li>When a request finishes, decrement reference counts.</li>
<li>If a node has no references, free its associated KV cache blocks.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kv-cache-manager-kernelskv_cachepy"><a class="header" href="#kv-cache-manager-kernelskv_cachepy">KV Cache Manager (<code>kernels/kv_cache.py</code>)</a></h1>
<h2 id="concept-1"><a class="header" href="#concept-1">Concept</a></h2>
<p>Manage a pool of fixed-size integer IDs representing GPU memory blocks.</p>
<h2 id="implementation-goal-1"><a class="header" href="#implementation-goal-1">Implementation Goal</a></h2>
<p>Implement <code>KVCacheManager</code>:</p>
<h3 id="1-__init__"><a class="header" href="#1-__init__">1. <code>__init__</code></a></h3>
<ul>
<li>Create a list of all available block indices: <code>free_blocks = [0, 1, ..., N-1]</code>.</li>
<li>Initialize an empty mapping <code>request_to_blocks = {}</code>.</li>
</ul>
<h3 id="2-allocaterequest-num_tokens"><a class="header" href="#2-allocaterequest-num_tokens">2. <code>allocate(request, num_tokens)</code></a></h3>
<ul>
<li>Calculate blocks needed: <code>ceil(num_tokens / block_size)</code>.</li>
<li>Pop that many indices from <code>free_blocks</code>.</li>
<li>Store them in <code>request_to_blocks</code>.</li>
<li><strong>Edge Case</strong>: If not enough blocks, raise OutOfMemory (or trigger eviction).</li>
</ul>
<h3 id="3-freerequest"><a class="header" href="#3-freerequest">3. <code>free(request)</code></a></h3>
<ul>
<li>Retrieve the blocks owned by the request.</li>
<li>Append them back to <code>free_blocks</code>.</li>
<li>Delete the request mapping.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="radix-attention-module"><a class="header" href="#radix-attention-module">Radix Attention Module</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="sampling-kernelssamplingpy"><a class="header" href="#sampling-kernelssamplingpy">Sampling (<code>kernels/sampling.py</code>)</a></h1>
<h2 id="concept-2"><a class="header" href="#concept-2">Concept</a></h2>
<p>Convert the model's raw output logits (probabilities) into a single token ID.</p>
<h2 id="implementation-goal-2"><a class="header" href="#implementation-goal-2">Implementation Goal</a></h2>
<p>Implement <code>SamplingKernel.sample</code>:</p>
<h3 id="1-temperature"><a class="header" href="#1-temperature">1. Temperature</a></h3>
<ul>
<li><code>logits = logits / temperature</code></li>
<li>Higher temp = flatter distribution (more random).</li>
<li>Lower temp = sharper distribution (more deterministic).</li>
</ul>
<h3 id="2-top-p-nucleus"><a class="header" href="#2-top-p-nucleus">2. Top-P (Nucleus)</a></h3>
<ul>
<li>Sort probabilities descending.</li>
<li>Compute cumulative sum.</li>
<li>Cut off where sum &gt; <code>top_p</code>.</li>
<li>Renormalize remaining probabilities.</li>
</ul>
<h3 id="3-selection"><a class="header" href="#3-selection">3. Selection</a></h3>
<ul>
<li><code>torch.multinomial(probs, 1)</code></li>
<li>Return the selected token ID.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cuda-setup"><a class="header" href="#cuda-setup">CUDA Setup</a></h1>
<p>Implementing custom CUDA kernels requires compiling C++/CUDA code and binding it to Python.</p>
<h2 id="setuppy"><a class="header" href="#setuppy"><code>setup.py</code></a></h2>
<p>We use <code>torch.utils.cpp_extension</code> to handle compilation.
The <code>setup.py</code> file in the root directory is already configured to look for kernels in <code>src/kernels/cuda/</code>.</p>
<h3 id="triggering-compilation"><a class="header" href="#triggering-compilation">Triggering Compilation</a></h3>
<p>To compile your kernels, simply run:</p>
<pre><code class="language-bash">pip install -e .
</code></pre>
<p>This command invokes <code>nvcc</code> (NVIDIA CUDA Compiler) on your <code>.cu</code> files.</p>
<h3 id="using-the-kernels"><a class="header" href="#using-the-kernels">Using the Kernels</a></h3>
<p>Once compiled, you can import them in Python:</p>
<pre><code class="language-python">import mini_yaie_kernels

# Call your C++ function
mini_yaie_kernels.flash_attention.forward(...)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="memory-operations-kernelscudamemory_opscu"><a class="header" href="#memory-operations-kernelscudamemory_opscu">Memory Operations (<code>kernels/cuda/memory_ops.cu</code>)</a></h1>
<h2 id="concept-3"><a class="header" href="#concept-3">Concept</a></h2>
<p>Moving data between different GPU memory locations is a frequent operation in Paged Attention.</p>
<h2 id="implementation-goal-3"><a class="header" href="#implementation-goal-3">Implementation Goal</a></h2>
<p>Implement <code>copy_blocks_kernel</code>:</p>
<h3 id="signature"><a class="header" href="#signature">Signature</a></h3>
<pre><code class="language-cpp">void copy_blocks_kernel(
    torch::Tensor key_cache,      // [num_blocks, block_size, head_dim]
    torch::Tensor value_cache,    // [num_blocks, block_size, head_dim]
    torch::Tensor block_mapping,  // [num_mappings, 2] (src, dst)
    int num_mappings
);
</code></pre>
<h3 id="logic"><a class="header" href="#logic">Logic</a></h3>
<ol>
<li><strong>Parallelism</strong>: Launch one thread per token to copy.</li>
<li><strong>Indexing</strong>:
<ul>
<li><code>mapping_idx = blockIdx.x</code></li>
<li><code>src_block = block_mapping[mapping_idx][0]</code></li>
<li><code>dst_block = block_mapping[mapping_idx][1]</code></li>
</ul>
</li>
<li><strong>Copy</strong>:
<ul>
<li>Read <code>key/value</code> from <code>src_block</code> at <code>threadIdx</code> offset.</li>
<li>Write to <code>dst_block</code>.</li>
</ul>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="flash-attention-kernelscudaflash_attentioncu"><a class="header" href="#flash-attention-kernelscudaflash_attentioncu">Flash Attention (<code>kernels/cuda/flash_attention.cu</code>)</a></h1>
<h2 id="concept-4"><a class="header" href="#concept-4">Concept</a></h2>
<p>A standard attention implementation is $O(N^2)$ in memory usage. Flash Attention uses tiling to compute attention in constant memory.</p>
<h2 id="implementation-goal-4"><a class="header" href="#implementation-goal-4">Implementation Goal</a></h2>
<p>Implement <code>flash_attention_forward</code>:</p>
<h3 id="algorithm-simplified"><a class="header" href="#algorithm-simplified">Algorithm (simplified)</a></h3>
<ol>
<li><strong>Tiling</strong>: Load a block of Queries (Q) into shared memory (SRAM).</li>
<li><strong>Loop</strong>: Iterate over blocks of Keys (K) and Values (V) from HBM (Global Memory).
<ul>
<li>Load K, V block into SRAM.</li>
<li>Compute QK^T (Attention Scores).</li>
<li>Apply Softmax (using online softmax scaling).</li>
<li>Compute Score * V.</li>
<li>Accumulate result to Output.</li>
</ul>
</li>
<li><strong>Write Output</strong>: Store final result to HBM.</li>
</ol>
<blockquote>
<p><strong>Note</strong>: For this educational project, a naive CUDA implementation is acceptable if tiling is too complex.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="paged-attention-kernelscudapaged_attentioncu"><a class="header" href="#paged-attention-kernelscudapaged_attentioncu">Paged Attention (<code>kernels/cuda/paged_attention.cu</code>)</a></h1>
<h2 id="concept-5"><a class="header" href="#concept-5">Concept</a></h2>
<p>Compute attention where K and V are stored in non-contiguous blocks.</p>
<h2 id="implementation-goal-5"><a class="header" href="#implementation-goal-5">Implementation Goal</a></h2>
<p>Implement <code>paged_attention_kernel</code>:</p>
<h3 id="inputs"><a class="header" href="#inputs">Inputs</a></h3>
<ul>
<li><code>block_tables</code>: A tensor mapping <code>[request_id, logical_block_idx] -&gt; physical_block_idx</code>.</li>
</ul>
<h3 id="logic-1"><a class="header" href="#logic-1">Logic</a></h3>
<ol>
<li><strong>Thread Mapping</strong>: Each thread block handles one sequence (request).</li>
<li><strong>Gathering</strong>:
<ul>
<li>Instead of <code>K[i]</code>, we must compute the physical address.</li>
<li><code>block_number = block_tables[request_id][token_index / block_size]</code></li>
<li><code>block_offset = token_index % block_size</code></li>
<li><code>physical_addr = base_ptr + block_number * stride + block_offset</code></li>
</ul>
</li>
<li><strong>Attention</strong>:
<ul>
<li>Load K, V using the calculated physical addresses.</li>
<li>Compute Attention as usual.</li>
</ul>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="radix-operations-kernelscudaradix_opscu"><a class="header" href="#radix-operations-kernelscudaradix_opscu">Radix Operations (<code>kernels/cuda/radix_ops.cu</code>)</a></h1>
<h2 id="concept-6"><a class="header" href="#concept-6">Concept</a></h2>
<p>If we have a Radix Tree, we can optimize attention even further by knowing exactly which tokens are shared.</p>
<h2 id="implementation-goal-6"><a class="header" href="#implementation-goal-6">Implementation Goal</a></h2>
<p>This is an advanced extension.</p>
<h3 id="logic-2"><a class="header" href="#logic-2">Logic</a></h3>
<ol>
<li><strong>Tree Traversal on GPU</strong>: Mapping the Radix Tree structure to a GPU-friendly format (e.g., flattened arrays).</li>
<li><strong>Prefix Matching</strong>: A kernel that takes a batch of prompts and quickly identifies the longest common prefix node ID for each.</li>
</ol>
<p><em>Note: In the simplified version, this logic is often kept in CPU (Python) and only the KV indices are passed to the GPU.</em></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="api-endpoints-serverapipy"><a class="header" href="#api-endpoints-serverapipy">API Endpoints (<code>server/api.py</code>)</a></h1>
<h2 id="openai-compatibility"><a class="header" href="#openai-compatibility">OpenAI Compatibility</a></h2>
<p>Mini-YAIE strives to be drop-in compatible with OpenAI's API format.</p>
<h3 id="post-v1chatcompletions"><a class="header" href="#post-v1chatcompletions">POST <code>/v1/chat/completions</code></a></h3>
<p><strong>Request Body</strong>:</p>
<pre><code class="language-json">{
  "model": "gpt2",
  "messages": [{ "role": "user", "content": "Hello!" }],
  "temperature": 0.7,
  "max_tokens": 100
}
</code></pre>
<p><strong>Response</strong>:</p>
<pre><code class="language-json">{
  "id": "chat-123",
  "object": "chat.completion",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Hi there!"
      },
      "finish_reason": "stop"
    }
  ]
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cli-usage"><a class="header" href="#cli-usage">CLI Usage</a></h1>
<h2 id="starting-the-server"><a class="header" href="#starting-the-server">Starting the Server</a></h2>
<pre><code class="language-bash">yaie serve --model gpt2 --port 8000
</code></pre>
<h2 id="interactive-chat"><a class="header" href="#interactive-chat">Interactive Chat</a></h2>
<pre><code class="language-bash">yaie chat --model gpt2
</code></pre>
<h2 id="arguments"><a class="header" href="#arguments">Arguments</a></h2>
<ul>
<li><code>--model</code>: Name of the HuggingFace model or path to local model.</li>
<li><code>--max-batch-size</code>: Limit the number of concurrent requests.</li>
<li><code>--gpu-memory-utilization</code>: Fraction of GPU memory to use for KV cache (default 0.9).</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="production-considerations"><a class="header" href="#production-considerations">Production Considerations</a></h1>
<p>While Mini-YAIE is educational, here is what you would need for production:</p>
<h2 id="1-batching--latency"><a class="header" href="#1-batching--latency">1. Batching &amp; Latency</a></h2>
<ul>
<li><strong>Timeouts</strong>: Requests waiting too long in the queue should be handled.</li>
<li><strong>Preemption</strong>: If high-priority requests come in, lower priority ones should be paused (swapped to CPU).</li>
</ul>
<h2 id="2-distributed-inference"><a class="header" href="#2-distributed-inference">2. Distributed Inference</a></h2>
<ul>
<li><strong>Tensor Parallelism</strong>: Splitting the model weights across multiple GPUs (Megatron-LM style).</li>
<li><strong>Pipeline Parallelism</strong>: Splitting layers across GPUs.</li>
</ul>
<h2 id="3-quantization"><a class="header" href="#3-quantization">3. Quantization</a></h2>
<ul>
<li><strong>FP8 / INT8</strong>: Running with lower precision to save memory and increase compute speed (using library like <code>bitsandbytes</code>).</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="references"><a class="header" href="#references">References</a></h1>
<ol>
<li><strong>SGLang</strong>: Efficient Execution of Structured Language Model Programs. <a href="https://github.com/sgl-project/sglang">Link</a></li>
<li><strong>vLLM</strong>: Easy, Fast, and Cheap LLM Serving with PagedAttention. <a href="https://github.com/vllm-project/vllm">Link</a></li>
<li><strong>FlashAttention</strong>: Fast and Memory-Efficient Exact Attention with IO-Awareness. <a href="https://github.com/Dao-AILab/flash-attention">Link</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="troubleshooting"><a class="header" href="#troubleshooting">Troubleshooting</a></h1>
<h2 id="cuda-kernel-not-found"><a class="header" href="#cuda-kernel-not-found">"CUDA kernel not found"</a></h2>
<ul>
<li>Ensure you ran <code>pip install -e .</code>.</li>
<li>Check if <code>nvcc</code> is in your path: <code>nvcc --version</code>.</li>
</ul>
<h2 id="outofmemoryerror"><a class="header" href="#outofmemoryerror">"OutOfMemoryError"</a></h2>
<ul>
<li>Decrease <code>max_batch_size</code>.</li>
<li>Decrease <code>kv_cache_manager</code> block count.</li>
</ul>
<h2 id="importerror-attempted-relative-import"><a class="header" href="#importerror-attempted-relative-import">"ImportError: attempted relative import..."</a></h2>
<ul>
<li>Ensure you are running the <code>yaie</code> command, or running python as a module <code>python -m src.cli.main</code>.</li>
<li>Do not run scripts directly like <code>python src/engine.py</code>.</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <!-- Livereload script (if served using the cli tool) -->
        <script>
            const wsProtocol = location.protocol === 'https:' ? 'wss:' : 'ws:';
            const wsAddress = wsProtocol + "//" + location.host + "/" + "__livereload";
            const socket = new WebSocket(wsAddress);
            socket.onmessage = function (event) {
                if (event.data === "reload") {
                    socket.close();
                    location.reload();
                }
            };

            window.onbeforeunload = function() {
                socket.close();
            }
        </script>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>


    </div>
    </body>
</html>
