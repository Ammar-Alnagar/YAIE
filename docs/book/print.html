<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Mini-YAIE: Educational LLM Inference Engine</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        <!-- Custom Head with Mermaid Support -->
        <script type="module">
            import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10.9.1/dist/mermaid.esm.min.mjs';
            mermaid.initialize({ startOnLoad: false });
        
            document.addEventListener('DOMContentLoaded', async () => {
                const elements = document.querySelectorAll('code.language-mermaid');
                for (const element of elements) {
                    const pre = element.parentElement;
                    const div = document.createElement('div');
                    div.classList.add('mermaid');
                    div.textContent = element.textContent;
                    pre.replaceWith(div);
                }
                await mermaid.run({
                    querySelector: '.mermaid'
                });
            });
        </script>

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Mini-YAIE: Educational LLM Inference Engine</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/ammar-alnagar/Mini-YAIE" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="welcome-to-mini-yaie"><a class="header" href="#welcome-to-mini-yaie">Welcome to Mini-YAIE</a></h1>
<p><strong>Mini-YAIE</strong> (Yet Another Inference Engine) is an educational project designed to demystify modern Large Language Model (LLM) inference engines.</p>
<p>Driven by the need for efficiency, modern engines like <strong>SGLang</strong>, <strong>vLLM</strong>, and <strong>TensorRT-LLM</strong> use sophisticated techniques to maximize GPU throughput and minimize latency. Mini-YAIE provides a simplified, clean implementation of these concepts, focusing on:</p>
<ul>
<li><strong>Continuous Batching</strong></li>
<li><strong>Paged KV Caching</strong></li>
<li><strong>Radix Attention (Prefix Sharing)</strong></li>
</ul>
<h2 id="how-to-use-this-guide"><a class="header" href="#how-to-use-this-guide">How to use this guide</a></h2>
<p>This documentation is structured to take you from high-level concepts to low-level implementation.</p>
<ol>
<li><strong>Core Concepts</strong>: Start here to understand the <em>why</em> and <em>what</em> of inference optimization.</li>
<li><strong>Architecture</strong>: Understand how the system components fit together.</li>
<li><strong>Implementation Guides</strong>: Step-by-step guides to implementing the missing "kernels" in Python and CUDA.</li>
</ol>
<h2 id="your-mission"><a class="header" href="#your-mission">Your Mission</a></h2>
<p>The codebase contains <strong>placeholders</strong> (<code>NotImplementedError</code>) for critical components. Your goal is to implement these components following this guide, turning Mini-YAIE from a skeleton into a fully functional inference engine.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h1>
<p>To successfully implement the kernels in Mini-YAIE, you should be familiar with:</p>
<h2 id="programming-languages"><a class="header" href="#programming-languages">Programming Languages</a></h2>
<ul>
<li><strong>Python (Intermediate)</strong>: Understanding of classes, inheritance, type hinting, and PyTorch tensors.</li>
<li><strong>C++ (Basic)</strong>: For reading and writing the CUDA kernels (though much of the boilerplate is provided).</li>
<li><strong>CUDA (Basic)</strong>: Understanding of the GPU execution model (blocks, threads, shared memory).</li>
</ul>
<h2 id="machine-learning-concepts"><a class="header" href="#machine-learning-concepts">Machine Learning Concepts</a></h2>
<ul>
<li><strong>Transformer Architecture</strong>: Queries, Keys, Values, Attention mechanism.</li>
<li><strong>Tensors</strong>: Shapes, dimensions, matrix multiplication.</li>
</ul>
<h2 id="tools"><a class="header" href="#tools">Tools</a></h2>
<ul>
<li><strong>Git</strong>: For version control.</li>
<li><strong>Linux/Unix Shell</strong>: For running commands.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="environment-setup"><a class="header" href="#environment-setup">Environment Setup</a></h1>
<h2 id="1-clone-the-repository"><a class="header" href="#1-clone-the-repository">1. Clone the Repository</a></h2>
<pre><code class="language-bash">git clone https://github.com/yourusername/YAIE.git
cd YAIE
</code></pre>
<h2 id="2-python-environment"><a class="header" href="#2-python-environment">2. Python Environment</a></h2>
<p>It is highly recommended to use a virtual environment.</p>
<pre><code class="language-bash">python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
pip install -e .
</code></pre>
<h2 id="3-cuda-requirements-optional"><a class="header" href="#3-cuda-requirements-optional">3. CUDA Requirements (Optional)</a></h2>
<p>To build and run the CUDA kernels, you need:</p>
<ul>
<li>NVIDIA GPU (Compute Capability 7.0+)</li>
<li>CUDA Toolkit 11.8+</li>
<li>PyTorch with CUDA support</li>
</ul>
<p>If you do not have a GPU, you can still implement the Python logic and the CPU fallback kernels.</p>
<h2 id="4-documentation-setup"><a class="header" href="#4-documentation-setup">4. Documentation Setup</a></h2>
<p>To serve this documentation locally:</p>
<ol>
<li>
<p><strong>Install mdbook</strong>:</p>
<pre><code class="language-bash"># If you have Rust/Cargo installed:
cargo install mdbook

# Or download the binary from their GitHub releases.
</code></pre>
</li>
<li>
<p><strong>Serve the docs</strong>:</p>
<pre><code class="language-bash">mdbook serve docs
</code></pre>
<p>Navigate to <code>http://localhost:3000</code> in your browser.</p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="llm-inference-the-basics"><a class="header" href="#llm-inference-the-basics">LLM Inference: The Basics</a></h1>
<p>Large Language Model (LLM) inference is the process of generating text from a trained model. It consists of two distinct phases.</p>
<h2 id="1-prefill-phase-the-prompt"><a class="header" href="#1-prefill-phase-the-prompt">1. Prefill Phase (The "Prompt")</a></h2>
<ul>
<li><strong>Input</strong>: The user's prompt (e.g., "Write a poem about cats").</li>
<li><strong>Operation</strong>: The model processes all input tokens in parallel.</li>
<li><strong>Output</strong>: The KV (Key-Value) cache for the prompt and the first generated token.</li>
<li><strong>Characteristic</strong>: Compute-bound. We maximize parallelism here.</li>
</ul>
<h2 id="the-process-visualized"><a class="header" href="#the-process-visualized">The Process Visualized</a></h2>
<pre><code class="language-mermaid">sequenceDiagram
    participant U as User
    participant E as Engine
    participant M as Model

    rect rgb(200, 220, 255)
    note right of U: Prefill Phase (Parallel)
    U-&gt;&gt;E: Prompt: "A B C"
    E-&gt;&gt;M: Forward(["A", "B", "C"])
    M--&gt;&gt;E: KV Cache + Logits(C)
    end

    rect rgb(220, 255, 200)
    note right of U: Decode Phase (Serial)
    loop Until EOS
        E-&gt;&gt;M: Forward([Last Token])
        M--&gt;&gt;E: Update KV + Logits
        E-&gt;&gt;E: Sample Next Token
    end
    end
    E-&gt;&gt;U: Response
</code></pre>
<h2 id="2-decode-phase-the-generation"><a class="header" href="#2-decode-phase-the-generation">2. Decode Phase (The "Generation")</a></h2>
<ul>
<li><strong>Input</strong>: The previously generated token.</li>
<li><strong>Operation</strong>: The model generates one token at a time, autoregressively.</li>
<li><strong>Output</strong>: The next token and an updated KV cache.</li>
<li><strong>Characteristic</strong>: Memory-bound. We are limited by how fast we can move weights and KV cache from memory to the compute units.</li>
</ul>
<h2 id="the-kv-cache"><a class="header" href="#the-kv-cache">The KV Cache</a></h2>
<p>State management is crucial. Instead of re-computing the attention for all previous tokens at every step, we cache the <strong>Key</strong> and <strong>Value</strong> vectors for every token in the sequence. This is the <strong>KV Cache</strong>. Managing this cache efficiently is the main challenge of high-performance inference engines.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="continuous-batching"><a class="header" href="#continuous-batching">Continuous Batching</a></h1>
<h2 id="the-problem-static-batching"><a class="header" href="#the-problem-static-batching">The Problem: Static Batching</a></h2>
<p>In traditional deep learning (like training), we use static batches: all sequences in a batch must have the same length (padded to the max length).</p>
<ul>
<li><strong>Waste</strong>: Padding wastes computation and memory.</li>
<li><strong>Latency</strong>: We must wait for the longest sequence to finish generating before finishing the batch.</li>
</ul>
<h2 id="visualizing-the-difference"><a class="header" href="#visualizing-the-difference">Visualizing the Difference</a></h2>
<pre><code class="language-mermaid">gantt
    title Static Batching (Inefficient)
    dateFormat X
    axisFormat %s

    section Batch 1
    Req A (Short) :done, a1, 0, 2
    Padding       :crit, 2, 4
    Req B (Long)  :active, b1, 0, 4

    section Batch 2
    Req C :c1, 4, 6
</code></pre>
<pre><code class="language-mermaid">gantt
    title Continuous Batching (Efficient)
    dateFormat X
    axisFormat %s

    section GPU Stream
    Req A (Short) :done, a1, 0, 2
    Req C (New!)  :active, c1, 2, 4

    section GPU Stream 2
    Req B (Long)  :active, b1, 0, 4
</code></pre>
<h2 id="the-solution-continuous-batching-orca"><a class="header" href="#the-solution-continuous-batching-orca">The Solution: Continuous Batching (Orca)</a></h2>
<p>Introduced by the Orca paper, Continuous Batching (or Iteration-level Batching) decouples the implementation of a batch from the user's view.</p>
<ol>
<li><strong>Iteration Level</strong>: The engine runs one iteration (one forward pass) at a time.</li>
<li><strong>Dynamic Insertion</strong>: As soon as a request finishes, it enters the "Completed" state. A new request from the queue can immediately take its place in the next iteration.</li>
<li><strong>No Padding</strong>: We process only the valid tokens for each request.</li>
</ol>
<p>This significantly improves <strong>throughput</strong> (requests per second) without hurting <strong>latency</strong> (time per token) for individual requests.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="radix-attention-sglang"><a class="header" href="#radix-attention-sglang">Radix Attention (SGLang)</a></h1>
<p><strong>Radix Attention</strong> is the core innovation of SGLang. It optimizes the <strong>Prefill Phase</strong> by reusing computation from previous requests.</p>
<h2 id="the-intuition"><a class="header" href="#the-intuition">The Intuition</a></h2>
<p>If two users ask:</p>
<ol>
<li>"Write a Python script to <strong>scrape a website</strong>."</li>
<li>"Write a Python script to <strong>sort a list</strong>."</li>
</ol>
<p>They share the prefix "Write a Python script to ". In a standard engine, we would compute the KV cache for this prefix twice.</p>
<pre><code class="language-mermaid">graph TD
    classDef shared fill:#aaffaa,stroke:#333,stroke-width:2px;
    classDef unique fill:#ffaaaa,stroke:#333,stroke-width:2px;

    Root((Root)) --&gt; Node1["Write a Python script to"]:::shared
    Node1 --&gt; Node2["scrape a website"]:::unique
    Node1 --&gt; Node3["sort a list"]:::unique

    style Node1 fill:#aaffaa
</code></pre>
<h2 id="the-radix-tree"><a class="header" href="#the-radix-tree">The Radix Tree</a></h2>
<p>SGLang maintains a <strong>Radix Tree</strong> (Trie) of all token sequences currently in the KV cache.</p>
<ul>
<li><strong>Nodes</strong>: Sequences of tokens.</li>
<li><strong>Edges</strong>: Transitions to new tokens.</li>
</ul>
<p>When a new request arrives, we map its prompt to the longest matching path in the Radix Tree.</p>
<ul>
<li><strong>Hit</strong>: We reuse the KV Cache for the matched part. The prefill only needs to compute the <em>new</em> suffix.</li>
<li><strong>Miss</strong>: We compute from scratch.</li>
</ul>
<h2 id="benefits"><a class="header" href="#benefits">Benefits</a></h2>
<ul>
<li><strong>Reduced Latency</strong>: "Time To First Token" (TTFT) is nearly zero for cached prefixes.</li>
<li><strong>Higher Throughput</strong>: Less computation required per request.</li>
<li><strong>Complex Workflows</strong>: Enables efficient multi-turn chat, few-shot learning, and tree-of-thought prompting.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="paged-attention-vllm"><a class="header" href="#paged-attention-vllm">Paged Attention (vLLM)</a></h1>
<p><strong>Paged Attention</strong> is the core innovation of vLLM. It optimizes the <strong>Decode Phase</strong> by managing memory like an Operating System.</p>
<h2 id="the-problem-memory-fragmentation"><a class="header" href="#the-problem-memory-fragmentation">The Problem: Memory Fragmentation</a></h2>
<p>Before vLLM, engines allocated contiguous memory for the maximum possible length of a request.</p>
<ul>
<li><strong>Internal Fragmentation</strong>: If a request was shorter than max length, memory was wasted.</li>
<li><strong>External Fragmentation</strong>: We couldn't fit a new request even if total free memory was sufficient, because no single contiguous block was large enough.</li>
</ul>
<h2 id="the-solution-paging"><a class="header" href="#the-solution-paging">The Solution: Paging</a></h2>
<p>Inspired by virtual memory in OS:</p>
<ol>
<li><strong>Blocks</strong>: Divide KV Cache into fixed-size blocks (e.g., 16 tokens per block).</li>
<li><strong>Non-Contiguous</strong>: Blocks can be stored anywhere in physical GPU memory.</li>
<li><strong>Mapping</strong>: A "Block Table" maps logical token positions to physical block addresses.</li>
</ol>
<pre><code class="language-mermaid">graph LR
    subgraph Logical["Logical Sequence (Request)"]
        L0[Block 0: "Hello"]
        L1[Block 1: "World"]
        L2[Block 2: "!"]
    end

    subgraph Table["Page Table"]
        T0[0 -&gt; 7]
        T1[1 -&gt; 2]
        T2[2 -&gt; 9]
    end

    subgraph Physical["GPU Memory (Physical Blocks)"]
        B0[Block 0]
        B1[Block 1]
        B2[Block 2: "World"]:::used
        B3...
        B7[Block 7: "Hello"]:::used
        B8...
        B9[Block 9: "!"]:::used
    end

    L0 --&gt; T0 --&gt; B7
    L1 --&gt; T1 --&gt; B2
    L2 --&gt; T2 --&gt; B9

    classDef used fill:#aaffaa;
</code></pre>
<h2 id="the-kernel"><a class="header" href="#the-kernel">The Kernel</a></h2>
<p>The Paged Attention kernel allows the Attention mechanism to read keys and values from these non-contiguous blocks on the fly, enabling near-zero memory waste.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="system-overview"><a class="header" href="#system-overview">System Overview</a></h1>
<p>Mini-YAIE follows a modular architecture similar to vLLM.</p>
<h2 id="high-level-components"><a class="header" href="#high-level-components">High-Level Components</a></h2>
<pre><code class="language-mermaid">graph TD
    User[User / API Client] --&gt; API[FastAPI Server (server/api.py)]
    API --&gt; Engine[Inference Engine (engine.py)]

    subgraph Core Logic
        Engine --&gt; Scheduler[SGLang Scheduler (core/sglang_scheduler.py)]
        Engine --&gt; MM[Memory Manager (kernels/kv_cache.py)]
        Engine --&gt; Model[LLM Model (HuggingFace)]
    end

    subgraph Kernels
        Scheduler --&gt; RadixTree[Radix Tree (kernels/radix_tree.py)]
        Model --&gt; RadixAttn[Radix Attention (kernels/radix_attention.py)]
        Model --&gt; PagedAttn[Paged Attention (kernels/cuda/paged_attention.cu)]
    end
</code></pre>
<h2 id="data-flow"><a class="header" href="#data-flow">Data Flow</a></h2>
<ol>
<li><strong>Request</strong>: User sends a prompt to the API.</li>
<li><strong>Scheduling</strong>: Scheduler analyzes the prompt, checks the Radix Tree for cached prefixes, and assigns a Request ID.</li>
<li><strong>Batching</strong>: <code>schedule_step</code> groups requests into Prefill (new) and Decode (running) batches.</li>
<li><strong>Execution</strong>: The Engine runs the model.
<ul>
<li><strong>Prefill</strong>: Computes initial KV cache for new prompts.</li>
<li><strong>Decode</strong>: Generates one token for running requests.</li>
</ul>
</li>
<li><strong>Memory</strong>: The Memory Manager allocates/frees GPU blocks as needed.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="the-engine-enginepy"><a class="header" href="#the-engine-enginepy">The Engine (<code>engine.py</code>)</a></h1>
<p>The <code>InferenceEngine</code> class is the conductor of the orchestra. It ties everything together.</p>
<h2 id="the-process-visualized-1"><a class="header" href="#the-process-visualized-1">The Process Visualized</a></h2>
<pre><code class="language-mermaid">graph TD
    User --&gt;|Prompts| Engine
    Engine --&gt;|Request| Scheduler
    Scheduler --&gt;|Batch| Engine
    Engine --&gt;|Prefill| Model
    Engine --&gt;|Decode| Model
    Model --&gt;|Logits| Sampling
    Sampling --&gt;|Token| Engine
</code></pre>
<h2 id="responsibilities"><a class="header" href="#responsibilities">Responsibilities</a></h2>
<ul>
<li><strong>Initialization</strong>: Loads the Model, Tokenizer, Scheduler, and Memory Manager.</li>
<li><strong>Request Entry</strong>: Accepts <code>generate()</code> calls and pushes them to the Scheduler.</li>
<li><strong>The Loop</strong>: Runs the continuous batching loop <code>_run_generation_loop</code>.</li>
</ul>
<h2 id="key-method-_run_generation_loop"><a class="header" href="#key-method-_run_generation_loop">Key Method: <code>_run_generation_loop</code></a></h2>
<p>This is the heart of continuous batching.</p>
<pre><code class="language-python">while has_active_requests:
    # 1. Ask Scheduler for work
    prefill_batch, decode_batch = scheduler.schedule_step()

    # 2. Run Prefill (Compute-bound)
    if prefill_batch:
        run_prefill(prefill_batch)

    # 3. Run Decode (Memory-bound)
    if decode_batch:
        run_decode(decode_batch)
</code></pre>
<p>In Mini-YAIE, the loop logic is simplified for education, but the structure mirrors production engines.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="the-scheduler-coresglang_schedulerpy"><a class="header" href="#the-scheduler-coresglang_schedulerpy">The Scheduler (<code>core/sglang_scheduler.py</code>)</a></h1>
<p>The <code>SGLangScheduler</code> is the "Brain". It decides <em>what</em> runs <em>when</em>.</p>
<h2 id="sglang-features"><a class="header" href="#sglang-features">SGLang Features</a></h2>
<p>Unlike a simple FIFO queue, this scheduler is aware of <strong>Radix Attention</strong>.</p>
<h3 id="prefix-hashing"><a class="header" href="#prefix-hashing">Prefix Hashing</a></h3>
<p>When a request arrives, <code>_calculate_prefix_hash</code> identifies its common prefix.</p>
<pre><code class="language-python"># Simplified Logic
prefix_hash = sha256(prompt)
</code></pre>
<p>In a full implementation, this uses the Radix Tree to find the precise token match.</p>
<h3 id="scheduling-policy"><a class="header" href="#scheduling-policy">Scheduling Policy</a></h3>
<ol>
<li><strong>Prioritize Decode</strong>: To minimize latency, we always try to run pending decode steps first.</li>
<li><strong>Fill with Prefill</strong>: If there is leftover GPU memory (or batch size capacity), we pull new requests from the queue for prefill.</li>
</ol>
<pre><code class="language-mermaid">stateDiagram-v2
    [*] --&gt; Pending
    Pending --&gt; RunningPrefill : schedule_step()
    RunningPrefill --&gt; ScheduledDecode : process_prefill()
    ScheduledDecode --&gt; RunningDecode : schedule_step()
    RunningDecode --&gt; ScheduledDecode : Not Finished
    RunningDecode --&gt; Completed : Finished
    Completed --&gt; [*]
</code></pre>
<h2 id="state-machine"><a class="header" href="#state-machine">State Machine</a></h2>
<p>Requests transition through states:
<code>PENDING</code> -&gt; <code>RUNNING_PREFILL</code> -&gt; <code>SCHEDULED_DECODE</code> -&gt; <code>RUNNING_DECODE</code> -&gt; <code>COMPLETED</code></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="memory-manager-kernelskv_cachepy"><a class="header" href="#memory-manager-kernelskv_cachepy">Memory Manager (<code>kernels/kv_cache.py</code>)</a></h1>
<p>The <code>KVCacheManager</code> is the "Allocator". It manages the Paged KV Cache.</p>
<h2 id="the-block-table"><a class="header" href="#the-block-table">The Block Table</a></h2>
<p>Just like an OS manages RAM pages, this manager tracks GPU memory blocks.</p>
<ul>
<li><strong>Physical Blocks</strong>: Fixed-size tensors in GPU memory (e.g., <code>[num_blocks, block_size, head_dim]</code>).</li>
<li><strong>Logical Slots</strong>: The token positions in a sequence.</li>
</ul>
<h2 id="key-methods"><a class="header" href="#key-methods">Key Methods</a></h2>
<ul>
<li><code>allocate_blocks(request)</code>: Finds free physical blocks and assigns them to a request.</li>
<li><code>free_blocks(request)</code>: Returns blocks to the free pool.</li>
<li><code>get_block_table(request)</code>: Returns the mapping <code>[logical_idx -&gt; physical_idx]</code> for the attention kernel.</li>
</ul>
<p><strong>Student Task</strong>: You will implement the allocation logic in the Python Kernels phase.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="python-kernels-guide"><a class="header" href="#python-kernels-guide">Python Kernels Guide</a></h1>
<p>This section guides you through implementing the core Python logic "kernels". These are not CUDA kernels, but critical algorithmic components.</p>
<h2 id="your-tasks"><a class="header" href="#your-tasks">Your Tasks</a></h2>
<ol>
<li><strong>Radix Tree</strong>: Implement the Trie data structure for prefix matching.</li>
<li><strong>KV Cache Manager</strong>: Implement the block allocation strategy.</li>
<li><strong>Sampling</strong>: Implement the token sampling logic.</li>
</ol>
<p><strong>Why Python?</strong>
While computation happens in CUDA, the <em>logic</em> of memory management and prefix matching is complex and best handled in Python (or C++ CPU code) before dispatching to the GPU.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="radix-tree-kernelsradix_treepy"><a class="header" href="#radix-tree-kernelsradix_treepy">Radix Tree (<code>kernels/radix_tree.py</code>)</a></h1>
<h2 id="1-concept-the-prefix-tree"><a class="header" href="#1-concept-the-prefix-tree">1. Concept: The Prefix Tree</a></h2>
<p>A <strong>Radix Tree</strong> (or Compressed Trie) is a data structure that succinctly stores sequences of tokens. Unlike a standard trie where each edge is a single character (or token), a Radix Tree allows edges to be <strong>sequences</strong> of tokens.</p>
<h3 id="optimization-goal"><a class="header" href="#optimization-goal">Optimization Goal</a></h3>
<p>When two requests start with <code>"The quick brown fox"</code>, we want to store that sequence <strong>once</strong>.</p>
<ul>
<li><strong>Request A</strong>: <code>"The quick brown fox jumps"</code></li>
<li><strong>Request B</strong>: <code>"The quick brown fox sleeps"</code></li>
</ul>
<p>In our tree, we should have a shared node for <code>"The quick brown fox"</code>, which then branches into <code>"jumps"</code> and <code>"sleeps"</code>.</p>
<pre><code class="language-mermaid">flowchart TD
    Root((Root)) --&gt; Shared["The quick brown fox"]
    Shared --&gt; Branch1["jumps"]
    Shared --&gt; Branch2["sleeps"]

    style Shared fill:#aaffaa
</code></pre>
<hr />
<h2 id="2-implementation-guide"><a class="header" href="#2-implementation-guide">2. Implementation Guide</a></h2>
<p>Open <code>src/kernels/radix_tree.py</code>. You will implement the <code>RadixTree</code> class step-by-step.</p>
<h3 id="step-1-define-the-tree-node"><a class="header" href="#step-1-define-the-tree-node">Step 1: Define the Tree Node</a></h3>
<p>First, we need a node structure. Unlike a binary tree, a Radix Node can have many children.</p>
<pre><code class="language-python">class RadixTreeNode:
    def __init__(self, prefix: List[int]):
        self.prefix = prefix               # The sequence of tokens on this edge
        self.children: Dict[int, RadixTreeNode] = {} # Map: first_token -&gt; Child Node
        self.request_id: Optional[str] = None # If a request ends here, store its ID
        self.lock_count = 0                # Reference counting (how many requests use this?)
</code></pre>
<p><strong>Task</strong>: Locate the <code>RadixTreeNode</code> class and ensure it has these fields.</p>
<hr />
<h3 id="step-2-implement-match_prefix"><a class="header" href="#step-2-implement-match_prefix">Step 2: Implement <code>match_prefix</code></a></h3>
<p>Before inserting, we need a way to see how much of a new prompt <em>already exists</em> in the tree.</p>
<p><strong>Algorithm</strong>:</p>
<ol>
<li>Start at <code>self.root</code>.</li>
<li>Compare the input <code>token_ids</code> with the edges in the tree.</li>
<li>Traverse down as long as the tokens match exactly.</li>
<li>Return the last matching Node and the number of matching tokens.</li>
</ol>
<p><strong>Your Turn</strong>:
Implement <code>find_shared_prefixes(token_ids)</code> in <code>RadixTree</code>.</p>
<blockquote>
<p><em>Hint</em>: Use a while loop. At each node, look at <code>node.children[token_ids[current_idx]]</code>. If it exists, check if the full edge <code>child.prefix</code> matches the next chunk of your input.</p>
</blockquote>
<hr />
<h3 id="step-3-implement-insert_request-the-hard-part"><a class="header" href="#step-3-implement-insert_request-the-hard-part">Step 3: Implement <code>insert_request</code> (The Hard Part)</a></h3>
<p>Now, inserting a new request. This involves <strong>splitting</strong> nodes if a partial match is found.</p>
<p><strong>Scenario</strong>:</p>
<ul>
<li>Tree has edge <code>[1, 2, 3, 4]</code>.</li>
<li>You insert <code>[1, 2, 5]</code>.</li>
</ul>
<p><strong>Algorithm</strong>:</p>
<ol>
<li>Trace the path like in Step 2.</li>
<li>If you differ in the <em>middle</em> of an edge (e.g., matched <code>1, 2</code> but tree has <code>3</code>, you have <code>5</code>):
<ul>
<li><strong>Split</strong>: Create a new parent node for <code>[1, 2]</code>.</li>
<li>Make the old node <code>[3, 4]</code> a child of this new parent.</li>
<li>Create your new node <code>[5]</code> as another child.</li>
</ul>
</li>
</ol>
<pre><code class="language-mermaid">flowchart TD
    Start([Insert 1, 2, 5]) --&gt; Match{Match 1, 2?}
    Match --&gt;|Yes| Diverge{Next is 3 vs 5}
    Diverge --&gt; Split[Split Edge]
    Split --&gt; Old[Child: 3, 4]
    Split --&gt; New[Child: 5]
</code></pre>
<p><strong>Your Turn</strong>:
Implement <code>insert_request(request_id, token_ids)</code>.</p>
<ul>
<li>Use your <code>match_prefix</code> logic helper.</li>
<li>Handle the 3 cases: Exact match, New Branch, or Split Edge.</li>
</ul>
<hr />
<h3 id="step-4-verify"><a class="header" href="#step-4-verify">Step 4: Verify</a></h3>
<p>Create a test script <code>tests/test_radix_manual.py</code>:</p>
<pre><code class="language-python">tree = RadixTree()
tree.insert_request("req1", [1, 2, 3])
match, count = tree.find_shared_prefixes([1, 2, 3, 4])
print(f"Matched {count} tokens") # Should be 3!
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kv-cache-manager-kernelskv_cachepy"><a class="header" href="#kv-cache-manager-kernelskv_cachepy">KV Cache Manager (<code>kernels/kv_cache.py</code>)</a></h1>
<h2 id="1-concept-paged-attention"><a class="header" href="#1-concept-paged-attention">1. Concept: Paged Attention</a></h2>
<p>In a standard implementation, KV Cache is a huge contiguous tensor <code>[MAX_SEQ_LEN, HEADS, DIM]</code>. This wastes memory because most prompts are shorter than <code>MAX_SEQ_LEN</code>.</p>
<p><strong>Paged Attention</strong> breaks this tensor into small fixed-size blocks (e.g., size 16).</p>
<ul>
<li><strong>Physical Memory</strong>: A big pool of blocks <code>[NUM_BLOCKS, 16, HEADS, DIM]</code>.</li>
<li><strong>Logical Memory</strong>: For each request, we just keep a list of block indices <code>[0, 5, 12]</code>.</li>
</ul>
<p>Your job is to write the <strong>Allocator</strong> (like <code>malloc</code> in C).</p>
<hr />
<h2 id="2-implementation-guide-1"><a class="header" href="#2-implementation-guide-1">2. Implementation Guide</a></h2>
<p>Open <code>src/kernels/kv_cache.py</code>.</p>
<h3 id="step-1-initialization"><a class="header" href="#step-1-initialization">Step 1: Initialization</a></h3>
<p>We need to track which blocks are free and which are used.</p>
<p><strong>Task</strong>: In <code>__init__</code>:</p>
<ol>
<li>Create a list <code>self.free_blocks</code>. Initially, it should contain <em>all</em> integers from <code>0</code> to <code>num_blocks - 1</code>.</li>
<li>Create a dictionary <code>self.block_tables</code>. This will map <code>request_id -&gt; List[int]</code> (the list of blocks owned by that request).</li>
</ol>
<pre><code class="language-python"># Hint
self.free_blocks = list(range(num_blocks))
</code></pre>
<hr />
<h3 id="step-2-the-allocate_blocks-method"><a class="header" href="#step-2-the-allocate_blocks-method">Step 2: The <code>allocate_blocks</code> Method</a></h3>
<p>When a request comes in (or generates new tokens), it needs memory.</p>
<p><strong>Signature</strong>:</p>
<pre><code class="language-python">def allocate_blocks(self, request_id: str, num_tokens: int) -&gt; List[int]:
</code></pre>
<p><strong>Algorithm</strong>:</p>
<ol>
<li>Calculate how many blocks are needed.
<ul>
<li>$N_{blocks} = \lceil num_tokens / block_size \rceil$</li>
</ul>
</li>
<li>Check if we have enough <code>free_blocks</code>.
<ul>
<li>If <code>len(free_blocks) &lt; needed</code>, raise an Error (or handle OOM).</li>
</ul>
</li>
<li><strong>Pop</strong> the blocks from <code>free_blocks</code>.</li>
<li><strong>Assign</strong> them to <code>self.block_tables[request_id]</code>.</li>
<li>Return the list of allocated block indices.</li>
</ol>
<p><strong>Your Turn</strong>: Implement this logic. Watch out for integer division!</p>
<hr />
<h3 id="step-3-the-free_blocks-method"><a class="header" href="#step-3-the-free_blocks-method">Step 3: The <code>free_blocks</code> Method</a></h3>
<p>When a request finishes, we must reclaim memory.</p>
<p><strong>Algorithm</strong>:</p>
<ol>
<li>Look up the blocks for <code>request_id</code>.</li>
<li><strong>Append</strong> them back to <code>self.free_blocks</code>.</li>
<li>Delete the entry from <code>self.block_tables</code>.</li>
</ol>
<p><strong>Critical</strong>: Do not double-free! (Though Python sets make this easier, a list is faster for standard stacks).</p>
<hr />
<h3 id="step-4-connecting-to-the-engine"><a class="header" href="#step-4-connecting-to-the-engine">Step 4: Connecting to the Engine</a></h3>
<p>The <code>get_kv_tensors</code> method is checking if you can translate the "Logical" view to the "Physical" view.</p>
<p><strong>Task</strong>: Implement <code>get_kv_tensors</code>.</p>
<ul>
<li>It should presumably return the specific GPU tensors for the blocks.</li>
<li><em>Note</em>: In this Python simulation, just returning the indices is often enough for the Scheduler to know mapping. The actual <em>Tensor</em> access happens in the CUDA kernel.</li>
</ul>
<hr />
<h3 id="step-5-verify"><a class="header" href="#step-5-verify">Step 5: Verify</a></h3>
<p>Create <code>tests/test_kv_manual.py</code>:</p>
<pre><code class="language-python">manager = KVCacheManager(num_blocks=10, block_size=16, ...)
# Alloc 20 tokens -&gt; needs 2 blocks (indices 0, 1)
blocks = manager.allocate_blocks("req1", 20)
print(blocks)
# Free
manager.free_blocks("req1")
print(len(manager.free_blocks)) # Should be 10 again
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="radix-attention-module-kernelsradix_attentionpy"><a class="header" href="#radix-attention-module-kernelsradix_attentionpy">Radix Attention Module (<code>kernels/radix_attention.py</code>)</a></h1>
<h2 id="1-concept-connecting-the-dots"><a class="header" href="#1-concept-connecting-the-dots">1. Concept: Connecting the Dots</a></h2>
<p>We have a <strong>Radix Tree</strong> (prefix matching) and a <strong>Paged KV Cache</strong> (memory management). The <code>RadixAttentionWithPagedKVCache</code> class is the glue that runs on the CPU (Python side) to manage these resources before we launch the GPU kernels.</p>
<p>It doesn't run the attention <em>math</em> (that's the CUDA kernel's job). Instead, it manages the <strong>metadata</strong>:</p>
<ul>
<li>"Request A needs to append 'cat' to its sequence."</li>
<li>"Does 'cat' already exist in the Radix Tree?"</li>
<li>"If yes, reuse the block."</li>
<li>"If no, allocate a new block."</li>
</ul>
<hr />
<h2 id="2-implementation-guide-2"><a class="header" href="#2-implementation-guide-2">2. Implementation Guide</a></h2>
<p>Open <code>src/kernels/radix_attention.py</code>.</p>
<h3 id="step-1-initialization-1"><a class="header" href="#step-1-initialization-1">Step 1: Initialization</a></h3>
<p>You need to initialize the two sub-components we built earlier.</p>
<pre><code class="language-python">class RadixAttentionWithPagedKVCache:
    def __init__(self, ...):
        # ...
        self.radix_tree = RadixTree()
        self.kv_cache_manager = KVCacheManager(...)
</code></pre>
<h3 id="step-2-append_slot-the-critical-logic"><a class="header" href="#step-2-append_slot-the-critical-logic">Step 2: <code>append_slot</code> (The Critical Logic)</a></h3>
<p>This method is called when we want to add a new token (or tokens) to a request.</p>
<p><strong>Signature</strong>:</p>
<pre><code class="language-python">def append_slot(self, key: torch.Tensor, value: torch.Tensor, request_id: str):
</code></pre>
<ul>
<li><code>key</code>/<code>value</code>: The computed K/V tensors for the <em>new</em> token(s).</li>
</ul>
<p><strong>Algorithm</strong>:</p>
<ol>
<li><strong>Check Tree</strong>: Use <code>self.radix_tree</code> to see if this <code>(request_id + new_token)</code> path already exists?
<ul>
<li><em>Note</em>: In a real system, we check <em>before</em> computing K/V. Here, we might just be managing the cache storage.</li>
</ul>
</li>
<li><strong>Allocate</strong>: If we need new space, call <code>self.kv_cache_manager.allocate_blocks()</code>.</li>
<li><strong>Store</strong>: We need to perform the copy.
<ul>
<li>Ideally, we just return the <em>indices</em> of where to write, and the GPU kernel does the writing.</li>
<li>For this Python simulation, you might simulate the copy or just track the metadata.</li>
</ul>
</li>
</ol>
<h3 id="step-3-get_kv_cache"><a class="header" href="#step-3-get_kv_cache">Step 3: <code>get_kv_cache</code></a></h3>
<p>The scheduler asks: "I am about to run requests <code>[R1, R2]</code>. Where is their data?"</p>
<p><strong>Algorithm</strong>:</p>
<ol>
<li>Loop through <code>request_ids</code>.</li>
<li>For each, ask <code>self.kv_cache_manager</code> for its block table (list of integers).</li>
<li>Pack these lists into a single Tensor <code>block_tables</code>.</li>
<li>Return <code>block_tables</code> to the Engine.</li>
</ol>
<h3 id="step-4-free_request"><a class="header" href="#step-4-free_request">Step 4: <code>free_request</code></a></h3>
<p>When a request is done:</p>
<ol>
<li><code>self.radix_tree.remove_request(request_id)</code> (Decrement ref counts).</li>
<li><code>self.kv_cache_manager.free_blocks(request_id)</code> (Reclaim memory).</li>
</ol>
<hr />
<h2 id="3-the-radixattentionblock-model-layer"><a class="header" href="#3-the-radixattentionblock-model-layer">3. The <code>RadixAttentionBlock</code> (Model Layer)</a></h2>
<p>The class <code>RadixAttentionBlock</code> is the PyTorch module that sits in the model.</p>
<p><strong>Task</strong>:
In <code>forward()</code>:</p>
<ol>
<li>Compute Q, K, V projections.</li>
<li>Compute RoPE (Rotary Embeddings).</li>
<li><strong>If Prefill</strong>: Use Flash Attention (or a standard attention) on the new tokens.</li>
<li><strong>If Decode</strong>:
<ul>
<li>Call <code>append_slot</code> to save the new K/V.</li>
<li>Call <code>paged_attention_kernel</code> (the CUDA op) to attend to the <em>entire</em> history using the block tables.</li>
</ul>
</li>
</ol>
<p><strong>Exercise</strong>:
Since we don't have the full model weight loading for this specific block, focus on the <strong>logic flow</strong> in the comments.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="sampling-kernelssamplingpy"><a class="header" href="#sampling-kernelssamplingpy">Sampling (<code>kernels/sampling.py</code>)</a></h1>
<h2 id="1-concept-from-logits-to-tokens"><a class="header" href="#1-concept-from-logits-to-tokens">1. Concept: From Logits to Tokens</a></h2>
<p>The model outputs <strong>logits</strong>: a vector of size <code>[VOCAB_SIZE]</code> (e.g., 50,000) containing raw scores for the next token.
We need to pick <strong>one</strong> token ID.</p>
<ul>
<li><strong>Greedy</strong>: Just pick <code>argmax()</code>. Boring, repetitive.</li>
<li><strong>Sampling</strong>: Pick randomly based on probability. Creative!</li>
</ul>
<p>We control the randomness with <strong>Temperature</strong>, <strong>Top-P</strong> (Nucleus), and <strong>Top-K</strong>.</p>
<hr />
<h2 id="2-implementation-guide-3"><a class="header" href="#2-implementation-guide-3">2. Implementation Guide</a></h2>
<p>Open <code>src/kernels/sampling.py</code>.</p>
<h3 id="step-1-temperature-scaling"><a class="header" href="#step-1-temperature-scaling">Step 1: Temperature Scaling</a></h3>
<p><strong>Temperature</strong> ($T$) controls confidence.</p>
<ul>
<li>$T &lt; 1$: Makes peakier (more confident).</li>
<li>$T &gt; 1$: Makes flatter (more random).</li>
</ul>
<p><strong>Algorithm</strong>:</p>
<pre><code class="language-python">logits = logits / temperature
</code></pre>
<ul>
<li><strong>Watch out</strong>: If $T$ is very close to 0, just do <code>argmax</code> to avoid division by zero!</li>
</ul>
<hr />
<h3 id="step-2-softmax"><a class="header" href="#step-2-softmax">Step 2: Softmax</a></h3>
<p>Convert logits to probabilities (0.0 to 1.0).</p>
<pre><code class="language-python">probs = torch.softmax(logits, dim=-1)
</code></pre>
<hr />
<h3 id="step-3-top-k-filtering"><a class="header" href="#step-3-top-k-filtering">Step 3: Top-K Filtering</a></h3>
<p>Keep only the $K$ most likely tokens. Zero out the rest.</p>
<p><strong>Algorithm</strong>:</p>
<ol>
<li>Find the value of the $K$-th highest score.</li>
<li>Mask (set to $-\infty$) anything below that value in <code>logits</code> (or 0 in <code>probs</code>).</li>
<li>Re-normalize probabilities.</li>
</ol>
<hr />
<h3 id="step-4-top-p-nucleus-filtering-the-tricky-one"><a class="header" href="#step-4-top-p-nucleus-filtering-the-tricky-one">Step 4: Top-P (Nucleus) Filtering (The Tricky One)</a></h3>
<p>Keep the smallest set of tokens whose cumulative probability adds up to $P$ (e.g., 0.9). This dynamically truncates the long tail of "nonsense" words.</p>
<p><strong>Algorithm</strong>:</p>
<ol>
<li>Sort probabilities in descending order: <code>sorted_probs, sorted_indices = torch.sort(probs, descending=True)</code>.</li>
<li>Compute cumulative sum: <code>cumulative_probs = torch.cumsum(sorted_probs, dim=-1)</code>.</li>
<li>Find cut-off: Mask where <code>cumulative_probs &gt; top_p</code>.
<ul>
<li><em>Tip</em>: You want to include the <em>first</em> token that crosses the threshold. So shift the mask right by one.</li>
</ul>
</li>
<li>Scatter the mask back to the original ordering.</li>
<li>Re-normalize.</li>
</ol>
<hr />
<h3 id="step-5-the-final-selection"><a class="header" href="#step-5-the-final-selection">Step 5: The Final Selection</a></h3>
<p>Once you have your clean probability distribution:</p>
<pre><code class="language-python">next_token = torch.multinomial(probs, num_samples=1)
</code></pre>
<p><strong>Your Turn</strong>: Implement <code>sample</code> in <code>SamplingKernel</code>. Start simple (just Temperature) and verify, then add Top-P.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cuda-setup"><a class="header" href="#cuda-setup">CUDA Setup</a></h1>
<p>Implementing custom CUDA kernels requires compiling C++/CUDA code and binding it to Python.</p>
<h2 id="setuppy"><a class="header" href="#setuppy"><code>setup.py</code></a></h2>
<p>We use <code>torch.utils.cpp_extension</code> to handle compilation.
The <code>setup.py</code> file in the root directory is already configured to look for kernels in <code>src/kernels/cuda/</code>.</p>
<h3 id="triggering-compilation"><a class="header" href="#triggering-compilation">Triggering Compilation</a></h3>
<p>To compile your kernels, simply run:</p>
<pre><code class="language-bash">pip install -e .
</code></pre>
<p>This command invokes <code>nvcc</code> (NVIDIA CUDA Compiler) on your <code>.cu</code> files.</p>
<h3 id="using-the-kernels"><a class="header" href="#using-the-kernels">Using the Kernels</a></h3>
<p>Once compiled, you can import them in Python:</p>
<pre><code class="language-python">import mini_yaie_kernels

# Call your C++ function
mini_yaie_kernels.flash_attention.forward(...)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="memory-operations-kernelscudamemory_opscu"><a class="header" href="#memory-operations-kernelscudamemory_opscu">Memory Operations (<code>kernels/cuda/memory_ops.cu</code>)</a></h1>
<h2 id="concept"><a class="header" href="#concept">Concept</a></h2>
<p>Moving data between different GPU memory locations is a frequent operation in Paged Attention.</p>
<h2 id="implementation-goal"><a class="header" href="#implementation-goal">Implementation Goal</a></h2>
<p>Implement <code>copy_blocks_kernel</code>:</p>
<h3 id="signature"><a class="header" href="#signature">Signature</a></h3>
<pre><code class="language-cpp">void copy_blocks_kernel(
    torch::Tensor key_cache,      // [num_blocks, block_size, head_dim]
    torch::Tensor value_cache,    // [num_blocks, block_size, head_dim]
    torch::Tensor block_mapping,  // [num_mappings, 2] (src, dst)
    int num_mappings
);
</code></pre>
<h3 id="logic"><a class="header" href="#logic">Logic</a></h3>
<ol>
<li><strong>Parallelism</strong>: Launch one thread per token to copy.</li>
<li><strong>Indexing</strong>:
<ul>
<li><code>mapping_idx = blockIdx.x</code></li>
<li><code>src_block = block_mapping[mapping_idx][0]</code></li>
<li><code>dst_block = block_mapping[mapping_idx][1]</code></li>
</ul>
</li>
<li><strong>Copy</strong>:
<ul>
<li>Read <code>key/value</code> from <code>src_block</code> at <code>threadIdx</code> offset.</li>
<li>Write to <code>dst_block</code>.</li>
</ul>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="flash-attention-kernelscudaflash_attentioncu"><a class="header" href="#flash-attention-kernelscudaflash_attentioncu">Flash Attention (<code>kernels/cuda/flash_attention.cu</code>)</a></h1>
<h2 id="1-concept-memory-bandwidth"><a class="header" href="#1-concept-memory-bandwidth">1. Concept: Memory Bandwidth</a></h2>
<p>The main bottleneck in Attention is reading the huge $N \times N$ matrix from memory.
<strong>Flash Attention</strong> breaks the problem into small "tiles" that fit into the GPU's fast <strong>SRAM</strong> (Shared Memory). We compute everything for that tile without going back to slow Global Memory.</p>
<pre><code class="language-mermaid">graph TB
    subgraph GlobalMemory [Global Memory (HBM)]
        Q[Matrix Q]
        K[Matrix K]
        V[Matrix V]
    end

    subgraph SRAM [Shared Memory (SRAM)]
        TileQ[Tile Q]
        TileK[Tile K]
        TileV[Tile V]
        Comp(("Compute QK^T * V"))
    end

    Q --&gt; TileQ
    K --&gt; TileK
    V --&gt; TileV

    TileQ --&gt; Comp
    TileK --&gt; Comp
    TileV --&gt; Comp
</code></pre>
<hr />
<h2 id="2-implementation-guide-4"><a class="header" href="#2-implementation-guide-4">2. Implementation Guide</a></h2>
<p>We will implement a <strong>simplified</strong> version. Doing full FlashAttention v2 is extremely complex. We aim for "Tiled Attention".</p>
<h3 id="step-0-the-setup"><a class="header" href="#step-0-the-setup">Step 0: The Setup</a></h3>
<p>Open <code>src/kernels/cuda/flash_attention.cu</code>.
Identify the <code>flash_attention_forward</code> function.</p>
<p>You have pointers to:</p>
<ul>
<li><code>query</code> (Q), <code>key</code> (K), <code>value</code> (V) residing in Global Memory.</li>
</ul>
<h3 id="step-1-define-thread-layout"><a class="header" href="#step-1-define-thread-layout">Step 1: Define Thread Layout</a></h3>
<p>We want to process tiles.</p>
<ul>
<li><strong>Grid</strong>: One block per query chunk.</li>
<li><strong>Block</strong>: Threads within the block handle individual heads or elements.</li>
</ul>
<pre><code class="language-cpp">// Example
dim3 grid(num_batches, num_heads);
dim3 block(128); // 128 threads work together on one head
</code></pre>
<h3 id="step-2-load-tiles-to-shared-memory"><a class="header" href="#step-2-load-tiles-to-shared-memory">Step 2: Load Tiles to Shared Memory</a></h3>
<p>You need <code>__shared__</code> memory arrays.</p>
<pre><code class="language-cpp">__shared__ float s_Q[TILE_SIZE][HEAD_DIM];
__shared__ float s_K[TILE_SIZE][HEAD_DIM];
</code></pre>
<p>Use <code>threadIdx.x</code> to cooperatively load data from Global <code>Q</code> to Shared <code>s_Q</code>.
<strong>Remember</strong>: call <code>__syncthreads()</code> after loading!</p>
<h3 id="step-3-compute-qkt-scores"><a class="header" href="#step-3-compute-qkt-scores">Step 3: Compute $QK^T$ (Scores)</a></h3>
<p>Iterate over your shared Q and K.
Calculate the dot product.
Store in a register (local variable).</p>
<h3 id="step-4-softmax-the-online-trick"><a class="header" href="#step-4-softmax-the-online-trick">Step 4: Softmax (The "Online" Trick)</a></h3>
<p>In standard softmax, you need the max of the <em>entire</em> row. Here we only see a tile!
<strong>Trick</strong>: Keep a running max ($m$) and running sum ($l$). Update them as you see new tiles.</p>
<ul>
<li>$m_{new} = \max(m_{old}, \max(current_tile))$</li>
<li>Adjust previous sums by multiplying by $e^{m_{old} - m_{new}}$.</li>
</ul>
<h3 id="step-5-compute-score-times-v"><a class="header" href="#step-5-compute-score-times-v">Step 5: Compute Score $\times$ V</a></h3>
<p>Once you have the probabilities for the tile, multiply by <code>s_V</code> (which you also loaded).
Accumulate into <code>output</code>.</p>
<hr />
<h2 id="3-hints"><a class="header" href="#3-hints">3. Hints</a></h2>
<ul>
<li>Start with a <strong>Naive</strong> kernel first! Forget shared memory. Just loops.
<ul>
<li>Thread per query token.</li>
<li>Loop over all key tokens.</li>
<li>Compute.</li>
<li>This is $O(N^2)$ memory reads but verifies your logic is correct.</li>
</ul>
</li>
<li>Only optimize to Shared Memory once logic works.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="paged-attention-kernelscudapaged_attentioncu"><a class="header" href="#paged-attention-kernelscudapaged_attentioncu">Paged Attention (<code>kernels/cuda/paged_attention.cu</code>)</a></h1>
<h2 id="1-concept-indirection"><a class="header" href="#1-concept-indirection">1. Concept: Indirection</a></h2>
<p>Paged Attention is just standard attention, but <code>K</code> and <code>V</code> are not contiguous.
We have to "gather" them using a Page Table.</p>
<pre><code class="language-mermaid">graph LR
    Thread --&gt;|1. Get Logical idx| Logic[Token #42]
    Logic --&gt;|2. Lookup Table| Table[Block 2, Offset 10]
    Table --&gt;|3. Get Physical Addr| Phys[0xA000...]
    Phys --&gt;|4. Read| Data[Value]
</code></pre>
<hr />
<h2 id="2-implementation-guide-5"><a class="header" href="#2-implementation-guide-5">2. Implementation Guide</a></h2>
<h3 id="step-1-understand-the-block-table"><a class="header" href="#step-1-understand-the-block-table">Step 1: Understand the Block Table</a></h3>
<p>You are passed <code>block_tables</code> tensor of shape <code>[num_seqs, max_blocks]</code>.</p>
<ul>
<li>It holds integer indices of physical blocks.</li>
<li><code>block_tables[req_id][0]</code> is the first block of that request.</li>
</ul>
<h3 id="step-2-calculate-physical-address"><a class="header" href="#step-2-calculate-physical-address">Step 2: Calculate Physical Address</a></h3>
<p>Inside your kernel, you want the Key vector for token <code>t</code> of request <code>r</code>.</p>
<pre><code class="language-cpp">int block_idx = t / BLOCK_SIZE;
int block_offset = t % BLOCK_SIZE;
int physical_block_number = block_tables[r][block_idx];

// Pointer arithmetic
float* k_ptr = key_cache_base
             + physical_block_number * (BLOCK_SIZE * HEAD_DIM * NUM_HEADS)
             + ... // navigate to specific head and offset
</code></pre>
<h3 id="step-3-load-data"><a class="header" href="#step-3-load-data">Step 3: Load Data</a></h3>
<p>Using the pointer <code>k_ptr</code>, load the vector into registers or shared memory.</p>
<h3 id="step-4-compute-attention"><a class="header" href="#step-4-compute-attention">Step 4: Compute Attention</a></h3>
<p>Once loaded, the math is identical to standard Attention or Flash Attention.
$Q \cdot K^T$, Softmax, $\cdot V$.</p>
<hr />
<h2 id="3-your-task"><a class="header" href="#3-your-task">3. Your Task</a></h2>
<p>Implement <code>paged_attention_kernel</code> in <code>src/kernels/cuda/paged_attention.cu</code>.</p>
<ol>
<li>Focus on the <strong>address calculation</strong> logic. That is the only difference!</li>
<li>Use the <code>copy_blocks</code> kernel (Memory Ops) to help set up test data if needed.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="radix-operations-kernelscudaradix_opscu"><a class="header" href="#radix-operations-kernelscudaradix_opscu">Radix Operations (<code>kernels/cuda/radix_ops.cu</code>)</a></h1>
<h2 id="concept-1"><a class="header" href="#concept-1">Concept</a></h2>
<p>If we have a Radix Tree, we can optimize attention even further by knowing exactly which tokens are shared.</p>
<h2 id="implementation-goal-1"><a class="header" href="#implementation-goal-1">Implementation Goal</a></h2>
<p>This is an advanced extension.</p>
<h3 id="logic-1"><a class="header" href="#logic-1">Logic</a></h3>
<ol>
<li><strong>Tree Traversal on GPU</strong>: Mapping the Radix Tree structure to a GPU-friendly format (e.g., flattened arrays).</li>
<li><strong>Prefix Matching</strong>: A kernel that takes a batch of prompts and quickly identifies the longest common prefix node ID for each.</li>
</ol>
<p><em>Note: In the simplified version, this logic is often kept in CPU (Python) and only the KV indices are passed to the GPU.</em></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="api-endpoints-serverapipy"><a class="header" href="#api-endpoints-serverapipy">API Endpoints (<code>server/api.py</code>)</a></h1>
<h2 id="openai-compatibility"><a class="header" href="#openai-compatibility">OpenAI Compatibility</a></h2>
<p>Mini-YAIE strives to be drop-in compatible with OpenAI's API format.</p>
<h3 id="post-v1chatcompletions"><a class="header" href="#post-v1chatcompletions">POST <code>/v1/chat/completions</code></a></h3>
<p><strong>Request Body</strong>:</p>
<pre><code class="language-json">{
  "model": "gpt2",
  "messages": [{ "role": "user", "content": "Hello!" }],
  "temperature": 0.7,
  "max_tokens": 100
}
</code></pre>
<p><strong>Response</strong>:</p>
<pre><code class="language-json">{
  "id": "chat-123",
  "object": "chat.completion",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Hi there!"
      },
      "finish_reason": "stop"
    }
  ]
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cli-usage"><a class="header" href="#cli-usage">CLI Usage</a></h1>
<h2 id="starting-the-server"><a class="header" href="#starting-the-server">Starting the Server</a></h2>
<pre><code class="language-bash">yaie serve --model gpt2 --port 8000
</code></pre>
<h2 id="interactive-chat"><a class="header" href="#interactive-chat">Interactive Chat</a></h2>
<pre><code class="language-bash">yaie chat --model gpt2
</code></pre>
<h2 id="arguments"><a class="header" href="#arguments">Arguments</a></h2>
<ul>
<li><code>--model</code>: Name of the HuggingFace model or path to local model.</li>
<li><code>--max-batch-size</code>: Limit the number of concurrent requests.</li>
<li><code>--gpu-memory-utilization</code>: Fraction of GPU memory to use for KV cache (default 0.9).</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="production-considerations"><a class="header" href="#production-considerations">Production Considerations</a></h1>
<p>While Mini-YAIE is educational, here is what you would need for production:</p>
<h2 id="1-batching--latency"><a class="header" href="#1-batching--latency">1. Batching &amp; Latency</a></h2>
<ul>
<li><strong>Timeouts</strong>: Requests waiting too long in the queue should be handled.</li>
<li><strong>Preemption</strong>: If high-priority requests come in, lower priority ones should be paused (swapped to CPU).</li>
</ul>
<h2 id="2-distributed-inference"><a class="header" href="#2-distributed-inference">2. Distributed Inference</a></h2>
<ul>
<li><strong>Tensor Parallelism</strong>: Splitting the model weights across multiple GPUs (Megatron-LM style).</li>
<li><strong>Pipeline Parallelism</strong>: Splitting layers across GPUs.</li>
</ul>
<h2 id="3-quantization"><a class="header" href="#3-quantization">3. Quantization</a></h2>
<ul>
<li><strong>FP8 / INT8</strong>: Running with lower precision to save memory and increase compute speed (using library like <code>bitsandbytes</code>).</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="references"><a class="header" href="#references">References</a></h1>
<ol>
<li><strong>SGLang</strong>: Efficient Execution of Structured Language Model Programs. <a href="https://github.com/sgl-project/sglang">Link</a></li>
<li><strong>vLLM</strong>: Easy, Fast, and Cheap LLM Serving with PagedAttention. <a href="https://github.com/vllm-project/vllm">Link</a></li>
<li><strong>FlashAttention</strong>: Fast and Memory-Efficient Exact Attention with IO-Awareness. <a href="https://github.com/Dao-AILab/flash-attention">Link</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="troubleshooting"><a class="header" href="#troubleshooting">Troubleshooting</a></h1>
<h2 id="cuda-kernel-not-found"><a class="header" href="#cuda-kernel-not-found">"CUDA kernel not found"</a></h2>
<ul>
<li>Ensure you ran <code>pip install -e .</code>.</li>
<li>Check if <code>nvcc</code> is in your path: <code>nvcc --version</code>.</li>
</ul>
<h2 id="outofmemoryerror"><a class="header" href="#outofmemoryerror">"OutOfMemoryError"</a></h2>
<ul>
<li>Decrease <code>max_batch_size</code>.</li>
<li>Decrease <code>kv_cache_manager</code> block count.</li>
</ul>
<h2 id="importerror-attempted-relative-import"><a class="header" href="#importerror-attempted-relative-import">"ImportError: attempted relative import..."</a></h2>
<ul>
<li>Ensure you are running the <code>yaie</code> command, or running python as a module <code>python -m src.cli.main</code>.</li>
<li>Do not run scripts directly like <code>python src/engine.py</code>.</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <!-- Livereload script (if served using the cli tool) -->
        <script>
            const wsProtocol = location.protocol === 'https:' ? 'wss:' : 'ws:';
            const wsAddress = wsProtocol + "//" + location.host + "/" + "__livereload";
            const socket = new WebSocket(wsAddress);
            socket.onmessage = function (event) {
                if (event.data === "reload") {
                    socket.close();
                    location.reload();
                }
            };

            window.onbeforeunload = function() {
                socket.close();
            }
        </script>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>


    </div>
    </body>
</html>
