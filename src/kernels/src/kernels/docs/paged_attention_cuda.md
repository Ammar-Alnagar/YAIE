# Paged Attention CUDA Kernel (`paged_attention.cu`)

The `paged_attention.cu` module provides an optimized CUDA kernel implementation for Paged Attention. This technique is crucial for efficient Key-Value (KV) cache management in large language models (LLMs), particularly when handling multiple concurrent sequences with varying lengths and dynamic allocation patterns.

## Key Concepts

*   **Paged Attention**: An advanced memory management scheme for the KV-cache. Instead of allocating contiguous memory for each sequence's KV-cache, Paged Attention stores the KV-cache in fixed-size "blocks" (or pages). These blocks can be non-contiguous in GPU memory, allowing for flexible allocation, efficient sharing of KV-cache across sequences with shared prefixes, and reduced memory fragmentation.
*   **KV-Cache**: The Key and Value tensors generated by a transformer's attention mechanism for previously processed tokens. Storing this cache avoids redundant computation, especially during token generation.
*   **Benefits**: Paged Attention significantly improves GPU memory utilization, allows for higher throughput due to better batching, and naturally supports sharing of prefixes among different sequences.

## Macros

*   `MAX_SEQ_LEN`: Defines a maximum sequence length (e.g., `4096`).
*   `MAX_BLOCK_SIZE`: Defines the maximum size (in tokens) of a KV-cache block (e.g., `16`).
*   `WARP_SIZE`: Standard CUDA warp size, often used for setting thread block dimensions and optimizing parallel reductions (typically `32`).

## `paged_attention_kernel_impl` (CUDA Kernel)

This is the core CUDA kernel responsible for computing attention using the paged KV-cache. It performs the attention score calculation, applies scaling and masking, and accumulates results for softmax.

*   **Purpose**: To compute the attention output for a batch of sequences, efficiently accessing key and value data from the paged KV-cache.
*   **Function Parameters**:
    *   `query_ptr` (const `float*`): Pointer to the query tensor data (`[num_seqs, num_heads, head_dim]`).
    *   `key_cache_ptr` (const `float*`): Pointer to the physical key cache memory (`[num_blocks, block_size, num_heads, head_dim]`).
    *   `value_cache_ptr` (const `float*`): Pointer to the physical value cache memory (`[num_blocks, block_size, num_heads, head_dim]`).
    *   `block_tables_ptr` (const `int*`): Pointer to the block table mapping logical sequence positions to physical block IDs (`[num_seqs, max_num_blocks_per_seq]`).
    *   `context_lens_ptr` (const `int*`): Pointer to the actual context length for each sequence (`[num_seqs]`).
    *   `output_ptr` (`float*`): Pointer to the output tensor data (`[num_seqs, num_heads, head_dim]`).
    *   `exp_sums_ptr` (`float*`): Temporary storage for exponential sums for numerical stability (for partial softmax).
    *   `max_logits_ptr` (`float*`): Temporary storage for maximum logits for numerical stability (for partial softmax).
    *   `scale` (const `float`): Scaling factor for attention scores (e.g., `1 / sqrt(head_dim)`).
    *   `num_seqs` (const `int`): Total number of sequences in the batch.
    *   `num_heads` (const `int`): Number of attention heads.
    *   `head_dim` (const `int`): Dimension of each attention head.
    *   `block_size` (const `int`): Size of each KV-cache block.
    *   `max_num_blocks_per_seq` (const `int`): Maximum number of blocks a single sequence can occupy.
    *   `max_context_len` (const `int`): Maximum context length among all sequences in the batch.
    *   `max_num_partitions` (const `int`): Maximum number of partitions for softmax accumulation.
*   **Internal Logic**:
    1.  **Thread Indexing**: Threads are organized to process specific sequences (`seq_idx`), attention heads (`head_idx`), and blocks within a sequence (`block_idx`), with `tid` identifying the thread within a warp.
    2.  **Context Length and Block Count**: Retrieves the actual context length and calculates the number of blocks for the current sequence.
    3.  **Local Accumulators**: Initializes `sum_exp`, `max_logit`, and `out_val` for numerically stable softmax calculation within a thread.
    4.  **Physical Block Access**: Uses `block_tables_ptr` to map a logical block index within a sequence to its physical location in `key_cache_ptr` and `value_cache_ptr`.
    5.  **Iterating Block Positions**: Loops through each token position within the current physical block.
    6.  **Attention Score Calculation**: Computes the dot product between the query vector and the key vector for the current position, then applies the `scale` factor.
    7.  **Causal Masking**: Applies a causal mask by setting scores to `-INFINITY` for future tokens (for autoregressive decoding).
    8.  **Numerical Stability**: Implements an online softmax calculation by iteratively updating `max_logit` (maximum logit seen so far), `sum_exp` (sum of exponentials), and `out_val` (weighted sum of values), using scaling factors (`expf(old_max - current_max)`) to prevent overflow/underflow.
    9.  **Reduction**: Uses `__shared__` memory and warp-level primitives (simplified here with a basic reduction) to aggregate partial `sum_exp`, `max_logit`, and `out_val` across threads within the block.
    10. **Result Storage**: The final attention output for the sequence and head is written to `output_ptr`.

## `paged_attention_kernel` (CUDA C++ Wrapper)

This C++ function acts as a wrapper to prepare the input tensors, configure kernel launch parameters, and invoke the `paged_attention_kernel_impl` CUDA kernel.

*   **Purpose**: Provides a PyTorch-compatible function to execute the Paged Attention CUDA kernel.
*   **Parameters**:
    *   `output` (torch::Tensor): The output tensor for attention results.
    *   `query` (torch::Tensor): Input query tensor.
    *   `key_cache` (torch::Tensor): The global key cache tensor.
    *   `value_cache` (torch::Tensor): The global value cache tensor.
    *   `block_tables` (torch::Tensor): The block tables tensor.
    *   `context_lens` (torch::Tensor): The context lengths tensor for each sequence.
    *   `block_size` (int): The size of each KV-cache block.
    *   `max_context_len` (int): The maximum context length in the batch.
*   **Internal Logic**:
    1.  **Dimension Extraction**: Retrieves `num_seqs`, `num_heads`, `head_dim` from input tensors and `num_blocks`, `max_num_blocks_per_seq` from cache tensors.
    2.  **Partition Calculation**: Determines `max_num_partitions` for softmax accumulation.
    3.  **Grid and Block Configuration**: Sets up the `grid` and `block` dimensions for the kernel launch. The `block.x` dimension is dynamically adjusted to ensure it does not exceed hardware limits.
    4.  **Temporary Tensor Allocation**: Allocates `exp_sums` and `max_logits` tensors on the GPU for the kernel's intermediate calculations.
    5.  **Scaling Factor**: Computes the `scale` factor (`1.0f / sqrtf(static_cast<float>(head_dim))`).
    6.  **Kernel Launch**: Invokes the `paged_attention_kernel_impl` using the configured `grid`, `block`, shared memory size (0 in this case, as shared memory is dynamically allocated within the kernel for reduction), and CUDA stream.
    7.  **Error Checking**: Includes `cudaGetLastError()` to catch and report any CUDA kernel launch errors.
